{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "# import scipy.ndimage\n",
    "import scipy.io as io\n",
    "# import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mat = io.loadmat('Indian_pines.mat')['indian_pines']\n",
    "target_mat = io.loadmat('Indian_pines_gt.mat')['indian_pines_gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "PATCH_SIZE = 3\n",
    "HEIGHT = input_mat.shape[0]\n",
    "WIDTH = input_mat.shape[1]\n",
    "BAND = input_mat.shape[2]\n",
    "CLASSES = [] \n",
    "COUNT = 200\n",
    "OUTPUT_CLASSES = np.max(target_mat)\n",
    "print (OUTPUT_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mat = input_mat.astype(float)\n",
    "input_mat -= np.min(input_mat)\n",
    "input_mat /= np.max(input_mat)\n",
    "list_labels = [2,3,5,6,8,10,11,12,14]\n",
    "train_idx = [178, 178, 178, 177, 177, 178, 178, 178, 178]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Patch(height_index,width_index):\n",
    "    \"\"\"\n",
    "    Returns a mean-normalized patch, the top left corner of which \n",
    "    is at (height_index, width_index)\n",
    "    \n",
    "    Inputs: \n",
    "    height_index - row index of the top left corner of the image patch\n",
    "    width_index - column index of the top left corner of the image patch\n",
    "    \n",
    "    Outputs:\n",
    "    mean_normalized_patch - mean normalized patch of size (PATCH_SIZE, PATCH_SIZE) \n",
    "    whose top left corner is at (height_index, width_index)\n",
    "    \"\"\"\n",
    "    transpose_array = input_mat\n",
    "    height_slice = slice(height_index, height_index+PATCH_SIZE)\n",
    "    width_slice = slice(width_index, width_index+PATCH_SIZE)\n",
    "    patch = transpose_array[:, height_slice, width_slice]\n",
    "    mean_normalized_patch = []\n",
    "    for i in range(patch.shape[0]):\n",
    "        mean_normalized_patch.append(patch[i] - MEAN_ARRAY[i]) \n",
    "    \n",
    "    return np.array(mean_normalized_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220, 145, 145)\n",
      "(220, 147, 147)\n"
     ]
    }
   ],
   "source": [
    "MEAN_ARRAY = np.ndarray(shape=(BAND,),dtype=float)\n",
    "new_input_mat = []\n",
    "input_mat = np.transpose(input_mat,(2,0,1))\n",
    "print(input_mat.shape)\n",
    "for i in range(BAND):\n",
    "    MEAN_ARRAY[i] = np.mean(input_mat[i,:,:])\n",
    "    new_input_mat.append(np.pad(input_mat[i,:,:],int(PATCH_SIZE/2),'constant',constant_values = 0))\n",
    "    \n",
    "print (np.array(new_input_mat).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10249\n"
     ]
    }
   ],
   "source": [
    "input_mat = np.array(new_input_mat)\n",
    "\n",
    "\n",
    "for i in range(OUTPUT_CLASSES):\n",
    "    CLASSES.append([])\n",
    "count = 0\n",
    "image = []\n",
    "image_label = []\n",
    "for i in range(HEIGHT):\n",
    "    for j in range(WIDTH):\n",
    "        curr_inp = Patch(i,j)\n",
    "        curr_tar = target_mat[i , j]\n",
    "        if(curr_tar!=0): #Ignore patches with unknown landcover type for the central pixel\n",
    "            CLASSES[curr_tar-1].append(curr_inp)\n",
    "            count += 1\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATCH,TRAIN_LABELS,TEST_PATCH,TEST_LABELS,VAL_PATCH, VAL_LABELS = [],[],[],[],[],[]\n",
    "FULL_TRAIN_PATCH = []\n",
    "FULL_TRAIN_LABELS = []\n",
    "count = 0\n",
    "for i, data in enumerate(CLASSES):\n",
    "    if i+1 in list_labels:\n",
    "        shuffle(data)\n",
    "        TRAIN_PATCH += data[:train_idx[count]]\n",
    "        TRAIN_LABELS += [count]*train_idx[count]\n",
    "        VAL_PATCH += data[train_idx[count]:200]\n",
    "        VAL_LABELS += [count]*(200-train_idx[count])\n",
    "        TEST_PATCH += data[200:]\n",
    "        TEST_LABELS += [count]*(len(data) - 200)\n",
    "        count += 1\n",
    "\n",
    "FULL_TRAIN_LABELS = TRAIN_LABELS + VAL_LABELS\n",
    "FULL_TRAIN_PATCH = TRAIN_PATCH + VAL_PATCH\n",
    "\n",
    "TRAIN_LABELS = np.array(TRAIN_LABELS)\n",
    "TRAIN_PATCH = np.array(TRAIN_PATCH)\n",
    "TEST_PATCH = np.array(TEST_PATCH)\n",
    "TEST_LABELS = np.array(TEST_LABELS)\n",
    "VAL_PATCH = np.array(VAL_PATCH)\n",
    "VAL_LABELS = np.array(VAL_LABELS)\n",
    "FULL_TRAIN_LABELS = np.array(FULL_TRAIN_LABELS)\n",
    "FULL_TRAIN_PATCH = np.array(FULL_TRAIN_PATCH)\n",
    "\n",
    "train_idx = list(range(len(TRAIN_PATCH)))\n",
    "shuffle(train_idx)\n",
    "TRAIN_PATCH = TRAIN_PATCH[train_idx]\n",
    "TRAIN_LABELS = TRAIN_LABELS[train_idx]\n",
    "test_idx = range(len(TEST_PATCH))\n",
    "TEST_PATCH = TEST_PATCH[test_idx]\n",
    "TEST_LABELS = TEST_LABELS[test_idx]\n",
    "val_idx = list(range(len(VAL_PATCH)))\n",
    "shuffle(val_idx)\n",
    "VAL_PATCH = VAL_PATCH[val_idx]\n",
    "VAL_LABELS = VAL_LABELS[val_idx]\n",
    "full_train_idx = shuffle(list(range(len(FULL_TRAIN_PATCH))))\n",
    "FULL_TRAIN_PATCH = FULL_TRAIN_PATCH[full_train_idx]\n",
    "FULL_TRAIN_LABELS = FULL_TRAIN_LABELS[full_train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 220, 3, 3)\n",
      "(7434, 220, 3, 3)\n",
      "(200, 220, 3, 3)\n",
      "(1, 1800)\n"
     ]
    }
   ],
   "source": [
    "train = {}\n",
    "train[\"train_patch\"] = TRAIN_PATCH\n",
    "train[\"train_labels\"] = TRAIN_LABELS\n",
    "io.savemat(\"indian_pines_Train_patch_\" + str(PATCH_SIZE) + \".mat\", train)\n",
    "print (TRAIN_PATCH.shape)\n",
    "\n",
    "\n",
    "test = {}\n",
    "test[\"test_patch\"] = TEST_PATCH\n",
    "test[\"test_labels\"] = TEST_LABELS\n",
    "io.savemat(\"indian_pines_Test_patch_\" + str(PATCH_SIZE) + \".mat\", test)\n",
    "print (TEST_PATCH.shape)\n",
    "\n",
    "val = {}\n",
    "val[\"val_patch\"] = VAL_PATCH\n",
    "val[\"val_labels\"] = VAL_LABELS\n",
    "io.savemat(\"indian_pines_Val_patch_\" + str(PATCH_SIZE) + \".mat\", val)\n",
    "print (VAL_PATCH.shape)\n",
    "\n",
    "full_train = {}\n",
    "full_train[\"train_patch\"] = FULL_TRAIN_PATCH\n",
    "full_train[\"train_labels\"] = FULL_TRAIN_LABELS\n",
    "io.savemat(\"indian_pines_Full_Train_patch_\" + str(PATCH_SIZE) + \".mat\", full_train)\n",
    "print (FULL_TRAIN_LABELS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require 'nn'\n",
    "# require 'optim'\n",
    "# matio = require 'matio'\n",
    "# require 'xlua'\n",
    "# require 'pl'\n",
    "# require 'paths'\n",
    "# require 'torch'\n",
    "# require 'math'\n",
    "\n",
    "# local opt = lapp[[\n",
    "#    --development      (default 1)                    Use development dataset/ Whole training dataset \n",
    "#    -s,--save          (default \"logs/\")               subdirectory to save logs\n",
    "#    -p,--plot                                         plot while training\n",
    "#    -o,--optimization  (default \"Adam\")               optimization: SGD | LBFGS | Adam\n",
    "#    -l,--learningRate  (default 0.0005)              learning rate, for SGD only\n",
    "#    -b,--batchSize     (default 200)                  batch size\n",
    "#    -m,--momentum      (default 0)                    momentum, for SGD only\n",
    "#    -i,--maxIter       (default 8000)                 maximum nb of iterations per batch, for LBFGS\n",
    "#    --coefL1           (default 0)                    L1 penalty on the weights\n",
    "#    --coefL2           (default 0)                    L2 penalty on the weights\n",
    "#    -t,--type          (default \"cpu\")                GPU or CPU\n",
    "#    --network          (default \"MLP\")                MLP or CNN\n",
    "#    --patch_size       (default 3)                    patch size of tthe image\n",
    "#    --nbands           (default 10)                   number of bands\n",
    "#    --block1_conv1     (default 3333)                 number of filters in the 1*1 convolution (here, 3333 is the sentinel value)\n",
    "# ]]\n",
    "development = 1\n",
    "save = 'logs/'\n",
    "learningRate = 0.0005\n",
    "batchSize = 200\n",
    "momentum = 0\n",
    "maxIter = 8000\n",
    "coefL1 = 0\n",
    "coefL2 = 0\n",
    "patch_size = 3\n",
    "nbands = 10\n",
    "block1_conv1 = 3333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nval = 200\n",
    "nclasses = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datamat = io.loadmat('indian_pines_Test_patch_3.mat')\n",
    "test_data = test_datamat['test_patch']\n",
    "test_labels = test_datamat['test_labels'].transpose()\n",
    "channels = test_data.shape[2] #test_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datamat = io.loadmat('indian_pines_Train_patch_3.mat')\n",
    "train_data = train_datamat['train_patch']\n",
    "train_labels = train_datamat['train_labels'].transpose()\n",
    "\n",
    "val_data = io.loadmat(\"indian_pines_Val_patch_3.mat\")['val_patch']\n",
    "val_labels = io.loadmat(\"indian_pines_Val_patch_3.mat\")['val_labels'].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if block1_conv1 == 3333:\n",
    "    block1_conv1 = channels\n",
    "\n",
    "while (block1_conv1 % nbands == 0):\n",
    "    nbands = nbands + 1\n",
    "    print(\"Number of parallel networks reinitialized to \",nbands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 220, 3, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = {}\n",
    "trainset[\"data\"] = train_data\n",
    "trainset[\"labels\"] = train_labels\n",
    "\n",
    "testset = {}\n",
    "testset[\"data\"] = test_data\n",
    "testset[\"labels\"] = test_labels\n",
    "\n",
    "valset = {}\n",
    "valset[\"data\"] = val_data\n",
    "valset[\"labels\"] = val_labels\n",
    "\n",
    "# print(trainset)\n",
    "# print(testset)\n",
    "# print(valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "band_size = block1_conv1/nbands\n",
    "band_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Reshape, ReLU, Conv1D, Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation=’relu’, input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation=’relu’))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation=’softmax’))\n",
    "\n",
    "\n",
    "model = nn.Sequential()\n",
    "\n",
    "model:add(nn.Reshape(opt.band_size, opt.patch_size * opt.patch_size))\n",
    "model:add(nn.TemporalConvolution(opt.patch_size * opt.patch_size, 20, 3, 1))\n",
    "model:add(nn.ReLU())\n",
    "model:add(nn.TemporalConvolution(20, 20, 3, 1))\n",
    "model:add(nn.ReLU())\n",
    "model:add(nn.TemporalConvolution(20, 10, 3, 1))\n",
    "model:add(nn.ReLU())\n",
    "model:add(nn.TemporalConvolution(10, 5, 5, 1))\n",
    "model:add(nn.ReLU())\n",
    "model:add(nn.Reshape((opt.band_size-10) * 5, 1))\n",
    "parallel_model = nn.Parallel(2, 2)\n",
    "for i = 1, opt.nbands do\n",
    "    parallel_model:add(model:clone())\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Reshape((band_size, patch_size*patch_size)))\n",
    "model.add(Conv1D( 20, kernel_size=3, strides=1))\n",
    "model.add(ReLU())\n",
    "model.add(Conv1D( 20, kernel_size=3, strides=1))\n",
    "model.add(ReLU())\n",
    "model.add(Conv1D( 10, kernel_size=3, strides=1))\n",
    "model.add(ReLU())\n",
    "model.add(Conv1D( 5, kernel_size=3, strides=1))\n",
    "model.add(ReLU())\n",
    "model.add(Reshape(((band_size-10)*5, 1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "net = nn.Sequential()\n",
    "net:add(nn.SpatialConvolution(opt.channels, opt.block1_conv1, 1, 1))\n",
    "net:add(nn.ReLU())\n",
    "net:add(nn.Reshape(opt.nbands, opt.band_size, opt.patch_size*opt.patch_size))\n",
    "net:add(parallel_model)\n",
    "net:add(nn.Reshape(opt.nbands*(opt.band_size-10)*5))\n",
    "net:add(nn.Linear(opt.nbands*(opt.band_size-10)*5, 100))\n",
    "net:add(nn.ReLU())\n",
    "net:add(nn.Dropout())\n",
    "net:add(nn.Linear(100, opt.nclasses))\n",
    "net:add(nn.LogSoftMax())\n",
    "__SpatialConvolution : init(nInputPlane, nOutputPlane, kW, kH, dW, dH, padW, padH)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- For parameter sharing\n",
    "\n",
    "https://stackoverflow.com/questions/45245396/can-i-share-weights-between-keras-layers-but-have-other-parameters-differ\n",
    "\n",
    "lowing code snippet may be helpful:\n",
    "\n",
    "def create_shared_weights(conv1, conv2, input_shape):\n",
    "    with K.name_scope(conv1.name):\n",
    "        conv1.build(input_shape)\n",
    "    with K.name_scope(conv2.name):\n",
    "        conv2.build(input_shape)\n",
    "    conv2.kernel = conv1.kernel\n",
    "    conv2.bias = conv1.bias\n",
    "    conv2._trainable_weights = []\n",
    "    conv2._trainable_weights.append(conv2.kernel)\n",
    "    conv2._trainable_weights.append(conv2.bias)\n",
    "\n",
    "### check if weights are successfully shared\n",
    "input_img = Input(shape=(299, 299, 3))\n",
    "conv1 = Conv2D(64, 3, padding='same')\n",
    "conv2 = Conv2D(64, 3, padding='valid')\n",
    "create_shared_weights(conv1, conv2, input_img._keras_shape)\n",
    "print(conv2.weights == conv1.weights)  # True\n",
    "\n",
    "### check if weights are equal after model fitting\n",
    "left = conv1(input_img)\n",
    "right = conv2(input_img)\n",
    "left = GlobalAveragePooling2D()(left)\n",
    "right = GlobalAveragePooling2D()(right)\n",
    "merged = concatenate([left, right])\n",
    "output = Dense(1)(merged)\n",
    "model = Model(input_img, output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "X = np.random.rand(5, 299, 299, 3)\n",
    "Y = np.random.randint(2, size=5)\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Sequential()\n",
    "net.add(Conv2D( block1_conv1, kernel_size = 1, strides = 1))\n",
    "net.add(ReLU())\n",
    "net.add(Reshape(( nbands, band_size, patch_size**2)))\n",
    "# net.add(parallel_model)\n",
    "# net.add(Reshape(( nbands*(band_size-10)*5))\n",
    "net.add(Dense(100))\n",
    "net.add(ReLU())\n",
    "net.add(Dense(9, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setmetatable(trainset, {__index = function(self, index)\n",
    "#              local input = self.data[index]\n",
    "#              local class = self.labels[index]\n",
    "#              local labelvector = torch.zeros(opt.nclasses)\n",
    "#              local label = labelvector\n",
    "#              label[class[1]+1] = 1\n",
    "#              local example = {input, label}\n",
    "#                                    return example\n",
    "# end})\n",
    "\n",
    "# setmetatable(testset, {__index = function(self, index)\n",
    "#              local input = self.data[index]\n",
    "#              local class = self.labels[index]\n",
    "#              local labelvector = torch.zeros(opt.nclasses)\n",
    "#              local label = labelvector\n",
    "#              label[class[1]+1] = 1\n",
    "#              local example = {input, label}\n",
    "#                                    return example\n",
    "# end})\n",
    "\n",
    "# setmetatable(valset, {__index = function(self, index)\n",
    "#              local input = self.data[index]\n",
    "#              local class = self.labels[index]\n",
    "#              local labelvector = torch.zeros(opt.nclasses)\n",
    "#              local label = labelvector\n",
    "#              label[class[1]+1] = 1\n",
    "#              local example = {input, label}\n",
    "#                                    return example\n",
    "# end})\n",
    "\n",
    "\n",
    "# function trainset:size()\n",
    "#     return trainset.data:size(1)\n",
    "# end\n",
    "# function testset:size()\n",
    "#     return testset.data:size(1)\n",
    "# end\n",
    "# function valset:size()\n",
    "#     return valset.data:size(1)\n",
    "# end\n",
    "\n",
    "\n",
    "# model = nn.Sequential()\n",
    "\n",
    "# model:add(nn.Reshape(opt.band_size, opt.patch_size*opt.patch_size))\n",
    "# model:add(nn.TemporalConvolution(opt.patch_size*opt.patch_size, 20, 3, 1))\n",
    "# model:add(nn.ReLU())\n",
    "# model:add(nn.TemporalConvolution(20, 20, 3, 1))\n",
    "# model:add(nn.ReLU())\n",
    "# model:add(nn.TemporalConvolution(20, 10, 3, 1))\n",
    "# model:add(nn.ReLU())\n",
    "# model:add(nn.TemporalConvolution(10, 5, 5, 1))\n",
    "# model:add(nn.ReLU())\n",
    "# model:add(nn.Reshape((opt.band_size-10)*5, 1))\n",
    "\n",
    "parallel_model = nn.Parallel(2, 2)\n",
    "for i = 1, opt.nbands do\n",
    "    parallel_model:add(model:clone())\n",
    "end\n",
    "\n",
    "net = nn.Sequential()\n",
    "net:add(nn.SpatialConvolution(opt.channels, opt.block1_conv1, 1, 1))\n",
    "net:add(nn.ReLU())\n",
    "net:add(nn.Reshape(opt.nbands, opt.band_size, opt.patch_size*opt.patch_size))\n",
    "net:add(parallel_model)\n",
    "net:add(nn.Reshape(opt.nbands*(opt.band_size-10)*5))\n",
    "net:add(nn.Linear(opt.nbands*(opt.band_size-10)*5, 100))\n",
    "net:add(nn.ReLU())\n",
    "net:add(nn.Dropout())\n",
    "net:add(nn.Linear(100, opt.nclasses))\n",
    "net:add(nn.LogSoftMax())\n",
    "\n",
    "-- Parameter Sharing\n",
    "\n",
    "parallel_model = net:get(4)\n",
    "for band = 2, opt.nbands do\n",
    "  local current_module = parallel_model:get(band)\n",
    "  current_module:share(parallel_model:get(1), 'weight', 'bias',\n",
    "                       'gradWeight', 'gradBias')\n",
    "end\n",
    "\n",
    "net:training()\n",
    "\n",
    "criterion = nn.ClassNLLCriterion()\n",
    "\n",
    "if opt.data == \"Salinas\" then\n",
    "  classes = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15'}\n",
    "else\n",
    "  classes = {'0', '1', '2', '3', '4', '5', '6', '7', '8'}\n",
    "end\n",
    "\n",
    "parameters, gradParameters = net:getParameters()\n",
    "confusion = optim.ConfusionMatrix(classes)\n",
    "\n",
    "trainLogger = optim.Logger(paths.concat(\"./\" .. opt.save .. opt.data, 'train.log'))\n",
    "valLogger = optim.Logger(paths.concat(\"./\" .. opt.save .. opt.data, 'val.log'))\n",
    "testLogger = optim.Logger(paths.concat(\"./\" .. opt.save .. opt.data, 'test.log'))\n",
    "\n",
    "\n",
    "function train(dataset)\n",
    "   -- epoch tracker\n",
    "   epoch = epoch or 1\n",
    "\n",
    "   -- local vars\n",
    "   local time = sys.clock()\n",
    "\n",
    "   -- do one epoch\n",
    "   print('<trainer> on training set:')\n",
    "   print(\"<trainer> online epoch # \" .. epoch .. ' [batchSize = ' .. opt.batchSize .. ']')\n",
    "   for t = 1,dataset:size(),opt.batchSize do\n",
    "\n",
    "      local inputs = torch.Tensor(opt.batchSize, opt.channels, opt.patch_size, opt.patch_size)\n",
    "\n",
    "      local targets = torch.Tensor(opt.batchSize)\n",
    "      if opt.type == \"cuda\" then\n",
    "      \tinputs:cuda()\n",
    "      \ttargets:cuda()\n",
    "      end\n",
    "      local k = 1\n",
    "      for i = t,math.min(t+opt.batchSize-1,dataset:size()) do\n",
    "         -- load new sample\n",
    "         local sample = dataset[i]\n",
    "         local input = sample[1]:clone()\n",
    "         local _,target = sample[2]:clone():max(1)\n",
    "         target = target:squeeze()\n",
    "         inputs[k] = input\n",
    "         targets[k] = target\n",
    "         k = k + 1\n",
    "      end\n",
    "\n",
    "      -- create closure to evaluate f(X) and df/dX\n",
    "      local feval = function(x)\n",
    "         -- just in case:\n",
    "         collectgarbage()\n",
    "\n",
    "         -- get new parameters\n",
    "         if x ~= parameters then\n",
    "            parameters:copy(x)\n",
    "         end\n",
    "\n",
    "         -- reset gradients\n",
    "         gradParameters:zero()\n",
    "\n",
    "         -- evaluate function for complete mini batch\n",
    "         local outputs = net:forward(inputs)\n",
    "         local f = criterion:forward(outputs, targets)\n",
    "\n",
    "         -- estimate df/dW\n",
    "         local df_do = criterion:backward(outputs, targets)\n",
    "         net:backward(inputs, df_do)\n",
    "\n",
    "         -- penalties (L1 and L2):\n",
    "         if opt.coefL1 ~= 0 or opt.coefL2 ~= 0 then\n",
    "            -- locals:\n",
    "            local norm,sign= torch.norm,torch.sign\n",
    "\n",
    "            -- Loss:\n",
    "            f = f + opt.coefL1 * norm(parameters,1)\n",
    "            f = f + opt.coefL2 * norm(parameters,2)^2/2\n",
    "\n",
    "            -- Gradients:\n",
    "            gradParameters:add( sign(parameters):mul(opt.coefL1) + parameters:clone():mul(opt.coefL2) )\n",
    "         end\n",
    "\n",
    "         -- update confusion\n",
    "         -- print(inputs:size(1))\n",
    "         for i = 1,opt.batchSize do\n",
    "            confusion:add(outputs[i], targets[i])\n",
    "         end\n",
    "\n",
    "         -- return f and df/dX\n",
    "         return f,gradParameters\n",
    "      end\n",
    "\n",
    "      -- optimize on current mini-batch\n",
    "      if opt.optimization == 'LBFGS' then\n",
    "\n",
    "         -- Perform LBFGS step:\n",
    "         lbfgsState = lbfgsState or {\n",
    "            maxIter = opt.maxIter,\n",
    "            lineSearch = optim.lswolfe\n",
    "         }\n",
    "         optim.lbfgs(feval, parameters, lbfgsState)\n",
    "       \n",
    "         -- disp report:\n",
    "         print('LBFGS step')\n",
    "         print(' - progress in batch: ' .. t .. '/' .. dataset:size())\n",
    "         print(' - nb of iterations: ' .. lbfgsState.nIter)\n",
    "         print(' - nb of function evalutions: ' .. lbfgsState.funcEval)\n",
    "\n",
    "      elseif opt.optimization == 'SGD' then\n",
    "\n",
    "         -- Perform SGD step:\n",
    "         sgdState = sgdState or {\n",
    "            learningRate = opt.learningRate,\n",
    "            momentum = opt.momentum,\n",
    "            learningRateDecay = 5e-7\n",
    "         }\n",
    "         optim.sgd(feval, parameters, sgdState)\n",
    "      elseif opt.optimization == \"Adam\" then\n",
    "\n",
    "      \t adamState = adamState or {\n",
    "      \t \tlearningRate = opt.learningRate,\n",
    "      \t \tmomentum = opt.momentum,\n",
    "      \t \tlearningRateDecay = 5e-9\n",
    "      \t }\n",
    "      \t optim.adam(feval, parameters, adamState)\n",
    "         -- disp progress\n",
    "         xlua.progress(t, dataset:size())\n",
    "\n",
    "      else\n",
    "         error('unknown optimization method')\n",
    "      end\n",
    "   end\n",
    "   \n",
    "   -- time taken\n",
    "   time = sys.clock() - time\n",
    "   time = time / dataset:size()\n",
    "\n",
    "   -- print confusion matrix\n",
    "   print(confusion)\n",
    "   print('% mean class accuracy (train set)' .. tostring(confusion.totalValid*100))\n",
    "   trainLogger:add{['% mean class accuracy (train set)'] = confusion.totalValid * 100}\n",
    "   confusion:zero()\n",
    "   epoch = epoch + 1\n",
    "   return (1 - confusion.totalValid)*100\n",
    "end\n",
    "best_val = 0\n",
    "function val(dataset)\n",
    "\n",
    "    net:evaluate()\n",
    "    for t = 1, dataset:size(), opt.batchSize do\n",
    "        xlua.progress(t, dataset:size())\n",
    "        local inputs = torch.Tensor(opt.batchSize, opt.channels, opt.patch_size, opt.patch_size)\n",
    "        local targets = torch.Tensor(opt.batchSize)\n",
    "        local k = 1\n",
    "        for i = t, math.min(t+opt.batchSize-1, dataset:size()) do\n",
    "            local sample = dataset[i]\n",
    "            local input = sample[1]:clone()\n",
    "            local _,target = sample[2]:clone():max(1)\n",
    "            target = target:squeeze()\n",
    "            inputs[k] = input\n",
    "            targets[k] = target\n",
    "            k = k+1\n",
    "        end\n",
    "        \n",
    "        local preds = net:forward(inputs) -- Computing Loss\n",
    "        for l = 1, k - 1 do \n",
    "            confusion:add(preds[l], targets[l])\n",
    "        end\n",
    "    end\n",
    "    print(confusion)\n",
    "    if confusion.totalValid > best_val then\n",
    "        best_val = confusion.totalValid\n",
    "    end\n",
    "    print(\"Best validation accuracy yet :\" .. tostring(best_val*100) .. \"%\")\n",
    "    valLogger:add{['% mean class accuracy (val set)'] = confusion.totalValid * 100}\n",
    "    confusion:zero()\n",
    "    net:training()\n",
    "\n",
    "end     \n",
    "\n",
    "best_test = 0\n",
    "function test(dataset)\n",
    "\n",
    "    net:evaluate()\n",
    "    for t = 1, dataset:size(), opt.batchSize do\n",
    "        xlua.progress(t, dataset:size())\n",
    "        local inputs = torch.Tensor(opt.batchSize, opt.channels, opt.patch_size, opt.patch_size)\n",
    "        local targets = torch.Tensor(opt.batchSize)\n",
    "        local k = 1\n",
    "        for i = t, math.min(t+opt.batchSize-1, dataset:size()) do\n",
    "            local sample = dataset[i]\n",
    "            local input = sample[1]:clone()\n",
    "            local _,target = sample[2]:clone():max(1)\n",
    "            target = target:squeeze()\n",
    "            inputs[k] = input\n",
    "            targets[k] = target\n",
    "            k = k+1\n",
    "        end\n",
    "        \n",
    "        local preds = net:forward(inputs) -- Computing Loss\n",
    "        for l = 1, k - 1 do \n",
    "            confusion:add(preds[l], targets[l])\n",
    "        end\n",
    "    end\n",
    "    print(confusion)\n",
    "    if confusion.totalValid > best_test then\n",
    "        best_test = confusion.totalValid\n",
    "        torch.save(\"./pretrained/final_Block_2_no_fc.t7\", net)\n",
    "    end\n",
    "    print(\"Best test accuracy yet :\" .. tostring(best_test*100) .. \"%\")\n",
    "    testLogger:add{['% mean class accuracy (test set)'] = confusion.totalValid * 100}\n",
    "    confusion:zero()\n",
    "    net:training()\n",
    "\n",
    "end     \n",
    "\n",
    "\n",
    "for i = 1, opt.maxIter do\n",
    "    train(trainset)\n",
    "    if opt.development == 1 then\n",
    "      val(valset)\n",
    "    else\n",
    "      test(testset)\n",
    "  end\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:baseml] *",
   "language": "python",
   "name": "conda-env-baseml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
