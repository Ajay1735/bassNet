{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:baseml] *",
      "language": "python",
      "name": "conda-env-baseml-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "parallelModel v2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMGdDLUSmKR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.io as io\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEfDoGUKmKSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datamat = io.loadmat('indian_pines_Train_patch_3.mat')\n",
        "train_data = train_datamat['train_patch']\n",
        "train_labels = train_datamat['train_labels'].transpose()\n",
        "\n",
        "train_labels1 = train_labels.ravel()\n",
        "trainLabels = np.zeros((train_labels1.size, train_labels1.max()+1))\n",
        "trainLabels[np.arange(train_labels1.size),train_labels1] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X6XW2UYmKSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_datamat = io.loadmat('indian_pines_Val_patch_3.mat')\n",
        "val_data = val_datamat['val_patch']\n",
        "val_labels = val_datamat['val_labels'].transpose()\n",
        "\n",
        "val_labels1 = val_labels.ravel()\n",
        "valLabels = np.zeros((val_labels1.size, val_labels1.max()+1))\n",
        "valLabels[np.arange(val_labels1.size),val_labels1] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3xGavUWmKSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_datamat = io.loadmat('indian_pines_Test_patch_3.mat')\n",
        "test_data = test_datamat['test_patch']\n",
        "test_labels = test_datamat['test_labels'].transpose()\n",
        "\n",
        "test_labels1 = test_labels.ravel()\n",
        "testLabels = np.zeros((test_labels1.size, test_labels1.max()+1))\n",
        "testLabels[np.arange(test_labels1.size),test_labels1] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHnuqVKTmKST",
        "colab_type": "code",
        "outputId": "161539be-4235-4ccf-8a51-059c4e63f348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1600, 220, 3, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "eTGAZ61umKSY",
        "colab_type": "code",
        "outputId": "7df74fc4-1e5d-4d95-e75d-2ccb6d458066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Dense, Lambda, Flatten, Input, Conv2D, Conv1D, Reshape, Concatenate, Dropout\n",
        "\n",
        "inp = Input(shape=train_data.shape[1:])\n",
        "\n",
        "c1 = Conv2D(220, kernel_size = 1, activation=\"relu\", data_format=\"channels_first\")(inp)\n",
        "re1 = Reshape((10, 22, 3*3), input_shape = c1.shape)(c1)\n",
        "out = []\n",
        "for i in range(10):\n",
        "    reshape1 = Lambda(lambda x: x[:,i,:,:])(re1)  \n",
        "    conv1 = Conv1D(filters = 20, kernel_size = 3, strides = 1, activation=\"relu\")(reshape1)\n",
        "    conv2 = Conv1D(filters = 20, kernel_size = 3, strides = 1, activation=\"relu\")(conv1)\n",
        "    conv3 = Conv1D(filters = 10, kernel_size = 3, strides = 1, activation=\"relu\")(conv2)\n",
        "    conv4 = Conv1D(filters = 5, kernel_size = 5, strides = 1, activation=\"relu\")(conv3)\n",
        "    reshape2 = Reshape((60, 1), input_shape = conv4.shape)(conv4)\n",
        "    out.append(reshape2)\n",
        "outConc = keras.layers.concatenate(out)\n",
        "reshape3 = Flatten()(outConc)\n",
        "dense = Dense(100, activation='relu')(reshape3)\n",
        "dropout = Dropout(0.5)(dense)\n",
        "op = Dense(9, activation='softmax')(dropout)\n",
        "model = Model(inputs=[inp], outputs=op)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCkxIukEmKSd",
        "colab_type": "code",
        "outputId": "27c09119-5761-427f-d15f-4c9b03add118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAACC0AAAUtCAIAAABdg7svAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\nYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-2N6wW3mKSh",
        "colab_type": "code",
        "outputId": "9250cbb1-0626-4846-c642-a60dd71f90be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import adam\n",
        "\n",
        "ad = adam(lr=0.0005)\n",
        "model.compile(optimizer=ad, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_data, trainLabels, validation_data=(val_data, valLabels), batch_size = 200, epochs=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1600 samples, validate on 200 samples\n",
            "Epoch 1/1000\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0493 - acc: 0.9856 - val_loss: 0.2201 - val_acc: 0.9400\n",
            "Epoch 2/1000\n",
            "1600/1600 [==============================] - 1s 694us/step - loss: 0.0498 - acc: 0.9856 - val_loss: 0.2438 - val_acc: 0.9500\n",
            "Epoch 3/1000\n",
            "1600/1600 [==============================] - 1s 686us/step - loss: 0.0496 - acc: 0.9838 - val_loss: 0.2857 - val_acc: 0.9400\n",
            "Epoch 4/1000\n",
            "1600/1600 [==============================] - 1s 699us/step - loss: 0.0380 - acc: 0.9869 - val_loss: 0.1375 - val_acc: 0.9650\n",
            "Epoch 5/1000\n",
            "1600/1600 [==============================] - 1s 665us/step - loss: 0.0285 - acc: 0.9894 - val_loss: 0.2104 - val_acc: 0.9600\n",
            "Epoch 6/1000\n",
            "1600/1600 [==============================] - 1s 676us/step - loss: 0.0262 - acc: 0.9900 - val_loss: 0.3079 - val_acc: 0.9450\n",
            "Epoch 7/1000\n",
            "1600/1600 [==============================] - 1s 675us/step - loss: 0.0247 - acc: 0.9931 - val_loss: 0.1738 - val_acc: 0.9650\n",
            "Epoch 8/1000\n",
            "1600/1600 [==============================] - 1s 671us/step - loss: 0.0141 - acc: 0.9981 - val_loss: 0.1548 - val_acc: 0.9700\n",
            "Epoch 9/1000\n",
            "1600/1600 [==============================] - 1s 678us/step - loss: 0.0157 - acc: 0.9963 - val_loss: 0.1828 - val_acc: 0.9700\n",
            "Epoch 10/1000\n",
            "1600/1600 [==============================] - 1s 670us/step - loss: 0.0122 - acc: 0.9963 - val_loss: 0.1999 - val_acc: 0.9600\n",
            "Epoch 11/1000\n",
            "1600/1600 [==============================] - 1s 668us/step - loss: 0.0170 - acc: 0.9956 - val_loss: 0.1622 - val_acc: 0.9650\n",
            "Epoch 12/1000\n",
            "1600/1600 [==============================] - 1s 690us/step - loss: 0.0097 - acc: 0.9975 - val_loss: 0.1873 - val_acc: 0.9650\n",
            "Epoch 13/1000\n",
            "1600/1600 [==============================] - 1s 682us/step - loss: 0.0138 - acc: 0.9956 - val_loss: 0.1780 - val_acc: 0.9650\n",
            "Epoch 14/1000\n",
            "1600/1600 [==============================] - 1s 681us/step - loss: 0.0133 - acc: 0.9963 - val_loss: 0.1704 - val_acc: 0.9550\n",
            "Epoch 15/1000\n",
            "1600/1600 [==============================] - 1s 681us/step - loss: 0.0142 - acc: 0.9956 - val_loss: 0.2106 - val_acc: 0.9700\n",
            "Epoch 16/1000\n",
            "1600/1600 [==============================] - 1s 676us/step - loss: 0.0215 - acc: 0.9931 - val_loss: 0.1800 - val_acc: 0.9650\n",
            "Epoch 17/1000\n",
            "1600/1600 [==============================] - 1s 674us/step - loss: 0.0177 - acc: 0.9956 - val_loss: 0.1355 - val_acc: 0.9800\n",
            "Epoch 18/1000\n",
            "1600/1600 [==============================] - 1s 684us/step - loss: 0.0195 - acc: 0.9938 - val_loss: 0.1657 - val_acc: 0.9600\n",
            "Epoch 19/1000\n",
            "1600/1600 [==============================] - 1s 678us/step - loss: 0.0181 - acc: 0.9931 - val_loss: 0.1930 - val_acc: 0.9600\n",
            "Epoch 20/1000\n",
            "1600/1600 [==============================] - 1s 680us/step - loss: 0.0206 - acc: 0.9944 - val_loss: 0.1995 - val_acc: 0.9600\n",
            "Epoch 21/1000\n",
            "1600/1600 [==============================] - 1s 672us/step - loss: 0.0244 - acc: 0.9925 - val_loss: 0.1529 - val_acc: 0.9550\n",
            "Epoch 22/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0315 - acc: 0.9856 - val_loss: 0.2177 - val_acc: 0.9550\n",
            "Epoch 23/1000\n",
            "1600/1600 [==============================] - 1s 675us/step - loss: 0.0216 - acc: 0.9938 - val_loss: 0.2054 - val_acc: 0.9600\n",
            "Epoch 24/1000\n",
            "1600/1600 [==============================] - 1s 676us/step - loss: 0.0182 - acc: 0.9963 - val_loss: 0.1872 - val_acc: 0.9500\n",
            "Epoch 25/1000\n",
            "1600/1600 [==============================] - 1s 680us/step - loss: 0.0178 - acc: 0.9963 - val_loss: 0.2085 - val_acc: 0.9500\n",
            "Epoch 26/1000\n",
            "1600/1600 [==============================] - 1s 681us/step - loss: 0.0221 - acc: 0.9919 - val_loss: 0.1280 - val_acc: 0.9800\n",
            "Epoch 27/1000\n",
            "1600/1600 [==============================] - 1s 681us/step - loss: 0.0390 - acc: 0.9888 - val_loss: 0.1746 - val_acc: 0.9700\n",
            "Epoch 28/1000\n",
            "1600/1600 [==============================] - 1s 688us/step - loss: 0.0637 - acc: 0.9875 - val_loss: 0.2761 - val_acc: 0.9450\n",
            "Epoch 29/1000\n",
            "1600/1600 [==============================] - 1s 687us/step - loss: 0.0333 - acc: 0.9894 - val_loss: 0.2152 - val_acc: 0.9500\n",
            "Epoch 30/1000\n",
            "1600/1600 [==============================] - 1s 684us/step - loss: 0.0337 - acc: 0.9881 - val_loss: 0.2553 - val_acc: 0.9500\n",
            "Epoch 31/1000\n",
            "1600/1600 [==============================] - 1s 679us/step - loss: 0.0291 - acc: 0.9931 - val_loss: 0.1310 - val_acc: 0.9750\n",
            "Epoch 32/1000\n",
            "1600/1600 [==============================] - 1s 696us/step - loss: 0.0199 - acc: 0.9925 - val_loss: 0.1736 - val_acc: 0.9600\n",
            "Epoch 33/1000\n",
            "1600/1600 [==============================] - 1s 684us/step - loss: 0.0175 - acc: 0.9938 - val_loss: 0.2577 - val_acc: 0.9600\n",
            "Epoch 34/1000\n",
            "1600/1600 [==============================] - 1s 685us/step - loss: 0.0203 - acc: 0.9944 - val_loss: 0.1413 - val_acc: 0.9650\n",
            "Epoch 35/1000\n",
            "1600/1600 [==============================] - 1s 675us/step - loss: 0.0176 - acc: 0.9969 - val_loss: 0.1524 - val_acc: 0.9700\n",
            "Epoch 36/1000\n",
            "1600/1600 [==============================] - 1s 673us/step - loss: 0.0181 - acc: 0.9925 - val_loss: 0.1714 - val_acc: 0.9650\n",
            "Epoch 37/1000\n",
            "1600/1600 [==============================] - 1s 677us/step - loss: 0.0165 - acc: 0.9963 - val_loss: 0.1804 - val_acc: 0.9650\n",
            "Epoch 38/1000\n",
            "1600/1600 [==============================] - 1s 674us/step - loss: 0.0150 - acc: 0.9969 - val_loss: 0.1709 - val_acc: 0.9600\n",
            "Epoch 39/1000\n",
            "1600/1600 [==============================] - 1s 687us/step - loss: 0.0110 - acc: 0.9981 - val_loss: 0.1686 - val_acc: 0.9600\n",
            "Epoch 40/1000\n",
            "1600/1600 [==============================] - 1s 691us/step - loss: 0.0182 - acc: 0.9944 - val_loss: 0.2126 - val_acc: 0.9450\n",
            "Epoch 41/1000\n",
            "1600/1600 [==============================] - 1s 669us/step - loss: 0.0182 - acc: 0.9944 - val_loss: 0.1931 - val_acc: 0.9650\n",
            "Epoch 42/1000\n",
            "1600/1600 [==============================] - 1s 683us/step - loss: 0.0102 - acc: 0.9981 - val_loss: 0.1476 - val_acc: 0.9600\n",
            "Epoch 43/1000\n",
            "1600/1600 [==============================] - 1s 688us/step - loss: 0.0100 - acc: 0.9981 - val_loss: 0.1542 - val_acc: 0.9750\n",
            "Epoch 44/1000\n",
            "1600/1600 [==============================] - 1s 671us/step - loss: 0.0133 - acc: 0.9963 - val_loss: 0.2802 - val_acc: 0.9550\n",
            "Epoch 45/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0108 - acc: 0.9975 - val_loss: 0.2710 - val_acc: 0.9400\n",
            "Epoch 46/1000\n",
            "1600/1600 [==============================] - 1s 693us/step - loss: 0.0165 - acc: 0.9931 - val_loss: 0.1992 - val_acc: 0.9650\n",
            "Epoch 47/1000\n",
            "1600/1600 [==============================] - 1s 682us/step - loss: 0.0123 - acc: 0.9963 - val_loss: 0.1797 - val_acc: 0.9650\n",
            "Epoch 48/1000\n",
            "1600/1600 [==============================] - 1s 679us/step - loss: 0.0198 - acc: 0.9956 - val_loss: 0.1881 - val_acc: 0.9600\n",
            "Epoch 49/1000\n",
            "1600/1600 [==============================] - 1s 696us/step - loss: 0.0127 - acc: 0.9969 - val_loss: 0.1818 - val_acc: 0.9450\n",
            "Epoch 50/1000\n",
            "1600/1600 [==============================] - 1s 692us/step - loss: 0.0392 - acc: 0.9894 - val_loss: 0.1846 - val_acc: 0.9700\n",
            "Epoch 51/1000\n",
            "1600/1600 [==============================] - 1s 690us/step - loss: 0.0528 - acc: 0.9869 - val_loss: 0.2464 - val_acc: 0.9450\n",
            "Epoch 52/1000\n",
            "1600/1600 [==============================] - 1s 691us/step - loss: 0.0459 - acc: 0.9825 - val_loss: 0.1995 - val_acc: 0.9550\n",
            "Epoch 53/1000\n",
            "1600/1600 [==============================] - 1s 691us/step - loss: 0.0546 - acc: 0.9863 - val_loss: 0.2443 - val_acc: 0.9400\n",
            "Epoch 54/1000\n",
            "1600/1600 [==============================] - 1s 681us/step - loss: 0.0311 - acc: 0.9900 - val_loss: 0.1934 - val_acc: 0.9600\n",
            "Epoch 55/1000\n",
            "1600/1600 [==============================] - 1s 672us/step - loss: 0.0443 - acc: 0.9863 - val_loss: 0.2018 - val_acc: 0.9600\n",
            "Epoch 56/1000\n",
            "1600/1600 [==============================] - 1s 679us/step - loss: 0.0270 - acc: 0.9875 - val_loss: 0.2060 - val_acc: 0.9600\n",
            "Epoch 57/1000\n",
            "1600/1600 [==============================] - 1s 662us/step - loss: 0.0231 - acc: 0.9913 - val_loss: 0.2231 - val_acc: 0.9550\n",
            "Epoch 58/1000\n",
            "1600/1600 [==============================] - 1s 684us/step - loss: 0.0179 - acc: 0.9938 - val_loss: 0.1947 - val_acc: 0.9650\n",
            "Epoch 59/1000\n",
            "1600/1600 [==============================] - 1s 673us/step - loss: 0.0138 - acc: 0.9963 - val_loss: 0.2036 - val_acc: 0.9650\n",
            "Epoch 60/1000\n",
            "1600/1600 [==============================] - 1s 688us/step - loss: 0.0118 - acc: 0.9975 - val_loss: 0.1581 - val_acc: 0.9600\n",
            "Epoch 61/1000\n",
            "1600/1600 [==============================] - 1s 675us/step - loss: 0.0138 - acc: 0.9956 - val_loss: 0.1717 - val_acc: 0.9600\n",
            "Epoch 62/1000\n",
            "1600/1600 [==============================] - 1s 672us/step - loss: 0.0101 - acc: 0.9963 - val_loss: 0.2054 - val_acc: 0.9600\n",
            "Epoch 63/1000\n",
            "1600/1600 [==============================] - 1s 671us/step - loss: 0.0169 - acc: 0.9938 - val_loss: 0.2214 - val_acc: 0.9650\n",
            "Epoch 64/1000\n",
            "1600/1600 [==============================] - 1s 669us/step - loss: 0.0216 - acc: 0.9913 - val_loss: 0.1943 - val_acc: 0.9600\n",
            "Epoch 65/1000\n",
            "1600/1600 [==============================] - 1s 663us/step - loss: 0.0212 - acc: 0.9938 - val_loss: 0.1884 - val_acc: 0.9550\n",
            "Epoch 66/1000\n",
            "1600/1600 [==============================] - 1s 683us/step - loss: 0.0294 - acc: 0.9900 - val_loss: 0.2196 - val_acc: 0.9450\n",
            "Epoch 67/1000\n",
            "1600/1600 [==============================] - 1s 682us/step - loss: 0.0167 - acc: 0.9944 - val_loss: 0.1833 - val_acc: 0.9700\n",
            "Epoch 68/1000\n",
            "1600/1600 [==============================] - 1s 688us/step - loss: 0.0111 - acc: 0.9956 - val_loss: 0.1970 - val_acc: 0.9550\n",
            "Epoch 69/1000\n",
            "1600/1600 [==============================] - 1s 681us/step - loss: 0.0115 - acc: 0.9963 - val_loss: 0.1938 - val_acc: 0.9700\n",
            "Epoch 70/1000\n",
            "1600/1600 [==============================] - 1s 668us/step - loss: 0.0125 - acc: 0.9931 - val_loss: 0.2015 - val_acc: 0.9650\n",
            "Epoch 71/1000\n",
            "1600/1600 [==============================] - 1s 683us/step - loss: 0.0116 - acc: 0.9975 - val_loss: 0.2028 - val_acc: 0.9650\n",
            "Epoch 72/1000\n",
            "1600/1600 [==============================] - 1s 686us/step - loss: 0.0075 - acc: 0.9981 - val_loss: 0.2349 - val_acc: 0.9400\n",
            "Epoch 73/1000\n",
            "1600/1600 [==============================] - 1s 681us/step - loss: 0.0092 - acc: 0.9981 - val_loss: 0.2020 - val_acc: 0.9600\n",
            "Epoch 74/1000\n",
            "1600/1600 [==============================] - 1s 686us/step - loss: 0.0116 - acc: 0.9975 - val_loss: 0.2375 - val_acc: 0.9500\n",
            "Epoch 75/1000\n",
            "1600/1600 [==============================] - 1s 696us/step - loss: 0.0145 - acc: 0.9950 - val_loss: 0.2613 - val_acc: 0.9550\n",
            "Epoch 76/1000\n",
            "1600/1600 [==============================] - 1s 686us/step - loss: 0.0110 - acc: 0.9956 - val_loss: 0.1645 - val_acc: 0.9650\n",
            "Epoch 77/1000\n",
            "1600/1600 [==============================] - 1s 674us/step - loss: 0.0071 - acc: 0.9988 - val_loss: 0.2531 - val_acc: 0.9500\n",
            "Epoch 78/1000\n",
            "1600/1600 [==============================] - 1s 678us/step - loss: 0.0143 - acc: 0.9931 - val_loss: 0.1832 - val_acc: 0.9600\n",
            "Epoch 79/1000\n",
            "1600/1600 [==============================] - 1s 676us/step - loss: 0.0155 - acc: 0.9963 - val_loss: 0.2595 - val_acc: 0.9700\n",
            "Epoch 80/1000\n",
            "1600/1600 [==============================] - 1s 670us/step - loss: 0.0126 - acc: 0.9950 - val_loss: 0.2136 - val_acc: 0.9750\n",
            "Epoch 81/1000\n",
            "1600/1600 [==============================] - 1s 667us/step - loss: 0.0079 - acc: 0.9981 - val_loss: 0.1886 - val_acc: 0.9600\n",
            "Epoch 82/1000\n",
            "1600/1600 [==============================] - 1s 684us/step - loss: 0.0155 - acc: 0.9956 - val_loss: 0.2206 - val_acc: 0.9650\n",
            "Epoch 83/1000\n",
            "1600/1600 [==============================] - 1s 689us/step - loss: 0.0076 - acc: 0.9988 - val_loss: 0.2039 - val_acc: 0.9600\n",
            "Epoch 84/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0123 - acc: 0.9950 - val_loss: 0.1892 - val_acc: 0.9650\n",
            "Epoch 85/1000\n",
            "1600/1600 [==============================] - 1s 693us/step - loss: 0.0100 - acc: 0.9975 - val_loss: 0.1678 - val_acc: 0.9650\n",
            "Epoch 86/1000\n",
            "1600/1600 [==============================] - 1s 678us/step - loss: 0.0094 - acc: 0.9981 - val_loss: 0.1885 - val_acc: 0.9650\n",
            "Epoch 87/1000\n",
            "1600/1600 [==============================] - 1s 697us/step - loss: 0.0081 - acc: 0.9981 - val_loss: 0.2347 - val_acc: 0.9650\n",
            "Epoch 88/1000\n",
            "1600/1600 [==============================] - 1s 681us/step - loss: 0.0056 - acc: 0.9994 - val_loss: 0.2489 - val_acc: 0.9600\n",
            "Epoch 89/1000\n",
            "1600/1600 [==============================] - 1s 690us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.2044 - val_acc: 0.9550\n",
            "Epoch 90/1000\n",
            "1600/1600 [==============================] - 1s 686us/step - loss: 0.0060 - acc: 0.9994 - val_loss: 0.2067 - val_acc: 0.9550\n",
            "Epoch 91/1000\n",
            "1600/1600 [==============================] - 1s 690us/step - loss: 0.0096 - acc: 0.9981 - val_loss: 0.1748 - val_acc: 0.9750\n",
            "Epoch 92/1000\n",
            "1600/1600 [==============================] - 1s 677us/step - loss: 0.0093 - acc: 0.9950 - val_loss: 0.2205 - val_acc: 0.9500\n",
            "Epoch 93/1000\n",
            "1600/1600 [==============================] - 1s 687us/step - loss: 0.0253 - acc: 0.9900 - val_loss: 0.2672 - val_acc: 0.9600\n",
            "Epoch 94/1000\n",
            "1600/1600 [==============================] - 1s 687us/step - loss: 0.0343 - acc: 0.9894 - val_loss: 0.2767 - val_acc: 0.9550\n",
            "Epoch 95/1000\n",
            "1600/1600 [==============================] - 1s 685us/step - loss: 0.0475 - acc: 0.9800 - val_loss: 0.2458 - val_acc: 0.9400\n",
            "Epoch 96/1000\n",
            "1600/1600 [==============================] - 1s 688us/step - loss: 0.0473 - acc: 0.9856 - val_loss: 0.1658 - val_acc: 0.9600\n",
            "Epoch 97/1000\n",
            "1600/1600 [==============================] - 1s 683us/step - loss: 0.0341 - acc: 0.9888 - val_loss: 0.2523 - val_acc: 0.9550\n",
            "Epoch 98/1000\n",
            "1600/1600 [==============================] - 1s 678us/step - loss: 0.0182 - acc: 0.9919 - val_loss: 0.1790 - val_acc: 0.9600\n",
            "Epoch 99/1000\n",
            "1600/1600 [==============================] - 1s 688us/step - loss: 0.0157 - acc: 0.9950 - val_loss: 0.2283 - val_acc: 0.9600\n",
            "Epoch 100/1000\n",
            "1600/1600 [==============================] - 1s 700us/step - loss: 0.0096 - acc: 0.9981 - val_loss: 0.2072 - val_acc: 0.9700\n",
            "Epoch 101/1000\n",
            "1600/1600 [==============================] - 1s 696us/step - loss: 0.0214 - acc: 0.9944 - val_loss: 0.1930 - val_acc: 0.9700\n",
            "Epoch 102/1000\n",
            "1600/1600 [==============================] - 1s 693us/step - loss: 0.0148 - acc: 0.9963 - val_loss: 0.2142 - val_acc: 0.9700\n",
            "Epoch 103/1000\n",
            "1600/1600 [==============================] - 1s 686us/step - loss: 0.0173 - acc: 0.9963 - val_loss: 0.2163 - val_acc: 0.9600\n",
            "Epoch 104/1000\n",
            "1600/1600 [==============================] - 1s 688us/step - loss: 0.0167 - acc: 0.9969 - val_loss: 0.2342 - val_acc: 0.9650\n",
            "Epoch 105/1000\n",
            "1600/1600 [==============================] - 1s 686us/step - loss: 0.0117 - acc: 0.9975 - val_loss: 0.2539 - val_acc: 0.9350\n",
            "Epoch 106/1000\n",
            "1600/1600 [==============================] - 1s 699us/step - loss: 0.0133 - acc: 0.9944 - val_loss: 0.2794 - val_acc: 0.9550\n",
            "Epoch 107/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0152 - acc: 0.9931 - val_loss: 0.3061 - val_acc: 0.9450\n",
            "Epoch 108/1000\n",
            "1600/1600 [==============================] - 1s 694us/step - loss: 0.0269 - acc: 0.9913 - val_loss: 0.2630 - val_acc: 0.9450\n",
            "Epoch 109/1000\n",
            "1600/1600 [==============================] - 1s 703us/step - loss: 0.0239 - acc: 0.9900 - val_loss: 0.2701 - val_acc: 0.9450\n",
            "Epoch 110/1000\n",
            "1600/1600 [==============================] - 1s 704us/step - loss: 0.0216 - acc: 0.9938 - val_loss: 0.1975 - val_acc: 0.9650\n",
            "Epoch 111/1000\n",
            "1600/1600 [==============================] - 1s 688us/step - loss: 0.0196 - acc: 0.9944 - val_loss: 0.3240 - val_acc: 0.9500\n",
            "Epoch 112/1000\n",
            "1600/1600 [==============================] - 1s 685us/step - loss: 0.0226 - acc: 0.9925 - val_loss: 0.1773 - val_acc: 0.9600\n",
            "Epoch 113/1000\n",
            "1600/1600 [==============================] - 1s 684us/step - loss: 0.0178 - acc: 0.9963 - val_loss: 0.2115 - val_acc: 0.9600\n",
            "Epoch 114/1000\n",
            "1600/1600 [==============================] - 1s 703us/step - loss: 0.0175 - acc: 0.9956 - val_loss: 0.1863 - val_acc: 0.9650\n",
            "Epoch 115/1000\n",
            "1600/1600 [==============================] - 1s 688us/step - loss: 0.0111 - acc: 0.9956 - val_loss: 0.2029 - val_acc: 0.9600\n",
            "Epoch 116/1000\n",
            "1600/1600 [==============================] - 1s 684us/step - loss: 0.0107 - acc: 0.9963 - val_loss: 0.2523 - val_acc: 0.9600\n",
            "Epoch 117/1000\n",
            "1600/1600 [==============================] - 1s 680us/step - loss: 0.0083 - acc: 0.9975 - val_loss: 0.1676 - val_acc: 0.9700\n",
            "Epoch 118/1000\n",
            "1600/1600 [==============================] - 1s 683us/step - loss: 0.0093 - acc: 0.9975 - val_loss: 0.1417 - val_acc: 0.9650\n",
            "Epoch 119/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0052 - acc: 0.9988 - val_loss: 0.3188 - val_acc: 0.9400\n",
            "Epoch 120/1000\n",
            "1600/1600 [==============================] - 1s 675us/step - loss: 0.0140 - acc: 0.9963 - val_loss: 0.2715 - val_acc: 0.9550\n",
            "Epoch 121/1000\n",
            "1600/1600 [==============================] - 1s 686us/step - loss: 0.0093 - acc: 0.9975 - val_loss: 0.1917 - val_acc: 0.9600\n",
            "Epoch 122/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0084 - acc: 0.9975 - val_loss: 0.2309 - val_acc: 0.9550\n",
            "Epoch 123/1000\n",
            "1600/1600 [==============================] - 1s 678us/step - loss: 0.0103 - acc: 0.9975 - val_loss: 0.2164 - val_acc: 0.9600\n",
            "Epoch 124/1000\n",
            "1600/1600 [==============================] - 1s 681us/step - loss: 0.0110 - acc: 0.9969 - val_loss: 0.2869 - val_acc: 0.9500\n",
            "Epoch 125/1000\n",
            "1600/1600 [==============================] - 1s 683us/step - loss: 0.0047 - acc: 0.9994 - val_loss: 0.1856 - val_acc: 0.9550\n",
            "Epoch 126/1000\n",
            "1600/1600 [==============================] - 1s 677us/step - loss: 0.0084 - acc: 0.9975 - val_loss: 0.1780 - val_acc: 0.9700\n",
            "Epoch 127/1000\n",
            "1600/1600 [==============================] - 1s 683us/step - loss: 0.0044 - acc: 0.9994 - val_loss: 0.1804 - val_acc: 0.9700\n",
            "Epoch 128/1000\n",
            "1600/1600 [==============================] - 1s 686us/step - loss: 0.0051 - acc: 0.9994 - val_loss: 0.1789 - val_acc: 0.9700\n",
            "Epoch 129/1000\n",
            "1600/1600 [==============================] - 1s 708us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.2005 - val_acc: 0.9650\n",
            "Epoch 130/1000\n",
            "1600/1600 [==============================] - 1s 672us/step - loss: 0.0062 - acc: 0.9981 - val_loss: 0.2023 - val_acc: 0.9550\n",
            "Epoch 131/1000\n",
            "1600/1600 [==============================] - 1s 688us/step - loss: 0.0067 - acc: 0.9981 - val_loss: 0.3203 - val_acc: 0.9450\n",
            "Epoch 132/1000\n",
            "1600/1600 [==============================] - 1s 691us/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.2208 - val_acc: 0.9450\n",
            "Epoch 133/1000\n",
            "1600/1600 [==============================] - 1s 679us/step - loss: 0.0097 - acc: 0.9975 - val_loss: 0.2110 - val_acc: 0.9600\n",
            "Epoch 134/1000\n",
            "1600/1600 [==============================] - 1s 682us/step - loss: 0.0093 - acc: 0.9969 - val_loss: 0.2206 - val_acc: 0.9650\n",
            "Epoch 135/1000\n",
            "1600/1600 [==============================] - 1s 681us/step - loss: 0.0085 - acc: 0.9963 - val_loss: 0.2315 - val_acc: 0.9650\n",
            "Epoch 136/1000\n",
            "1600/1600 [==============================] - 1s 686us/step - loss: 0.0072 - acc: 0.9975 - val_loss: 0.2341 - val_acc: 0.9550\n",
            "Epoch 137/1000\n",
            "1600/1600 [==============================] - 1s 684us/step - loss: 0.0096 - acc: 0.9963 - val_loss: 0.2025 - val_acc: 0.9650\n",
            "Epoch 138/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0043 - acc: 0.9994 - val_loss: 0.2139 - val_acc: 0.9600\n",
            "Epoch 139/1000\n",
            "1600/1600 [==============================] - 1s 718us/step - loss: 0.0051 - acc: 0.9981 - val_loss: 0.2167 - val_acc: 0.9650\n",
            "Epoch 140/1000\n",
            "1600/1600 [==============================] - 1s 694us/step - loss: 0.0052 - acc: 0.9981 - val_loss: 0.1997 - val_acc: 0.9550\n",
            "Epoch 141/1000\n",
            "1600/1600 [==============================] - 1s 696us/step - loss: 0.0054 - acc: 0.9981 - val_loss: 0.1972 - val_acc: 0.9600\n",
            "Epoch 142/1000\n",
            "1600/1600 [==============================] - 1s 712us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.2348 - val_acc: 0.9600\n",
            "Epoch 143/1000\n",
            "1600/1600 [==============================] - 1s 708us/step - loss: 0.0034 - acc: 0.9994 - val_loss: 0.1815 - val_acc: 0.9650\n",
            "Epoch 144/1000\n",
            "1600/1600 [==============================] - 1s 696us/step - loss: 0.0064 - acc: 0.9975 - val_loss: 0.2027 - val_acc: 0.9550\n",
            "Epoch 145/1000\n",
            "1600/1600 [==============================] - 1s 683us/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.2077 - val_acc: 0.9650\n",
            "Epoch 146/1000\n",
            "1600/1600 [==============================] - 1s 697us/step - loss: 0.0078 - acc: 0.9981 - val_loss: 0.2377 - val_acc: 0.9700\n",
            "Epoch 147/1000\n",
            "1600/1600 [==============================] - 1s 680us/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.2068 - val_acc: 0.9550\n",
            "Epoch 148/1000\n",
            "1600/1600 [==============================] - 1s 715us/step - loss: 0.0067 - acc: 0.9981 - val_loss: 0.1888 - val_acc: 0.9700\n",
            "Epoch 149/1000\n",
            "1600/1600 [==============================] - 1s 697us/step - loss: 0.0038 - acc: 0.9994 - val_loss: 0.1955 - val_acc: 0.9600\n",
            "Epoch 150/1000\n",
            "1600/1600 [==============================] - 1s 694us/step - loss: 0.0045 - acc: 0.9981 - val_loss: 0.1637 - val_acc: 0.9650\n",
            "Epoch 151/1000\n",
            "1600/1600 [==============================] - 1s 684us/step - loss: 0.0055 - acc: 0.9981 - val_loss: 0.1873 - val_acc: 0.9700\n",
            "Epoch 152/1000\n",
            "1600/1600 [==============================] - 1s 693us/step - loss: 0.0043 - acc: 0.9994 - val_loss: 0.2192 - val_acc: 0.9550\n",
            "Epoch 153/1000\n",
            "1600/1600 [==============================] - 1s 685us/step - loss: 0.0068 - acc: 0.9981 - val_loss: 0.2283 - val_acc: 0.9600\n",
            "Epoch 154/1000\n",
            "1600/1600 [==============================] - 1s 684us/step - loss: 0.0052 - acc: 0.9988 - val_loss: 0.2164 - val_acc: 0.9550\n",
            "Epoch 155/1000\n",
            "1600/1600 [==============================] - 1s 687us/step - loss: 0.0079 - acc: 0.9975 - val_loss: 0.1996 - val_acc: 0.9650\n",
            "Epoch 156/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0048 - acc: 0.9994 - val_loss: 0.2045 - val_acc: 0.9700\n",
            "Epoch 157/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0086 - acc: 0.9969 - val_loss: 0.2184 - val_acc: 0.9550\n",
            "Epoch 158/1000\n",
            "1600/1600 [==============================] - 1s 716us/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.2138 - val_acc: 0.9500\n",
            "Epoch 159/1000\n",
            "1600/1600 [==============================] - 1s 686us/step - loss: 0.0083 - acc: 0.9975 - val_loss: 0.1834 - val_acc: 0.9450\n",
            "Epoch 160/1000\n",
            "1600/1600 [==============================] - 1s 687us/step - loss: 0.0091 - acc: 0.9981 - val_loss: 0.2719 - val_acc: 0.9500\n",
            "Epoch 161/1000\n",
            "1600/1600 [==============================] - 1s 696us/step - loss: 0.0111 - acc: 0.9950 - val_loss: 0.2106 - val_acc: 0.9550\n",
            "Epoch 162/1000\n",
            "1600/1600 [==============================] - 1s 686us/step - loss: 0.0094 - acc: 0.9969 - val_loss: 0.2615 - val_acc: 0.9450\n",
            "Epoch 163/1000\n",
            "1600/1600 [==============================] - 1s 691us/step - loss: 0.0087 - acc: 0.9981 - val_loss: 0.2339 - val_acc: 0.9650\n",
            "Epoch 164/1000\n",
            "1600/1600 [==============================] - 1s 696us/step - loss: 0.0188 - acc: 0.9938 - val_loss: 0.2058 - val_acc: 0.9550\n",
            "Epoch 165/1000\n",
            "1600/1600 [==============================] - 1s 699us/step - loss: 0.0232 - acc: 0.9906 - val_loss: 0.2426 - val_acc: 0.9250\n",
            "Epoch 166/1000\n",
            "1600/1600 [==============================] - 1s 697us/step - loss: 0.0255 - acc: 0.9913 - val_loss: 0.2154 - val_acc: 0.9600\n",
            "Epoch 167/1000\n",
            "1600/1600 [==============================] - 1s 689us/step - loss: 0.0246 - acc: 0.9906 - val_loss: 0.2408 - val_acc: 0.9400\n",
            "Epoch 168/1000\n",
            "1600/1600 [==============================] - 1s 705us/step - loss: 0.0180 - acc: 0.9925 - val_loss: 0.2470 - val_acc: 0.9650\n",
            "Epoch 169/1000\n",
            "1600/1600 [==============================] - 1s 709us/step - loss: 0.0249 - acc: 0.9906 - val_loss: 0.2636 - val_acc: 0.9400\n",
            "Epoch 170/1000\n",
            "1600/1600 [==============================] - 1s 698us/step - loss: 0.0272 - acc: 0.9919 - val_loss: 0.1641 - val_acc: 0.9650\n",
            "Epoch 171/1000\n",
            "1600/1600 [==============================] - 1s 707us/step - loss: 0.0558 - acc: 0.9794 - val_loss: 0.2663 - val_acc: 0.9500\n",
            "Epoch 172/1000\n",
            "1600/1600 [==============================] - 1s 690us/step - loss: 0.0341 - acc: 0.9863 - val_loss: 0.2256 - val_acc: 0.9600\n",
            "Epoch 173/1000\n",
            "1600/1600 [==============================] - 1s 700us/step - loss: 0.0352 - acc: 0.9913 - val_loss: 0.1991 - val_acc: 0.9450\n",
            "Epoch 174/1000\n",
            "1600/1600 [==============================] - 1s 700us/step - loss: 0.0210 - acc: 0.9913 - val_loss: 0.2445 - val_acc: 0.9600\n",
            "Epoch 175/1000\n",
            "1600/1600 [==============================] - 1s 700us/step - loss: 0.0130 - acc: 0.9963 - val_loss: 0.2404 - val_acc: 0.9550\n",
            "Epoch 176/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0092 - acc: 0.9975 - val_loss: 0.1951 - val_acc: 0.9650\n",
            "Epoch 177/1000\n",
            "1600/1600 [==============================] - 1s 718us/step - loss: 0.0103 - acc: 0.9963 - val_loss: 0.2609 - val_acc: 0.9550\n",
            "Epoch 178/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0127 - acc: 0.9931 - val_loss: 0.2307 - val_acc: 0.9650\n",
            "Epoch 179/1000\n",
            "1600/1600 [==============================] - 1s 697us/step - loss: 0.0200 - acc: 0.9913 - val_loss: 0.3036 - val_acc: 0.9450\n",
            "Epoch 180/1000\n",
            "1600/1600 [==============================] - 1s 708us/step - loss: 0.0149 - acc: 0.9969 - val_loss: 0.2308 - val_acc: 0.9550\n",
            "Epoch 181/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0095 - acc: 0.9969 - val_loss: 0.2286 - val_acc: 0.9650\n",
            "Epoch 182/1000\n",
            "1600/1600 [==============================] - 1s 692us/step - loss: 0.0220 - acc: 0.9925 - val_loss: 0.2571 - val_acc: 0.9550\n",
            "Epoch 183/1000\n",
            "1600/1600 [==============================] - 1s 711us/step - loss: 0.0138 - acc: 0.9975 - val_loss: 0.2006 - val_acc: 0.9650\n",
            "Epoch 184/1000\n",
            "1600/1600 [==============================] - 1s 705us/step - loss: 0.0071 - acc: 0.9981 - val_loss: 0.1700 - val_acc: 0.9650\n",
            "Epoch 185/1000\n",
            "1600/1600 [==============================] - 1s 694us/step - loss: 0.0103 - acc: 0.9963 - val_loss: 0.1537 - val_acc: 0.9800\n",
            "Epoch 186/1000\n",
            "1600/1600 [==============================] - 1s 697us/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.1881 - val_acc: 0.9600\n",
            "Epoch 187/1000\n",
            "1600/1600 [==============================] - 1s 711us/step - loss: 0.0172 - acc: 0.9925 - val_loss: 0.1689 - val_acc: 0.9600\n",
            "Epoch 188/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0151 - acc: 0.9956 - val_loss: 0.1896 - val_acc: 0.9450\n",
            "Epoch 189/1000\n",
            "1600/1600 [==============================] - 1s 700us/step - loss: 0.0203 - acc: 0.9925 - val_loss: 0.3019 - val_acc: 0.9350\n",
            "Epoch 190/1000\n",
            "1600/1600 [==============================] - 1s 692us/step - loss: 0.0172 - acc: 0.9944 - val_loss: 0.1617 - val_acc: 0.9650\n",
            "Epoch 191/1000\n",
            "1600/1600 [==============================] - 1s 694us/step - loss: 0.0136 - acc: 0.9963 - val_loss: 0.1872 - val_acc: 0.9700\n",
            "Epoch 192/1000\n",
            "1600/1600 [==============================] - 1s 706us/step - loss: 0.0092 - acc: 0.9963 - val_loss: 0.1734 - val_acc: 0.9750\n",
            "Epoch 193/1000\n",
            "1600/1600 [==============================] - 1s 712us/step - loss: 0.0053 - acc: 0.9994 - val_loss: 0.2393 - val_acc: 0.9650\n",
            "Epoch 194/1000\n",
            "1600/1600 [==============================] - 1s 728us/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.2750 - val_acc: 0.9600\n",
            "Epoch 195/1000\n",
            "1600/1600 [==============================] - 1s 717us/step - loss: 0.0051 - acc: 0.9981 - val_loss: 0.3103 - val_acc: 0.9350\n",
            "Epoch 196/1000\n",
            "1600/1600 [==============================] - 1s 767us/step - loss: 0.0062 - acc: 0.9988 - val_loss: 0.1708 - val_acc: 0.9550\n",
            "Epoch 197/1000\n",
            "1600/1600 [==============================] - 1s 720us/step - loss: 0.0066 - acc: 0.9988 - val_loss: 0.1486 - val_acc: 0.9700\n",
            "Epoch 198/1000\n",
            "1600/1600 [==============================] - 1s 728us/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.2002 - val_acc: 0.9550\n",
            "Epoch 199/1000\n",
            "1600/1600 [==============================] - 1s 735us/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.1479 - val_acc: 0.9750\n",
            "Epoch 200/1000\n",
            "1600/1600 [==============================] - 1s 724us/step - loss: 0.0074 - acc: 0.9975 - val_loss: 0.2544 - val_acc: 0.9550\n",
            "Epoch 201/1000\n",
            "1600/1600 [==============================] - 1s 724us/step - loss: 0.0094 - acc: 0.9963 - val_loss: 0.2223 - val_acc: 0.9750\n",
            "Epoch 202/1000\n",
            "1600/1600 [==============================] - 1s 712us/step - loss: 0.0149 - acc: 0.9938 - val_loss: 0.1786 - val_acc: 0.9600\n",
            "Epoch 203/1000\n",
            "1600/1600 [==============================] - 1s 704us/step - loss: 0.0107 - acc: 0.9969 - val_loss: 0.2597 - val_acc: 0.9600\n",
            "Epoch 204/1000\n",
            "1600/1600 [==============================] - 1s 691us/step - loss: 0.0079 - acc: 0.9975 - val_loss: 0.2830 - val_acc: 0.9550\n",
            "Epoch 205/1000\n",
            "1600/1600 [==============================] - 1s 742us/step - loss: 0.0049 - acc: 0.9988 - val_loss: 0.2389 - val_acc: 0.9650\n",
            "Epoch 206/1000\n",
            "1600/1600 [==============================] - 1s 699us/step - loss: 0.0076 - acc: 0.9963 - val_loss: 0.1318 - val_acc: 0.9750\n",
            "Epoch 207/1000\n",
            "1600/1600 [==============================] - 1s 704us/step - loss: 0.0049 - acc: 0.9981 - val_loss: 0.1175 - val_acc: 0.9800\n",
            "Epoch 208/1000\n",
            "1600/1600 [==============================] - 1s 706us/step - loss: 0.0102 - acc: 0.9963 - val_loss: 0.1831 - val_acc: 0.9550\n",
            "Epoch 209/1000\n",
            "1600/1600 [==============================] - 1s 710us/step - loss: 0.0037 - acc: 0.9994 - val_loss: 0.1990 - val_acc: 0.9600\n",
            "Epoch 210/1000\n",
            "1600/1600 [==============================] - 1s 707us/step - loss: 0.0051 - acc: 0.9988 - val_loss: 0.1816 - val_acc: 0.9700\n",
            "Epoch 211/1000\n",
            "1600/1600 [==============================] - 1s 708us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.1659 - val_acc: 0.9650\n",
            "Epoch 212/1000\n",
            "1600/1600 [==============================] - 1s 699us/step - loss: 0.0042 - acc: 0.9994 - val_loss: 0.1967 - val_acc: 0.9650\n",
            "Epoch 213/1000\n",
            "1600/1600 [==============================] - 1s 699us/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.1881 - val_acc: 0.9700\n",
            "Epoch 214/1000\n",
            "1600/1600 [==============================] - 1s 719us/step - loss: 0.0043 - acc: 0.9994 - val_loss: 0.1710 - val_acc: 0.9650\n",
            "Epoch 215/1000\n",
            "1600/1600 [==============================] - 1s 712us/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.2328 - val_acc: 0.9700\n",
            "Epoch 216/1000\n",
            "1600/1600 [==============================] - 1s 692us/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.2033 - val_acc: 0.9750\n",
            "Epoch 217/1000\n",
            "1600/1600 [==============================] - 1s 691us/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.1938 - val_acc: 0.9750\n",
            "Epoch 218/1000\n",
            "1600/1600 [==============================] - 1s 704us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.1795 - val_acc: 0.9700\n",
            "Epoch 219/1000\n",
            "1600/1600 [==============================] - 1s 703us/step - loss: 0.0044 - acc: 0.9994 - val_loss: 0.1476 - val_acc: 0.9750\n",
            "Epoch 220/1000\n",
            "1600/1600 [==============================] - 1s 703us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.1619 - val_acc: 0.9650\n",
            "Epoch 221/1000\n",
            "1600/1600 [==============================] - 1s 710us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.1834 - val_acc: 0.9650\n",
            "Epoch 222/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.2055 - val_acc: 0.9650\n",
            "Epoch 223/1000\n",
            "1600/1600 [==============================] - 1s 704us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.2036 - val_acc: 0.9550\n",
            "Epoch 224/1000\n",
            "1600/1600 [==============================] - 1s 715us/step - loss: 0.0101 - acc: 0.9963 - val_loss: 0.2678 - val_acc: 0.9550\n",
            "Epoch 225/1000\n",
            "1600/1600 [==============================] - 1s 717us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.2546 - val_acc: 0.9500\n",
            "Epoch 226/1000\n",
            "1600/1600 [==============================] - 1s 699us/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.2292 - val_acc: 0.9600\n",
            "Epoch 227/1000\n",
            "1600/1600 [==============================] - 1s 713us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.2430 - val_acc: 0.9650\n",
            "Epoch 228/1000\n",
            "1600/1600 [==============================] - 1s 700us/step - loss: 0.0076 - acc: 0.9988 - val_loss: 0.2537 - val_acc: 0.9650\n",
            "Epoch 229/1000\n",
            "1600/1600 [==============================] - 1s 705us/step - loss: 0.0080 - acc: 0.9981 - val_loss: 0.2284 - val_acc: 0.9600\n",
            "Epoch 230/1000\n",
            "1600/1600 [==============================] - 1s 700us/step - loss: 0.0113 - acc: 0.9963 - val_loss: 0.2475 - val_acc: 0.9600\n",
            "Epoch 231/1000\n",
            "1600/1600 [==============================] - 1s 691us/step - loss: 0.0065 - acc: 0.9981 - val_loss: 0.2014 - val_acc: 0.9600\n",
            "Epoch 232/1000\n",
            "1600/1600 [==============================] - 1s 694us/step - loss: 0.0078 - acc: 0.9969 - val_loss: 0.2341 - val_acc: 0.9650\n",
            "Epoch 233/1000\n",
            "1600/1600 [==============================] - 1s 694us/step - loss: 0.0125 - acc: 0.9950 - val_loss: 0.1946 - val_acc: 0.9650\n",
            "Epoch 234/1000\n",
            "1600/1600 [==============================] - 1s 711us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.1754 - val_acc: 0.9700\n",
            "Epoch 235/1000\n",
            "1600/1600 [==============================] - 1s 699us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.1984 - val_acc: 0.9700\n",
            "Epoch 236/1000\n",
            "1600/1600 [==============================] - 1s 707us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.1937 - val_acc: 0.9650\n",
            "Epoch 237/1000\n",
            "1600/1600 [==============================] - 1s 689us/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.1618 - val_acc: 0.9750\n",
            "Epoch 238/1000\n",
            "1600/1600 [==============================] - 1s 690us/step - loss: 0.0049 - acc: 0.9981 - val_loss: 0.1863 - val_acc: 0.9700\n",
            "Epoch 239/1000\n",
            "1600/1600 [==============================] - 1s 703us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.1605 - val_acc: 0.9700\n",
            "Epoch 240/1000\n",
            "1600/1600 [==============================] - 1s 713us/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.1781 - val_acc: 0.9700\n",
            "Epoch 241/1000\n",
            "1600/1600 [==============================] - 1s 703us/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.2267 - val_acc: 0.9650\n",
            "Epoch 242/1000\n",
            "1600/1600 [==============================] - 1s 707us/step - loss: 0.0035 - acc: 0.9981 - val_loss: 0.2537 - val_acc: 0.9650\n",
            "Epoch 243/1000\n",
            "1600/1600 [==============================] - 1s 699us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.2290 - val_acc: 0.9750\n",
            "Epoch 244/1000\n",
            "1600/1600 [==============================] - 1s 725us/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.2154 - val_acc: 0.9750\n",
            "Epoch 245/1000\n",
            "1600/1600 [==============================] - 1s 710us/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.2264 - val_acc: 0.9700\n",
            "Epoch 246/1000\n",
            "1600/1600 [==============================] - 1s 707us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.2220 - val_acc: 0.9700\n",
            "Epoch 247/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0045 - acc: 0.9981 - val_loss: 0.2178 - val_acc: 0.9700\n",
            "Epoch 248/1000\n",
            "1600/1600 [==============================] - 1s 711us/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.2120 - val_acc: 0.9650\n",
            "Epoch 249/1000\n",
            "1600/1600 [==============================] - 1s 724us/step - loss: 0.0105 - acc: 0.9969 - val_loss: 0.2696 - val_acc: 0.9550\n",
            "Epoch 250/1000\n",
            "1600/1600 [==============================] - 1s 710us/step - loss: 0.0541 - acc: 0.9850 - val_loss: 0.3978 - val_acc: 0.9450\n",
            "Epoch 251/1000\n",
            "1600/1600 [==============================] - 1s 719us/step - loss: 0.0252 - acc: 0.9900 - val_loss: 0.2203 - val_acc: 0.9500\n",
            "Epoch 252/1000\n",
            "1600/1600 [==============================] - 1s 699us/step - loss: 0.0309 - acc: 0.9881 - val_loss: 0.3777 - val_acc: 0.9250\n",
            "Epoch 253/1000\n",
            "1600/1600 [==============================] - 1s 726us/step - loss: 0.0232 - acc: 0.9925 - val_loss: 0.2203 - val_acc: 0.9600\n",
            "Epoch 254/1000\n",
            "1600/1600 [==============================] - 1s 700us/step - loss: 0.0139 - acc: 0.9950 - val_loss: 0.1743 - val_acc: 0.9750\n",
            "Epoch 255/1000\n",
            "1600/1600 [==============================] - 1s 721us/step - loss: 0.0107 - acc: 0.9969 - val_loss: 0.1725 - val_acc: 0.9700\n",
            "Epoch 256/1000\n",
            "1600/1600 [==============================] - 1s 714us/step - loss: 0.0178 - acc: 0.9944 - val_loss: 0.1506 - val_acc: 0.9700\n",
            "Epoch 257/1000\n",
            "1600/1600 [==============================] - 1s 717us/step - loss: 0.0739 - acc: 0.9819 - val_loss: 0.2392 - val_acc: 0.9350\n",
            "Epoch 258/1000\n",
            "1600/1600 [==============================] - 1s 713us/step - loss: 0.0553 - acc: 0.9844 - val_loss: 0.2127 - val_acc: 0.9450\n",
            "Epoch 259/1000\n",
            "1600/1600 [==============================] - 1s 714us/step - loss: 0.0690 - acc: 0.9775 - val_loss: 0.2385 - val_acc: 0.9500\n",
            "Epoch 260/1000\n",
            "1600/1600 [==============================] - 1s 712us/step - loss: 0.0404 - acc: 0.9900 - val_loss: 0.2039 - val_acc: 0.9550\n",
            "Epoch 261/1000\n",
            "1600/1600 [==============================] - 1s 716us/step - loss: 0.0173 - acc: 0.9944 - val_loss: 0.1936 - val_acc: 0.9700\n",
            "Epoch 262/1000\n",
            "1600/1600 [==============================] - 1s 727us/step - loss: 0.0147 - acc: 0.9969 - val_loss: 0.2509 - val_acc: 0.9600\n",
            "Epoch 263/1000\n",
            "1600/1600 [==============================] - 1s 723us/step - loss: 0.0113 - acc: 0.9963 - val_loss: 0.2188 - val_acc: 0.9550\n",
            "Epoch 264/1000\n",
            "1600/1600 [==============================] - 1s 722us/step - loss: 0.0061 - acc: 0.9988 - val_loss: 0.1878 - val_acc: 0.9700\n",
            "Epoch 265/1000\n",
            "1600/1600 [==============================] - 1s 723us/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.1963 - val_acc: 0.9700\n",
            "Epoch 266/1000\n",
            "1600/1600 [==============================] - 1s 714us/step - loss: 0.0036 - acc: 0.9994 - val_loss: 0.1782 - val_acc: 0.9700\n",
            "Epoch 267/1000\n",
            "1600/1600 [==============================] - 1s 718us/step - loss: 0.0040 - acc: 0.9994 - val_loss: 0.2179 - val_acc: 0.9650\n",
            "Epoch 268/1000\n",
            "1600/1600 [==============================] - 1s 712us/step - loss: 0.0053 - acc: 0.9981 - val_loss: 0.2622 - val_acc: 0.9600\n",
            "Epoch 269/1000\n",
            "1600/1600 [==============================] - 1s 711us/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.2381 - val_acc: 0.9550\n",
            "Epoch 270/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0058 - acc: 0.9994 - val_loss: 0.2140 - val_acc: 0.9600\n",
            "Epoch 271/1000\n",
            "1600/1600 [==============================] - 1s 734us/step - loss: 0.0035 - acc: 0.9988 - val_loss: 0.1919 - val_acc: 0.9650\n",
            "Epoch 272/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0049 - acc: 0.9981 - val_loss: 0.2446 - val_acc: 0.9600\n",
            "Epoch 273/1000\n",
            "1600/1600 [==============================] - 1s 719us/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.3689 - val_acc: 0.9350\n",
            "Epoch 274/1000\n",
            "1600/1600 [==============================] - 1s 718us/step - loss: 0.0081 - acc: 0.9969 - val_loss: 0.2603 - val_acc: 0.9550\n",
            "Epoch 275/1000\n",
            "1600/1600 [==============================] - 1s 715us/step - loss: 0.0082 - acc: 0.9956 - val_loss: 0.2596 - val_acc: 0.9650\n",
            "Epoch 276/1000\n",
            "1600/1600 [==============================] - 1s 708us/step - loss: 0.0185 - acc: 0.9950 - val_loss: 0.3985 - val_acc: 0.9450\n",
            "Epoch 277/1000\n",
            "1600/1600 [==============================] - 1s 704us/step - loss: 0.0182 - acc: 0.9944 - val_loss: 0.3287 - val_acc: 0.9300\n",
            "Epoch 278/1000\n",
            "1600/1600 [==============================] - 1s 701us/step - loss: 0.0483 - acc: 0.9813 - val_loss: 0.4073 - val_acc: 0.9150\n",
            "Epoch 279/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0443 - acc: 0.9856 - val_loss: 0.3190 - val_acc: 0.9450\n",
            "Epoch 280/1000\n",
            "1600/1600 [==============================] - 1s 700us/step - loss: 0.0326 - acc: 0.9900 - val_loss: 0.2242 - val_acc: 0.9550\n",
            "Epoch 281/1000\n",
            "1600/1600 [==============================] - 1s 725us/step - loss: 0.0224 - acc: 0.9919 - val_loss: 0.2817 - val_acc: 0.9450\n",
            "Epoch 282/1000\n",
            "1600/1600 [==============================] - 1s 698us/step - loss: 0.0118 - acc: 0.9969 - val_loss: 0.2265 - val_acc: 0.9650\n",
            "Epoch 283/1000\n",
            "1600/1600 [==============================] - 1s 693us/step - loss: 0.0062 - acc: 0.9981 - val_loss: 0.1718 - val_acc: 0.9650\n",
            "Epoch 284/1000\n",
            "1600/1600 [==============================] - 1s 724us/step - loss: 0.0083 - acc: 0.9969 - val_loss: 0.2166 - val_acc: 0.9500\n",
            "Epoch 285/1000\n",
            "1600/1600 [==============================] - 1s 705us/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.2838 - val_acc: 0.9500\n",
            "Epoch 286/1000\n",
            "1600/1600 [==============================] - 1s 708us/step - loss: 0.0063 - acc: 0.9988 - val_loss: 0.2567 - val_acc: 0.9600\n",
            "Epoch 287/1000\n",
            "1600/1600 [==============================] - 1s 706us/step - loss: 0.0072 - acc: 0.9975 - val_loss: 0.2201 - val_acc: 0.9650\n",
            "Epoch 288/1000\n",
            "1600/1600 [==============================] - 1s 718us/step - loss: 0.0068 - acc: 0.9988 - val_loss: 0.2626 - val_acc: 0.9600\n",
            "Epoch 289/1000\n",
            "1600/1600 [==============================] - 1s 712us/step - loss: 0.0080 - acc: 0.9981 - val_loss: 0.2420 - val_acc: 0.9600\n",
            "Epoch 290/1000\n",
            "1600/1600 [==============================] - 1s 730us/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.2182 - val_acc: 0.9650\n",
            "Epoch 291/1000\n",
            "1600/1600 [==============================] - 1s 710us/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.1925 - val_acc: 0.9600\n",
            "Epoch 292/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0083 - acc: 0.9975 - val_loss: 0.2242 - val_acc: 0.9650\n",
            "Epoch 293/1000\n",
            "1600/1600 [==============================] - 1s 701us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.1905 - val_acc: 0.9650\n",
            "Epoch 294/1000\n",
            "1600/1600 [==============================] - 1s 703us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.2632 - val_acc: 0.9500\n",
            "Epoch 295/1000\n",
            "1600/1600 [==============================] - 1s 701us/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.2193 - val_acc: 0.9700\n",
            "Epoch 296/1000\n",
            "1600/1600 [==============================] - 1s 704us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.2066 - val_acc: 0.9700\n",
            "Epoch 297/1000\n",
            "1600/1600 [==============================] - 1s 715us/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.1940 - val_acc: 0.9600\n",
            "Epoch 298/1000\n",
            "1600/1600 [==============================] - 1s 698us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.1459 - val_acc: 0.9750\n",
            "Epoch 299/1000\n",
            "1600/1600 [==============================] - 1s 696us/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.1509 - val_acc: 0.9750\n",
            "Epoch 300/1000\n",
            "1600/1600 [==============================] - 1s 732us/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.1714 - val_acc: 0.9650\n",
            "Epoch 301/1000\n",
            "1600/1600 [==============================] - 1s 724us/step - loss: 0.0036 - acc: 0.9981 - val_loss: 0.2036 - val_acc: 0.9750\n",
            "Epoch 302/1000\n",
            "1600/1600 [==============================] - 1s 711us/step - loss: 0.0252 - acc: 0.9956 - val_loss: 0.5294 - val_acc: 0.9400\n",
            "Epoch 303/1000\n",
            "1600/1600 [==============================] - 1s 716us/step - loss: 0.0268 - acc: 0.9888 - val_loss: 0.2567 - val_acc: 0.9600\n",
            "Epoch 304/1000\n",
            "1600/1600 [==============================] - 1s 704us/step - loss: 0.0243 - acc: 0.9913 - val_loss: 0.2093 - val_acc: 0.9550\n",
            "Epoch 305/1000\n",
            "1600/1600 [==============================] - 1s 733us/step - loss: 0.0191 - acc: 0.9938 - val_loss: 0.1584 - val_acc: 0.9750\n",
            "Epoch 306/1000\n",
            "1600/1600 [==============================] - 1s 721us/step - loss: 0.0123 - acc: 0.9950 - val_loss: 0.2015 - val_acc: 0.9600\n",
            "Epoch 307/1000\n",
            "1600/1600 [==============================] - 1s 717us/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.2551 - val_acc: 0.9500\n",
            "Epoch 308/1000\n",
            "1600/1600 [==============================] - 1s 716us/step - loss: 0.0062 - acc: 0.9981 - val_loss: 0.2122 - val_acc: 0.9500\n",
            "Epoch 309/1000\n",
            "1600/1600 [==============================] - 1s 736us/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.2256 - val_acc: 0.9600\n",
            "Epoch 310/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0040 - acc: 0.9994 - val_loss: 0.2184 - val_acc: 0.9650\n",
            "Epoch 311/1000\n",
            "1600/1600 [==============================] - 1s 714us/step - loss: 0.0053 - acc: 0.9981 - val_loss: 0.2604 - val_acc: 0.9600\n",
            "Epoch 312/1000\n",
            "1600/1600 [==============================] - 1s 728us/step - loss: 0.0058 - acc: 0.9975 - val_loss: 0.3113 - val_acc: 0.9450\n",
            "Epoch 313/1000\n",
            "1600/1600 [==============================] - 1s 707us/step - loss: 0.0043 - acc: 0.9994 - val_loss: 0.2586 - val_acc: 0.9550\n",
            "Epoch 314/1000\n",
            "1600/1600 [==============================] - 1s 711us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.2311 - val_acc: 0.9700\n",
            "Epoch 315/1000\n",
            "1600/1600 [==============================] - 1s 721us/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.2301 - val_acc: 0.9650\n",
            "Epoch 316/1000\n",
            "1600/1600 [==============================] - 1s 707us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.2538 - val_acc: 0.9700\n",
            "Epoch 317/1000\n",
            "1600/1600 [==============================] - 1s 688us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.2263 - val_acc: 0.9750\n",
            "Epoch 318/1000\n",
            "1600/1600 [==============================] - 1s 705us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.1804 - val_acc: 0.9650\n",
            "Epoch 319/1000\n",
            "1600/1600 [==============================] - 1s 730us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.1779 - val_acc: 0.9750\n",
            "Epoch 320/1000\n",
            "1600/1600 [==============================] - 1s 713us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.1837 - val_acc: 0.9750\n",
            "Epoch 321/1000\n",
            "1600/1600 [==============================] - 1s 699us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.1905 - val_acc: 0.9650\n",
            "Epoch 322/1000\n",
            "1600/1600 [==============================] - 1s 704us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.1948 - val_acc: 0.9700\n",
            "Epoch 323/1000\n",
            "1600/1600 [==============================] - 1s 727us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.2135 - val_acc: 0.9600\n",
            "Epoch 324/1000\n",
            "1600/1600 [==============================] - 1s 720us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.2315 - val_acc: 0.9600\n",
            "Epoch 325/1000\n",
            "1600/1600 [==============================] - 1s 720us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.2377 - val_acc: 0.9700\n",
            "Epoch 326/1000\n",
            "1600/1600 [==============================] - 1s 714us/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.2049 - val_acc: 0.9650\n",
            "Epoch 327/1000\n",
            "1600/1600 [==============================] - 1s 738us/step - loss: 0.0015 - acc: 0.9994 - val_loss: 0.1925 - val_acc: 0.9700\n",
            "Epoch 328/1000\n",
            "1600/1600 [==============================] - 1s 736us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.2102 - val_acc: 0.9600\n",
            "Epoch 329/1000\n",
            "1600/1600 [==============================] - 1s 709us/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.2384 - val_acc: 0.9600\n",
            "Epoch 330/1000\n",
            "1600/1600 [==============================] - 1s 707us/step - loss: 0.0064 - acc: 0.9981 - val_loss: 0.2821 - val_acc: 0.9550\n",
            "Epoch 331/1000\n",
            "1600/1600 [==============================] - 1s 694us/step - loss: 0.0047 - acc: 0.9969 - val_loss: 0.2866 - val_acc: 0.9600\n",
            "Epoch 332/1000\n",
            "1600/1600 [==============================] - 1s 698us/step - loss: 0.0092 - acc: 0.9981 - val_loss: 0.2736 - val_acc: 0.9400\n",
            "Epoch 333/1000\n",
            "1600/1600 [==============================] - 1s 706us/step - loss: 0.0075 - acc: 0.9963 - val_loss: 0.4190 - val_acc: 0.9400\n",
            "Epoch 334/1000\n",
            "1600/1600 [==============================] - 1s 714us/step - loss: 0.0088 - acc: 0.9969 - val_loss: 0.2636 - val_acc: 0.9550\n",
            "Epoch 335/1000\n",
            "1600/1600 [==============================] - 1s 714us/step - loss: 0.0199 - acc: 0.9931 - val_loss: 0.5540 - val_acc: 0.9300\n",
            "Epoch 336/1000\n",
            "1600/1600 [==============================] - 1s 691us/step - loss: 0.0103 - acc: 0.9963 - val_loss: 0.4892 - val_acc: 0.9250\n",
            "Epoch 337/1000\n",
            "1600/1600 [==============================] - 1s 720us/step - loss: 0.0222 - acc: 0.9931 - val_loss: 0.2835 - val_acc: 0.9550\n",
            "Epoch 338/1000\n",
            "1600/1600 [==============================] - 1s 698us/step - loss: 0.0318 - acc: 0.9894 - val_loss: 0.4267 - val_acc: 0.9200\n",
            "Epoch 339/1000\n",
            "1600/1600 [==============================] - 1s 694us/step - loss: 0.0265 - acc: 0.9906 - val_loss: 0.3399 - val_acc: 0.9500\n",
            "Epoch 340/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0155 - acc: 0.9950 - val_loss: 0.3764 - val_acc: 0.9300\n",
            "Epoch 341/1000\n",
            "1600/1600 [==============================] - 1s 701us/step - loss: 0.0226 - acc: 0.9938 - val_loss: 0.2844 - val_acc: 0.9400\n",
            "Epoch 342/1000\n",
            "1600/1600 [==============================] - 1s 700us/step - loss: 0.0270 - acc: 0.9900 - val_loss: 0.2385 - val_acc: 0.9500\n",
            "Epoch 343/1000\n",
            "1600/1600 [==============================] - 1s 706us/step - loss: 0.0095 - acc: 0.9963 - val_loss: 0.3181 - val_acc: 0.9350\n",
            "Epoch 344/1000\n",
            "1600/1600 [==============================] - 1s 692us/step - loss: 0.0345 - acc: 0.9913 - val_loss: 0.1848 - val_acc: 0.9700\n",
            "Epoch 345/1000\n",
            "1600/1600 [==============================] - 1s 698us/step - loss: 0.0115 - acc: 0.9969 - val_loss: 0.2162 - val_acc: 0.9550\n",
            "Epoch 346/1000\n",
            "1600/1600 [==============================] - 1s 704us/step - loss: 0.0105 - acc: 0.9963 - val_loss: 0.1994 - val_acc: 0.9550\n",
            "Epoch 347/1000\n",
            "1600/1600 [==============================] - 1s 719us/step - loss: 0.0074 - acc: 0.9981 - val_loss: 0.2446 - val_acc: 0.9650\n",
            "Epoch 348/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0108 - acc: 0.9969 - val_loss: 0.1813 - val_acc: 0.9550\n",
            "Epoch 349/1000\n",
            "1600/1600 [==============================] - 1s 731us/step - loss: 0.0070 - acc: 0.9981 - val_loss: 0.2048 - val_acc: 0.9650\n",
            "Epoch 350/1000\n",
            "1600/1600 [==============================] - 1s 710us/step - loss: 0.0049 - acc: 0.9981 - val_loss: 0.2224 - val_acc: 0.9650\n",
            "Epoch 351/1000\n",
            "1600/1600 [==============================] - 1s 690us/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.2509 - val_acc: 0.9700\n",
            "Epoch 352/1000\n",
            "1600/1600 [==============================] - 1s 715us/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.1669 - val_acc: 0.9650\n",
            "Epoch 353/1000\n",
            "1600/1600 [==============================] - 1s 735us/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.1549 - val_acc: 0.9700\n",
            "Epoch 354/1000\n",
            "1600/1600 [==============================] - 1s 719us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.1626 - val_acc: 0.9700\n",
            "Epoch 355/1000\n",
            "1600/1600 [==============================] - 1s 723us/step - loss: 0.0053 - acc: 0.9975 - val_loss: 0.1820 - val_acc: 0.9650\n",
            "Epoch 356/1000\n",
            "1600/1600 [==============================] - 1s 723us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.2078 - val_acc: 0.9600\n",
            "Epoch 357/1000\n",
            "1600/1600 [==============================] - 1s 714us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.2052 - val_acc: 0.9700\n",
            "Epoch 358/1000\n",
            "1600/1600 [==============================] - 1s 731us/step - loss: 0.0037 - acc: 0.9981 - val_loss: 0.1961 - val_acc: 0.9700\n",
            "Epoch 359/1000\n",
            "1600/1600 [==============================] - 1s 709us/step - loss: 0.0041 - acc: 0.9994 - val_loss: 0.1844 - val_acc: 0.9700\n",
            "Epoch 360/1000\n",
            "1600/1600 [==============================] - 1s 722us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.2845 - val_acc: 0.9500\n",
            "Epoch 361/1000\n",
            "1600/1600 [==============================] - 1s 717us/step - loss: 0.0053 - acc: 0.9981 - val_loss: 0.3314 - val_acc: 0.9550\n",
            "Epoch 362/1000\n",
            "1600/1600 [==============================] - 1s 712us/step - loss: 0.0103 - acc: 0.9975 - val_loss: 0.3229 - val_acc: 0.9400\n",
            "Epoch 363/1000\n",
            "1600/1600 [==============================] - 1s 710us/step - loss: 0.0230 - acc: 0.9900 - val_loss: 0.2930 - val_acc: 0.9600\n",
            "Epoch 364/1000\n",
            "1600/1600 [==============================] - 1s 706us/step - loss: 0.0127 - acc: 0.9950 - val_loss: 0.2131 - val_acc: 0.9600\n",
            "Epoch 365/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0082 - acc: 0.9981 - val_loss: 0.1862 - val_acc: 0.9700\n",
            "Epoch 366/1000\n",
            "1600/1600 [==============================] - 1s 715us/step - loss: 0.0075 - acc: 0.9988 - val_loss: 0.2576 - val_acc: 0.9500\n",
            "Epoch 367/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0091 - acc: 0.9963 - val_loss: 0.2208 - val_acc: 0.9650\n",
            "Epoch 368/1000\n",
            "1600/1600 [==============================] - 1s 722us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.1477 - val_acc: 0.9700\n",
            "Epoch 369/1000\n",
            "1600/1600 [==============================] - 1s 700us/step - loss: 0.0071 - acc: 0.9981 - val_loss: 0.1533 - val_acc: 0.9750\n",
            "Epoch 370/1000\n",
            "1600/1600 [==============================] - 1s 708us/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.3181 - val_acc: 0.9450\n",
            "Epoch 371/1000\n",
            "1600/1600 [==============================] - 1s 708us/step - loss: 0.0059 - acc: 0.9988 - val_loss: 0.2130 - val_acc: 0.9600\n",
            "Epoch 372/1000\n",
            "1600/1600 [==============================] - 1s 710us/step - loss: 0.0044 - acc: 0.9994 - val_loss: 0.1902 - val_acc: 0.9700\n",
            "Epoch 373/1000\n",
            "1600/1600 [==============================] - 1s 710us/step - loss: 0.0079 - acc: 0.9969 - val_loss: 0.2148 - val_acc: 0.9650\n",
            "Epoch 374/1000\n",
            "1600/1600 [==============================] - 1s 725us/step - loss: 0.0052 - acc: 0.9975 - val_loss: 0.2265 - val_acc: 0.9700\n",
            "Epoch 375/1000\n",
            "1600/1600 [==============================] - 1s 735us/step - loss: 0.0088 - acc: 0.9975 - val_loss: 0.1687 - val_acc: 0.9750\n",
            "Epoch 376/1000\n",
            "1600/1600 [==============================] - 1s 708us/step - loss: 0.0052 - acc: 0.9988 - val_loss: 0.1525 - val_acc: 0.9700\n",
            "Epoch 377/1000\n",
            "1600/1600 [==============================] - 1s 710us/step - loss: 0.0174 - acc: 0.9969 - val_loss: 0.2024 - val_acc: 0.9700\n",
            "Epoch 378/1000\n",
            "1600/1600 [==============================] - 1s 709us/step - loss: 0.0027 - acc: 0.9988 - val_loss: 0.1518 - val_acc: 0.9750\n",
            "Epoch 379/1000\n",
            "1600/1600 [==============================] - 1s 707us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.1810 - val_acc: 0.9700\n",
            "Epoch 380/1000\n",
            "1600/1600 [==============================] - 1s 722us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.2242 - val_acc: 0.9600\n",
            "Epoch 381/1000\n",
            "1600/1600 [==============================] - 1s 715us/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.2122 - val_acc: 0.9600\n",
            "Epoch 382/1000\n",
            "1600/1600 [==============================] - 1s 700us/step - loss: 0.0045 - acc: 0.9988 - val_loss: 0.2039 - val_acc: 0.9600\n",
            "Epoch 383/1000\n",
            "1600/1600 [==============================] - 1s 694us/step - loss: 0.0079 - acc: 0.9975 - val_loss: 0.4486 - val_acc: 0.9200\n",
            "Epoch 384/1000\n",
            "1600/1600 [==============================] - 1s 722us/step - loss: 0.0258 - acc: 0.9919 - val_loss: 0.2646 - val_acc: 0.9600\n",
            "Epoch 385/1000\n",
            "1600/1600 [==============================] - 1s 717us/step - loss: 0.0257 - acc: 0.9906 - val_loss: 0.3987 - val_acc: 0.9300\n",
            "Epoch 386/1000\n",
            "1600/1600 [==============================] - 1s 715us/step - loss: 0.0514 - acc: 0.9850 - val_loss: 0.2494 - val_acc: 0.9600\n",
            "Epoch 387/1000\n",
            "1600/1600 [==============================] - 1s 715us/step - loss: 0.0270 - acc: 0.9919 - val_loss: 0.2174 - val_acc: 0.9550\n",
            "Epoch 388/1000\n",
            "1600/1600 [==============================] - 1s 713us/step - loss: 0.0188 - acc: 0.9938 - val_loss: 0.1781 - val_acc: 0.9600\n",
            "Epoch 389/1000\n",
            "1600/1600 [==============================] - 1s 712us/step - loss: 0.0138 - acc: 0.9944 - val_loss: 0.2393 - val_acc: 0.9550\n",
            "Epoch 390/1000\n",
            "1600/1600 [==============================] - 1s 712us/step - loss: 0.0073 - acc: 0.9988 - val_loss: 0.3275 - val_acc: 0.9450\n",
            "Epoch 391/1000\n",
            "1600/1600 [==============================] - 1s 688us/step - loss: 0.0047 - acc: 0.9994 - val_loss: 0.2881 - val_acc: 0.9600\n",
            "Epoch 392/1000\n",
            "1600/1600 [==============================] - 1s 688us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.2035 - val_acc: 0.9700\n",
            "Epoch 393/1000\n",
            "1600/1600 [==============================] - 1s 692us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.2093 - val_acc: 0.9650\n",
            "Epoch 394/1000\n",
            "1600/1600 [==============================] - 1s 727us/step - loss: 0.0046 - acc: 0.9988 - val_loss: 0.2177 - val_acc: 0.9650\n",
            "Epoch 395/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.2254 - val_acc: 0.9650\n",
            "Epoch 396/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.2343 - val_acc: 0.9650\n",
            "Epoch 397/1000\n",
            "1600/1600 [==============================] - 1s 717us/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.2397 - val_acc: 0.9650\n",
            "Epoch 398/1000\n",
            "1600/1600 [==============================] - 1s 697us/step - loss: 0.0051 - acc: 0.9994 - val_loss: 0.2614 - val_acc: 0.9650\n",
            "Epoch 399/1000\n",
            "1600/1600 [==============================] - 1s 693us/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.2773 - val_acc: 0.9650\n",
            "Epoch 400/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0037 - acc: 0.9981 - val_loss: 0.2572 - val_acc: 0.9650\n",
            "Epoch 401/1000\n",
            "1600/1600 [==============================] - 1s 696us/step - loss: 0.0052 - acc: 0.9981 - val_loss: 0.2732 - val_acc: 0.9700\n",
            "Epoch 402/1000\n",
            "1600/1600 [==============================] - 1s 696us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.2687 - val_acc: 0.9700\n",
            "Epoch 403/1000\n",
            "1600/1600 [==============================] - 1s 718us/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.2271 - val_acc: 0.9700\n",
            "Epoch 404/1000\n",
            "1600/1600 [==============================] - 1s 698us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.2185 - val_acc: 0.9750\n",
            "Epoch 405/1000\n",
            "1600/1600 [==============================] - 1s 686us/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.2377 - val_acc: 0.9700\n",
            "Epoch 406/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.2504 - val_acc: 0.9700\n",
            "Epoch 407/1000\n",
            "1600/1600 [==============================] - 1s 694us/step - loss: 0.0027 - acc: 0.9988 - val_loss: 0.2441 - val_acc: 0.9700\n",
            "Epoch 408/1000\n",
            "1600/1600 [==============================] - 1s 709us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.2764 - val_acc: 0.9700\n",
            "Epoch 409/1000\n",
            "1600/1600 [==============================] - 1s 697us/step - loss: 0.0034 - acc: 0.9994 - val_loss: 0.2296 - val_acc: 0.9650\n",
            "Epoch 410/1000\n",
            "1600/1600 [==============================] - 1s 698us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.2010 - val_acc: 0.9700\n",
            "Epoch 411/1000\n",
            "1600/1600 [==============================] - 1s 694us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.2042 - val_acc: 0.9700\n",
            "Epoch 412/1000\n",
            "1600/1600 [==============================] - 1s 713us/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.2002 - val_acc: 0.9700\n",
            "Epoch 413/1000\n",
            "1600/1600 [==============================] - 1s 713us/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.2625 - val_acc: 0.9600\n",
            "Epoch 414/1000\n",
            "1600/1600 [==============================] - 1s 702us/step - loss: 0.0027 - acc: 0.9988 - val_loss: 0.2914 - val_acc: 0.9600\n",
            "Epoch 415/1000\n",
            "1600/1600 [==============================] - 1s 693us/step - loss: 0.0149 - acc: 0.9975 - val_loss: 0.3066 - val_acc: 0.9650\n",
            "Epoch 416/1000\n",
            "1600/1600 [==============================] - 1s 704us/step - loss: 0.0430 - acc: 0.9956 - val_loss: 0.2938 - val_acc: 0.9600\n",
            "Epoch 417/1000\n",
            "1600/1600 [==============================] - 1s 708us/step - loss: 0.0210 - acc: 0.9950 - val_loss: 0.2898 - val_acc: 0.9700\n",
            "Epoch 418/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0104 - acc: 0.9969 - val_loss: 0.2945 - val_acc: 0.9750\n",
            "Epoch 419/1000\n",
            "1600/1600 [==============================] - 1s 675us/step - loss: 0.0133 - acc: 0.9969 - val_loss: 0.2831 - val_acc: 0.9600\n",
            "Epoch 420/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0187 - acc: 0.9944 - val_loss: 0.2781 - val_acc: 0.9650\n",
            "Epoch 421/1000\n",
            "1600/1600 [==============================] - 1s 706us/step - loss: 0.0556 - acc: 0.9925 - val_loss: 0.2180 - val_acc: 0.9700\n",
            "Epoch 422/1000\n",
            "1600/1600 [==============================] - 1s 711us/step - loss: 0.0212 - acc: 0.9969 - val_loss: 0.2856 - val_acc: 0.9700\n",
            "Epoch 423/1000\n",
            "1600/1600 [==============================] - 1s 709us/step - loss: 0.0150 - acc: 0.9956 - val_loss: 0.2657 - val_acc: 0.9550\n",
            "Epoch 424/1000\n",
            "1600/1600 [==============================] - 1s 693us/step - loss: 0.0090 - acc: 0.9969 - val_loss: 0.3289 - val_acc: 0.9650\n",
            "Epoch 425/1000\n",
            "1600/1600 [==============================] - 1s 706us/step - loss: 0.0065 - acc: 0.9981 - val_loss: 0.3134 - val_acc: 0.9500\n",
            "Epoch 426/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0060 - acc: 0.9981 - val_loss: 0.3217 - val_acc: 0.9550\n",
            "Epoch 427/1000\n",
            "1600/1600 [==============================] - 1s 700us/step - loss: 0.0065 - acc: 0.9981 - val_loss: 0.2899 - val_acc: 0.9550\n",
            "Epoch 428/1000\n",
            "1600/1600 [==============================] - 1s 709us/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.2799 - val_acc: 0.9550\n",
            "Epoch 429/1000\n",
            "1600/1600 [==============================] - 1s 689us/step - loss: 0.0034 - acc: 0.9994 - val_loss: 0.2853 - val_acc: 0.9600\n",
            "Epoch 430/1000\n",
            "1600/1600 [==============================] - 1s 708us/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.3057 - val_acc: 0.9500\n",
            "Epoch 431/1000\n",
            "1600/1600 [==============================] - 1s 709us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.3249 - val_acc: 0.9500\n",
            "Epoch 432/1000\n",
            "1600/1600 [==============================] - 1s 692us/step - loss: 0.0014 - acc: 0.9994 - val_loss: 0.3530 - val_acc: 0.9500\n",
            "Epoch 433/1000\n",
            "1600/1600 [==============================] - 1s 698us/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.3302 - val_acc: 0.9550\n",
            "Epoch 434/1000\n",
            "1600/1600 [==============================] - 1s 687us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.2694 - val_acc: 0.9550\n",
            "Epoch 435/1000\n",
            "1600/1600 [==============================] - 1s 708us/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.2977 - val_acc: 0.9600\n",
            "Epoch 436/1000\n",
            "1600/1600 [==============================] - 1s 688us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.3271 - val_acc: 0.9700\n",
            "Epoch 437/1000\n",
            "1600/1600 [==============================] - 1s 691us/step - loss: 0.0035 - acc: 0.9988 - val_loss: 0.3353 - val_acc: 0.9500\n",
            "Epoch 438/1000\n",
            "1600/1600 [==============================] - 1s 694us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.3016 - val_acc: 0.9600\n",
            "Epoch 439/1000\n",
            "1600/1600 [==============================] - 1s 698us/step - loss: 0.0052 - acc: 0.9981 - val_loss: 0.2906 - val_acc: 0.9600\n",
            "Epoch 440/1000\n",
            "1600/1600 [==============================] - 1s 697us/step - loss: 9.8732e-04 - acc: 1.0000 - val_loss: 0.3156 - val_acc: 0.9550\n",
            "Epoch 441/1000\n",
            "1600/1600 [==============================] - 1s 719us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.2648 - val_acc: 0.9550\n",
            "Epoch 442/1000\n",
            "1600/1600 [==============================] - 1s 681us/step - loss: 0.0026 - acc: 0.9988 - val_loss: 0.2786 - val_acc: 0.9600\n",
            "Epoch 443/1000\n",
            "1600/1600 [==============================] - 1s 690us/step - loss: 0.0049 - acc: 0.9988 - val_loss: 0.3122 - val_acc: 0.9600\n",
            "Epoch 444/1000\n",
            "1600/1600 [==============================] - 1s 697us/step - loss: 0.0065 - acc: 0.9981 - val_loss: 0.2955 - val_acc: 0.9600\n",
            "Epoch 445/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.2625 - val_acc: 0.9500\n",
            "Epoch 446/1000\n",
            "1600/1600 [==============================] - 1s 692us/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.2489 - val_acc: 0.9600\n",
            "Epoch 447/1000\n",
            "1600/1600 [==============================] - 1s 701us/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.2362 - val_acc: 0.9600\n",
            "Epoch 448/1000\n",
            "1600/1600 [==============================] - 1s 690us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.2312 - val_acc: 0.9550\n",
            "Epoch 449/1000\n",
            "1600/1600 [==============================] - 1s 685us/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.2736 - val_acc: 0.9500\n",
            "Epoch 450/1000\n",
            "1600/1600 [==============================] - 1s 689us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.2950 - val_acc: 0.9550\n",
            "Epoch 451/1000\n",
            "1600/1600 [==============================] - 1s 720us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.3113 - val_acc: 0.9550\n",
            "Epoch 452/1000\n",
            "1600/1600 [==============================] - 1s 692us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.2209 - val_acc: 0.9750\n",
            "Epoch 453/1000\n",
            "1600/1600 [==============================] - 1s 707us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.2416 - val_acc: 0.9750\n",
            "Epoch 454/1000\n",
            "1600/1600 [==============================] - 1s 697us/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.2704 - val_acc: 0.9700\n",
            "Epoch 455/1000\n",
            "1600/1600 [==============================] - 1s 692us/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.2865 - val_acc: 0.9600\n",
            "Epoch 456/1000\n",
            "1600/1600 [==============================] - 1s 707us/step - loss: 0.0013 - acc: 0.9994 - val_loss: 0.3006 - val_acc: 0.9650\n",
            "Epoch 457/1000\n",
            "1600/1600 [==============================] - 1s 691us/step - loss: 0.0013 - acc: 0.9994 - val_loss: 0.2987 - val_acc: 0.9600\n",
            "Epoch 458/1000\n",
            "1600/1600 [==============================] - 1s 685us/step - loss: 7.1485e-04 - acc: 1.0000 - val_loss: 0.2832 - val_acc: 0.9600\n",
            "Epoch 459/1000\n",
            "1600/1600 [==============================] - 1s 693us/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.3336 - val_acc: 0.9650\n",
            "Epoch 460/1000\n",
            "1600/1600 [==============================] - 1s 723us/step - loss: 0.0015 - acc: 0.9994 - val_loss: 0.2964 - val_acc: 0.9500\n",
            "Epoch 461/1000\n",
            "1600/1600 [==============================] - 1s 692us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.2596 - val_acc: 0.9650\n",
            "Epoch 462/1000\n",
            "1600/1600 [==============================] - 1s 697us/step - loss: 0.0023 - acc: 0.9988 - val_loss: 0.2717 - val_acc: 0.9700\n",
            "Epoch 463/1000\n",
            "1600/1600 [==============================] - 1s 693us/step - loss: 6.8655e-04 - acc: 1.0000 - val_loss: 0.2738 - val_acc: 0.9650\n",
            "Epoch 464/1000\n",
            "1600/1600 [==============================] - 1s 696us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.2694 - val_acc: 0.9550\n",
            "Epoch 465/1000\n",
            "1600/1600 [==============================] - 1s 686us/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.2511 - val_acc: 0.9600\n",
            "Epoch 466/1000\n",
            "1600/1600 [==============================] - 1s 701us/step - loss: 0.0014 - acc: 0.9994 - val_loss: 0.2530 - val_acc: 0.9650\n",
            "Epoch 467/1000\n",
            "1600/1600 [==============================] - 1s 711us/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.2587 - val_acc: 0.9650\n",
            "Epoch 468/1000\n",
            "1600/1600 [==============================] - 1s 736us/step - loss: 0.0038 - acc: 0.9981 - val_loss: 0.2195 - val_acc: 0.9700\n",
            "Epoch 469/1000\n",
            "1600/1600 [==============================] - 1s 716us/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.2492 - val_acc: 0.9500\n",
            "Epoch 470/1000\n",
            "1600/1600 [==============================] - 1s 736us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.3131 - val_acc: 0.9650\n",
            "Epoch 471/1000\n",
            "1600/1600 [==============================] - 1s 728us/step - loss: 0.0109 - acc: 0.9963 - val_loss: 0.4222 - val_acc: 0.9450\n",
            "Epoch 472/1000\n",
            "1600/1600 [==============================] - 1s 720us/step - loss: 0.0045 - acc: 0.9994 - val_loss: 0.3180 - val_acc: 0.9550\n",
            "Epoch 473/1000\n",
            "1600/1600 [==============================] - 1s 718us/step - loss: 0.0055 - acc: 0.9975 - val_loss: 0.3093 - val_acc: 0.9450\n",
            "Epoch 474/1000\n",
            "1600/1600 [==============================] - 1s 714us/step - loss: 0.0059 - acc: 0.9988 - val_loss: 0.3650 - val_acc: 0.9450\n",
            "Epoch 475/1000\n",
            "1600/1600 [==============================] - 1s 720us/step - loss: 0.0410 - acc: 0.9900 - val_loss: 0.8519 - val_acc: 0.8850\n",
            "Epoch 476/1000\n",
            "1600/1600 [==============================] - 1s 713us/step - loss: 0.1220 - acc: 0.9681 - val_loss: 0.4448 - val_acc: 0.9250\n",
            "Epoch 477/1000\n",
            "1600/1600 [==============================] - 1s 696us/step - loss: 0.1167 - acc: 0.9600 - val_loss: 0.4606 - val_acc: 0.9050\n",
            "Epoch 478/1000\n",
            "1600/1600 [==============================] - 1s 693us/step - loss: 0.0596 - acc: 0.9763 - val_loss: 0.3331 - val_acc: 0.9500\n",
            "Epoch 479/1000\n",
            "1600/1600 [==============================] - 1s 725us/step - loss: 0.0326 - acc: 0.9875 - val_loss: 0.3517 - val_acc: 0.9500\n",
            "Epoch 480/1000\n",
            "1600/1600 [==============================] - 1s 696us/step - loss: 0.0224 - acc: 0.9925 - val_loss: 0.3163 - val_acc: 0.9500\n",
            "Epoch 481/1000\n",
            "1600/1600 [==============================] - 1s 686us/step - loss: 0.0190 - acc: 0.9944 - val_loss: 0.3391 - val_acc: 0.9500\n",
            "Epoch 482/1000\n",
            "1600/1600 [==============================] - 1s 697us/step - loss: 0.0178 - acc: 0.9944 - val_loss: 0.3429 - val_acc: 0.9350\n",
            "Epoch 483/1000\n",
            "1600/1600 [==============================] - 1s 694us/step - loss: 0.0125 - acc: 0.9956 - val_loss: 0.2315 - val_acc: 0.9500\n",
            "Epoch 484/1000\n",
            "1600/1600 [==============================] - 1s 679us/step - loss: 0.0099 - acc: 0.9963 - val_loss: 0.2883 - val_acc: 0.9550\n",
            "Epoch 485/1000\n",
            "1600/1600 [==============================] - 1s 703us/step - loss: 0.0083 - acc: 0.9969 - val_loss: 0.2299 - val_acc: 0.9650\n",
            "Epoch 486/1000\n",
            "1600/1600 [==============================] - 1s 691us/step - loss: 0.0233 - acc: 0.9931 - val_loss: 0.2768 - val_acc: 0.9500\n",
            "Epoch 487/1000\n",
            "1600/1600 [==============================] - 1s 704us/step - loss: 0.0277 - acc: 0.9913 - val_loss: 0.4598 - val_acc: 0.9350\n",
            "Epoch 488/1000\n",
            "1600/1600 [==============================] - 1s 703us/step - loss: 0.0258 - acc: 0.9900 - val_loss: 0.2855 - val_acc: 0.9500\n",
            "Epoch 489/1000\n",
            "1600/1600 [==============================] - 1s 709us/step - loss: 0.0186 - acc: 0.9944 - val_loss: 0.3206 - val_acc: 0.9350\n",
            "Epoch 490/1000\n",
            "1600/1600 [==============================] - 1s 695us/step - loss: 0.0311 - acc: 0.9913 - val_loss: 0.2873 - val_acc: 0.9450\n",
            "Epoch 491/1000\n",
            "1600/1600 [==============================] - 1s 696us/step - loss: 0.0334 - acc: 0.9906 - val_loss: 0.3864 - val_acc: 0.9250\n",
            "Epoch 492/1000\n",
            "1600/1600 [==============================] - 1s 709us/step - loss: 0.0147 - acc: 0.9956 - val_loss: 0.2958 - val_acc: 0.9450\n",
            "Epoch 493/1000\n",
            "1600/1600 [==============================] - 1s 714us/step - loss: 0.0160 - acc: 0.9956 - val_loss: 0.2105 - val_acc: 0.9500\n",
            "Epoch 494/1000\n",
            "1400/1600 [=========================>....] - ETA: 0s - loss: 0.0122 - acc: 0.9957"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5db7a55d4305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWmz5DLLmKSk",
        "colab_type": "code",
        "outputId": "60fb9a15-177f-485b-85d6-edfe5bf2db13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(test_data, testLabels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7434/7434 [==============================] - 3s 342us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5402073061524465, 0.9031476997578692]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vi1hkx2r-iw",
        "colab_type": "code",
        "outputId": "fce32f1b-f375-41fc-c5dc-4dafbf44e833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(val_data, valLabels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 0s 389us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3044787368364632, 0.95]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS6Ed2K5s12A",
        "colab_type": "code",
        "outputId": "14d77767-0ae5-4ceb-8cba-960dd3c4e29a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(train_data, trainLabels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1600/1600 [==============================] - 1s 340us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.004327859341601652, 0.998125]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-38J-2QNs4TX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Dense, Lambda, Flatten, Input, Conv2D, Conv1D, Reshape, Concatenate, Dropout\n",
        "\n",
        "inp = Input(shape=[10, 22, 3*3])\n",
        "out = []\n",
        "for i in range(10):\n",
        "    reshape1 = Lambda(lambda x: x[:,i,:,:])(inp)  \n",
        "    conv1 = Conv1D(filters = 20, kernel_size = 3, strides = 1, activation=\"relu\")(reshape1)\n",
        "    conv2 = Conv1D(filters = 20, kernel_size = 3, strides = 1, activation=\"relu\")(conv1)\n",
        "    conv3 = Conv1D(filters = 10, kernel_size = 3, strides = 1, activation=\"relu\")(conv2)\n",
        "    conv4 = Conv1D(filters = 5, kernel_size = 5, strides = 1, activation=\"relu\")(conv3)\n",
        "    reshape2 = Reshape((60, 1), input_shape = conv4.shape)(conv4)\n",
        "    out.append(reshape2)\n",
        "outConc = keras.layers.concatenate(out)\n",
        "parallelModel = Model(inputs=[inp], outputs=outConc)\n",
        "\n",
        "net = Sequential()\n",
        "net.add(Conv2D(220, kernel_size = 1, activation=\"relu\", data_format=\"channels_first\"))\n",
        "net.add(Reshape((10, 22, 3*3)))\n",
        "net.add(parallelModel)\n",
        "net.add(Flatten())\n",
        "net.add(Dense(100, activation='relu'))\n",
        "net.add(Dropout(0.5))\n",
        "net.add(Dense(9, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv4d9jsltJNS",
        "colab_type": "code",
        "outputId": "df1b25a7-d0f1-42a0-db2c-3575607ac9a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import adam\n",
        "\n",
        "ad = adam(lr=0.0005)\n",
        "net.compile(optimizer=ad, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "net.fit(train_data, trainLabels, validation_data=(val_data, valLabels), epochs=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1600 samples, validate on 200 samples\n",
            "Epoch 1/1000\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 2.0666 - acc: 0.2700 - val_loss: 1.6555 - val_acc: 0.3800\n",
            "Epoch 2/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 1.5063 - acc: 0.3719 - val_loss: 1.2090 - val_acc: 0.4800\n",
            "Epoch 3/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 1.2355 - acc: 0.4875 - val_loss: 1.0566 - val_acc: 0.5350\n",
            "Epoch 4/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 1.1206 - acc: 0.5275 - val_loss: 1.0042 - val_acc: 0.5450\n",
            "Epoch 5/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 1.0344 - acc: 0.5556 - val_loss: 0.9403 - val_acc: 0.5950\n",
            "Epoch 6/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.9799 - acc: 0.5969 - val_loss: 0.8893 - val_acc: 0.6500\n",
            "Epoch 7/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.9083 - acc: 0.6231 - val_loss: 0.8686 - val_acc: 0.6600\n",
            "Epoch 8/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.8840 - acc: 0.6506 - val_loss: 0.9009 - val_acc: 0.6000\n",
            "Epoch 9/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.8220 - acc: 0.6656 - val_loss: 0.7525 - val_acc: 0.6900\n",
            "Epoch 10/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.7676 - acc: 0.7000 - val_loss: 0.7361 - val_acc: 0.6800\n",
            "Epoch 11/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.7538 - acc: 0.7137 - val_loss: 0.6748 - val_acc: 0.7300\n",
            "Epoch 12/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.6771 - acc: 0.7425 - val_loss: 0.6631 - val_acc: 0.7450\n",
            "Epoch 13/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.6550 - acc: 0.7569 - val_loss: 0.6300 - val_acc: 0.7450\n",
            "Epoch 14/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.6260 - acc: 0.7519 - val_loss: 0.6085 - val_acc: 0.7500\n",
            "Epoch 15/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.5916 - acc: 0.7819 - val_loss: 0.6196 - val_acc: 0.7450\n",
            "Epoch 16/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.5598 - acc: 0.7919 - val_loss: 0.5649 - val_acc: 0.7950\n",
            "Epoch 17/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.5422 - acc: 0.7925 - val_loss: 0.5317 - val_acc: 0.8000\n",
            "Epoch 18/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.5234 - acc: 0.7994 - val_loss: 0.5056 - val_acc: 0.8300\n",
            "Epoch 19/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4743 - acc: 0.8263 - val_loss: 0.5139 - val_acc: 0.8100\n",
            "Epoch 20/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4502 - acc: 0.8263 - val_loss: 0.4643 - val_acc: 0.8200\n",
            "Epoch 21/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4573 - acc: 0.8269 - val_loss: 0.4801 - val_acc: 0.8250\n",
            "Epoch 22/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4426 - acc: 0.8306 - val_loss: 0.4595 - val_acc: 0.8000\n",
            "Epoch 23/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4236 - acc: 0.8406 - val_loss: 0.4763 - val_acc: 0.8250\n",
            "Epoch 24/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4169 - acc: 0.8400 - val_loss: 0.4168 - val_acc: 0.8400\n",
            "Epoch 25/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3864 - acc: 0.8675 - val_loss: 0.4130 - val_acc: 0.8500\n",
            "Epoch 26/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4313 - acc: 0.8419 - val_loss: 0.4527 - val_acc: 0.8300\n",
            "Epoch 27/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3855 - acc: 0.8569 - val_loss: 0.4897 - val_acc: 0.8000\n",
            "Epoch 28/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3445 - acc: 0.8669 - val_loss: 0.4041 - val_acc: 0.8400\n",
            "Epoch 29/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3478 - acc: 0.8756 - val_loss: 0.4594 - val_acc: 0.8100\n",
            "Epoch 30/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3288 - acc: 0.8869 - val_loss: 0.4103 - val_acc: 0.8500\n",
            "Epoch 31/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3317 - acc: 0.8812 - val_loss: 0.4028 - val_acc: 0.8400\n",
            "Epoch 32/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3297 - acc: 0.8825 - val_loss: 0.4355 - val_acc: 0.8200\n",
            "Epoch 33/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3107 - acc: 0.8931 - val_loss: 0.3616 - val_acc: 0.8550\n",
            "Epoch 34/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3039 - acc: 0.8875 - val_loss: 0.3903 - val_acc: 0.8500\n",
            "Epoch 35/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2999 - acc: 0.8925 - val_loss: 0.3730 - val_acc: 0.8650\n",
            "Epoch 36/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2781 - acc: 0.9025 - val_loss: 0.3447 - val_acc: 0.8700\n",
            "Epoch 37/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2651 - acc: 0.9100 - val_loss: 0.3813 - val_acc: 0.8850\n",
            "Epoch 38/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2813 - acc: 0.8975 - val_loss: 0.3867 - val_acc: 0.8700\n",
            "Epoch 39/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2576 - acc: 0.9025 - val_loss: 0.3293 - val_acc: 0.8550\n",
            "Epoch 40/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2411 - acc: 0.9113 - val_loss: 0.3496 - val_acc: 0.8750\n",
            "Epoch 41/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2523 - acc: 0.9069 - val_loss: 0.3167 - val_acc: 0.8950\n",
            "Epoch 42/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2349 - acc: 0.9156 - val_loss: 0.3222 - val_acc: 0.8750\n",
            "Epoch 43/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2223 - acc: 0.9200 - val_loss: 0.2977 - val_acc: 0.8950\n",
            "Epoch 44/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2541 - acc: 0.9094 - val_loss: 0.4032 - val_acc: 0.8500\n",
            "Epoch 45/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2609 - acc: 0.9075 - val_loss: 0.2904 - val_acc: 0.8800\n",
            "Epoch 46/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2228 - acc: 0.9194 - val_loss: 0.3888 - val_acc: 0.8550\n",
            "Epoch 47/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2061 - acc: 0.9256 - val_loss: 0.2983 - val_acc: 0.8850\n",
            "Epoch 48/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1944 - acc: 0.9231 - val_loss: 0.3310 - val_acc: 0.8750\n",
            "Epoch 49/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2075 - acc: 0.9231 - val_loss: 0.3653 - val_acc: 0.8800\n",
            "Epoch 50/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2043 - acc: 0.9237 - val_loss: 0.2665 - val_acc: 0.9100\n",
            "Epoch 51/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1861 - acc: 0.9350 - val_loss: 0.2956 - val_acc: 0.9050\n",
            "Epoch 52/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1833 - acc: 0.9381 - val_loss: 0.2603 - val_acc: 0.9100\n",
            "Epoch 53/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1870 - acc: 0.9281 - val_loss: 0.3406 - val_acc: 0.8850\n",
            "Epoch 54/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1823 - acc: 0.9400 - val_loss: 0.2503 - val_acc: 0.9100\n",
            "Epoch 55/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1751 - acc: 0.9375 - val_loss: 0.4519 - val_acc: 0.8600\n",
            "Epoch 56/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1639 - acc: 0.9431 - val_loss: 0.2634 - val_acc: 0.9150\n",
            "Epoch 57/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1495 - acc: 0.9481 - val_loss: 0.2746 - val_acc: 0.9050\n",
            "Epoch 58/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1597 - acc: 0.9406 - val_loss: 0.2828 - val_acc: 0.8800\n",
            "Epoch 59/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1750 - acc: 0.9356 - val_loss: 0.2386 - val_acc: 0.9200\n",
            "Epoch 60/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1743 - acc: 0.9412 - val_loss: 0.2262 - val_acc: 0.9250\n",
            "Epoch 61/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1539 - acc: 0.9450 - val_loss: 0.2190 - val_acc: 0.9050\n",
            "Epoch 62/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1297 - acc: 0.9519 - val_loss: 0.2483 - val_acc: 0.9200\n",
            "Epoch 63/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1224 - acc: 0.9550 - val_loss: 0.2191 - val_acc: 0.9100\n",
            "Epoch 64/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1353 - acc: 0.9513 - val_loss: 0.2441 - val_acc: 0.9050\n",
            "Epoch 65/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1196 - acc: 0.9550 - val_loss: 0.2344 - val_acc: 0.9150\n",
            "Epoch 66/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1382 - acc: 0.9556 - val_loss: 0.2437 - val_acc: 0.9200\n",
            "Epoch 67/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1286 - acc: 0.9525 - val_loss: 0.2329 - val_acc: 0.9200\n",
            "Epoch 68/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1245 - acc: 0.9531 - val_loss: 0.2194 - val_acc: 0.9100\n",
            "Epoch 69/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1121 - acc: 0.9619 - val_loss: 0.2789 - val_acc: 0.9000\n",
            "Epoch 70/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1741 - acc: 0.9412 - val_loss: 0.2645 - val_acc: 0.9050\n",
            "Epoch 71/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1614 - acc: 0.9387 - val_loss: 0.2080 - val_acc: 0.9100\n",
            "Epoch 72/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0959 - acc: 0.9681 - val_loss: 0.2027 - val_acc: 0.9150\n",
            "Epoch 73/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0988 - acc: 0.9656 - val_loss: 0.2319 - val_acc: 0.9150\n",
            "Epoch 74/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1040 - acc: 0.9637 - val_loss: 0.1870 - val_acc: 0.9150\n",
            "Epoch 75/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1127 - acc: 0.9537 - val_loss: 0.2828 - val_acc: 0.9000\n",
            "Epoch 76/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1115 - acc: 0.9650 - val_loss: 0.1916 - val_acc: 0.9200\n",
            "Epoch 77/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1027 - acc: 0.9637 - val_loss: 0.1795 - val_acc: 0.9250\n",
            "Epoch 78/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1051 - acc: 0.9631 - val_loss: 0.1694 - val_acc: 0.9300\n",
            "Epoch 79/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0857 - acc: 0.9725 - val_loss: 0.2391 - val_acc: 0.9150\n",
            "Epoch 80/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1064 - acc: 0.9650 - val_loss: 0.1698 - val_acc: 0.9250\n",
            "Epoch 81/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0889 - acc: 0.9675 - val_loss: 0.2418 - val_acc: 0.9100\n",
            "Epoch 82/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1166 - acc: 0.9562 - val_loss: 0.2519 - val_acc: 0.8950\n",
            "Epoch 83/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1059 - acc: 0.9594 - val_loss: 0.2184 - val_acc: 0.9200\n",
            "Epoch 84/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0871 - acc: 0.9694 - val_loss: 0.3063 - val_acc: 0.9050\n",
            "Epoch 85/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0968 - acc: 0.9650 - val_loss: 0.2020 - val_acc: 0.9200\n",
            "Epoch 86/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0815 - acc: 0.9719 - val_loss: 0.2087 - val_acc: 0.9150\n",
            "Epoch 87/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0615 - acc: 0.9788 - val_loss: 0.2150 - val_acc: 0.9150\n",
            "Epoch 88/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1057 - acc: 0.9594 - val_loss: 0.2508 - val_acc: 0.9300\n",
            "Epoch 89/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1038 - acc: 0.9650 - val_loss: 0.1676 - val_acc: 0.9300\n",
            "Epoch 90/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0860 - acc: 0.9681 - val_loss: 0.2362 - val_acc: 0.9150\n",
            "Epoch 91/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0690 - acc: 0.9750 - val_loss: 0.1798 - val_acc: 0.9300\n",
            "Epoch 92/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0757 - acc: 0.9706 - val_loss: 0.2121 - val_acc: 0.9150\n",
            "Epoch 93/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0651 - acc: 0.9794 - val_loss: 0.2881 - val_acc: 0.9100\n",
            "Epoch 94/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0600 - acc: 0.9794 - val_loss: 0.1938 - val_acc: 0.9250\n",
            "Epoch 95/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0698 - acc: 0.9750 - val_loss: 0.1582 - val_acc: 0.9350\n",
            "Epoch 96/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0831 - acc: 0.9725 - val_loss: 0.2497 - val_acc: 0.9400\n",
            "Epoch 97/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0561 - acc: 0.9781 - val_loss: 0.2261 - val_acc: 0.9300\n",
            "Epoch 98/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0744 - acc: 0.9712 - val_loss: 0.1973 - val_acc: 0.9500\n",
            "Epoch 99/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0653 - acc: 0.9744 - val_loss: 0.1650 - val_acc: 0.9400\n",
            "Epoch 100/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0467 - acc: 0.9825 - val_loss: 0.2266 - val_acc: 0.9300\n",
            "Epoch 101/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0593 - acc: 0.9756 - val_loss: 0.2457 - val_acc: 0.9200\n",
            "Epoch 102/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0679 - acc: 0.9794 - val_loss: 0.1917 - val_acc: 0.9400\n",
            "Epoch 103/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0445 - acc: 0.9838 - val_loss: 0.1583 - val_acc: 0.9450\n",
            "Epoch 104/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1411 - acc: 0.9575 - val_loss: 0.1610 - val_acc: 0.9300\n",
            "Epoch 105/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0726 - acc: 0.9731 - val_loss: 0.1242 - val_acc: 0.9450\n",
            "Epoch 106/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0384 - acc: 0.9894 - val_loss: 0.2720 - val_acc: 0.9100\n",
            "Epoch 107/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0537 - acc: 0.9806 - val_loss: 0.2374 - val_acc: 0.9300\n",
            "Epoch 108/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0859 - acc: 0.9675 - val_loss: 0.1773 - val_acc: 0.9400\n",
            "Epoch 109/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0780 - acc: 0.9725 - val_loss: 0.2097 - val_acc: 0.9250\n",
            "Epoch 110/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0486 - acc: 0.9806 - val_loss: 0.2306 - val_acc: 0.9400\n",
            "Epoch 111/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0388 - acc: 0.9888 - val_loss: 0.2303 - val_acc: 0.9350\n",
            "Epoch 112/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0421 - acc: 0.9856 - val_loss: 0.1995 - val_acc: 0.9250\n",
            "Epoch 113/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0756 - acc: 0.9700 - val_loss: 0.1710 - val_acc: 0.9550\n",
            "Epoch 114/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0754 - acc: 0.9775 - val_loss: 0.1869 - val_acc: 0.9300\n",
            "Epoch 115/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0865 - acc: 0.9694 - val_loss: 0.2243 - val_acc: 0.9200\n",
            "Epoch 116/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0516 - acc: 0.9806 - val_loss: 0.2705 - val_acc: 0.9250\n",
            "Epoch 117/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0365 - acc: 0.9850 - val_loss: 0.1950 - val_acc: 0.9400\n",
            "Epoch 118/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0389 - acc: 0.9831 - val_loss: 0.2056 - val_acc: 0.9300\n",
            "Epoch 119/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0471 - acc: 0.9831 - val_loss: 0.2449 - val_acc: 0.9300\n",
            "Epoch 120/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0509 - acc: 0.9806 - val_loss: 0.1668 - val_acc: 0.9400\n",
            "Epoch 121/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0648 - acc: 0.9775 - val_loss: 0.1696 - val_acc: 0.9450\n",
            "Epoch 122/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0703 - acc: 0.9762 - val_loss: 0.1396 - val_acc: 0.9300\n",
            "Epoch 123/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0490 - acc: 0.9806 - val_loss: 0.2273 - val_acc: 0.9350\n",
            "Epoch 124/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0407 - acc: 0.9862 - val_loss: 0.1931 - val_acc: 0.9450\n",
            "Epoch 125/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0582 - acc: 0.9775 - val_loss: 0.3589 - val_acc: 0.9050\n",
            "Epoch 126/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0650 - acc: 0.9806 - val_loss: 0.1822 - val_acc: 0.9450\n",
            "Epoch 127/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0567 - acc: 0.9806 - val_loss: 0.1611 - val_acc: 0.9350\n",
            "Epoch 128/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0326 - acc: 0.9900 - val_loss: 0.1396 - val_acc: 0.9500\n",
            "Epoch 129/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0261 - acc: 0.9906 - val_loss: 0.1542 - val_acc: 0.9450\n",
            "Epoch 130/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0314 - acc: 0.9900 - val_loss: 0.1577 - val_acc: 0.9450\n",
            "Epoch 131/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0553 - acc: 0.9806 - val_loss: 0.2142 - val_acc: 0.9350\n",
            "Epoch 132/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0371 - acc: 0.9875 - val_loss: 0.1274 - val_acc: 0.9600\n",
            "Epoch 133/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0200 - acc: 0.9944 - val_loss: 0.1966 - val_acc: 0.9400\n",
            "Epoch 134/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0293 - acc: 0.9912 - val_loss: 0.1950 - val_acc: 0.9450\n",
            "Epoch 135/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0266 - acc: 0.9912 - val_loss: 0.2225 - val_acc: 0.9300\n",
            "Epoch 136/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0359 - acc: 0.9869 - val_loss: 0.2544 - val_acc: 0.9250\n",
            "Epoch 137/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0323 - acc: 0.9894 - val_loss: 0.1859 - val_acc: 0.9400\n",
            "Epoch 138/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0380 - acc: 0.9862 - val_loss: 0.2925 - val_acc: 0.9300\n",
            "Epoch 139/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0416 - acc: 0.9869 - val_loss: 0.2282 - val_acc: 0.9200\n",
            "Epoch 140/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0633 - acc: 0.9819 - val_loss: 0.3088 - val_acc: 0.9100\n",
            "Epoch 141/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0706 - acc: 0.9769 - val_loss: 0.2150 - val_acc: 0.9350\n",
            "Epoch 142/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0360 - acc: 0.9894 - val_loss: 0.2097 - val_acc: 0.9400\n",
            "Epoch 143/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0336 - acc: 0.9862 - val_loss: 0.1879 - val_acc: 0.9350\n",
            "Epoch 144/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0277 - acc: 0.9906 - val_loss: 0.1235 - val_acc: 0.9500\n",
            "Epoch 145/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0235 - acc: 0.9925 - val_loss: 0.4949 - val_acc: 0.8950\n",
            "Epoch 146/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0899 - acc: 0.9688 - val_loss: 0.3064 - val_acc: 0.9100\n",
            "Epoch 147/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0503 - acc: 0.9831 - val_loss: 0.1754 - val_acc: 0.9550\n",
            "Epoch 148/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0320 - acc: 0.9919 - val_loss: 0.2029 - val_acc: 0.9450\n",
            "Epoch 149/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0234 - acc: 0.9931 - val_loss: 0.1626 - val_acc: 0.9450\n",
            "Epoch 150/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0372 - acc: 0.9875 - val_loss: 0.1636 - val_acc: 0.9350\n",
            "Epoch 151/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0572 - acc: 0.9844 - val_loss: 0.2083 - val_acc: 0.9450\n",
            "Epoch 152/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0366 - acc: 0.9888 - val_loss: 0.1454 - val_acc: 0.9500\n",
            "Epoch 153/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0294 - acc: 0.9906 - val_loss: 0.1693 - val_acc: 0.9300\n",
            "Epoch 154/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0229 - acc: 0.9912 - val_loss: 0.2790 - val_acc: 0.9350\n",
            "Epoch 155/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0169 - acc: 0.9944 - val_loss: 0.1495 - val_acc: 0.9500\n",
            "Epoch 156/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0415 - acc: 0.9869 - val_loss: 0.1479 - val_acc: 0.9500\n",
            "Epoch 157/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1546 - acc: 0.9569 - val_loss: 0.1779 - val_acc: 0.9250\n",
            "Epoch 158/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0378 - acc: 0.9900 - val_loss: 0.1142 - val_acc: 0.9650\n",
            "Epoch 159/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0251 - acc: 0.9906 - val_loss: 0.1722 - val_acc: 0.9500\n",
            "Epoch 160/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0221 - acc: 0.9944 - val_loss: 0.1533 - val_acc: 0.9600\n",
            "Epoch 161/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0174 - acc: 0.9938 - val_loss: 0.3080 - val_acc: 0.9300\n",
            "Epoch 162/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0302 - acc: 0.9888 - val_loss: 0.1519 - val_acc: 0.9400\n",
            "Epoch 163/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0449 - acc: 0.9844 - val_loss: 0.2582 - val_acc: 0.9300\n",
            "Epoch 164/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0551 - acc: 0.9844 - val_loss: 0.2784 - val_acc: 0.9300\n",
            "Epoch 165/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0528 - acc: 0.9825 - val_loss: 0.2002 - val_acc: 0.9150\n",
            "Epoch 166/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0256 - acc: 0.9906 - val_loss: 0.1892 - val_acc: 0.9500\n",
            "Epoch 167/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0273 - acc: 0.9875 - val_loss: 0.1305 - val_acc: 0.9600\n",
            "Epoch 168/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0205 - acc: 0.9944 - val_loss: 0.1592 - val_acc: 0.9400\n",
            "Epoch 169/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0126 - acc: 0.9963 - val_loss: 0.1804 - val_acc: 0.9450\n",
            "Epoch 170/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0181 - acc: 0.9938 - val_loss: 0.2375 - val_acc: 0.9500\n",
            "Epoch 171/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0138 - acc: 0.9944 - val_loss: 0.2423 - val_acc: 0.9450\n",
            "Epoch 172/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0126 - acc: 0.9963 - val_loss: 0.2420 - val_acc: 0.9350\n",
            "Epoch 173/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0194 - acc: 0.9931 - val_loss: 0.2254 - val_acc: 0.9400\n",
            "Epoch 174/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0499 - acc: 0.9825 - val_loss: 0.2851 - val_acc: 0.9100\n",
            "Epoch 175/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0635 - acc: 0.9781 - val_loss: 0.1426 - val_acc: 0.9550\n",
            "Epoch 176/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0468 - acc: 0.9825 - val_loss: 0.2478 - val_acc: 0.9350\n",
            "Epoch 177/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0340 - acc: 0.9881 - val_loss: 0.2265 - val_acc: 0.9450\n",
            "Epoch 178/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0247 - acc: 0.9888 - val_loss: 0.2043 - val_acc: 0.9450\n",
            "Epoch 179/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0183 - acc: 0.9925 - val_loss: 0.1738 - val_acc: 0.9500\n",
            "Epoch 180/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0192 - acc: 0.9925 - val_loss: 0.2092 - val_acc: 0.9500\n",
            "Epoch 181/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0678 - acc: 0.9781 - val_loss: 0.3253 - val_acc: 0.9100\n",
            "Epoch 182/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0839 - acc: 0.9681 - val_loss: 0.2116 - val_acc: 0.9450\n",
            "Epoch 183/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0232 - acc: 0.9912 - val_loss: 0.3994 - val_acc: 0.9200\n",
            "Epoch 184/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0300 - acc: 0.9906 - val_loss: 0.1891 - val_acc: 0.9500\n",
            "Epoch 185/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0169 - acc: 0.9956 - val_loss: 0.2185 - val_acc: 0.9500\n",
            "Epoch 186/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0141 - acc: 0.9956 - val_loss: 0.3100 - val_acc: 0.9350\n",
            "Epoch 187/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0147 - acc: 0.9956 - val_loss: 0.2758 - val_acc: 0.9350\n",
            "Epoch 188/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0137 - acc: 0.9956 - val_loss: 0.2814 - val_acc: 0.9250\n",
            "Epoch 189/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0445 - acc: 0.9844 - val_loss: 0.1438 - val_acc: 0.9600\n",
            "Epoch 190/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0334 - acc: 0.9894 - val_loss: 0.2016 - val_acc: 0.9400\n",
            "Epoch 191/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0227 - acc: 0.9938 - val_loss: 0.2514 - val_acc: 0.9400\n",
            "Epoch 192/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0193 - acc: 0.9963 - val_loss: 0.1803 - val_acc: 0.9400\n",
            "Epoch 193/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0237 - acc: 0.9906 - val_loss: 0.2847 - val_acc: 0.9350\n",
            "Epoch 194/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0285 - acc: 0.9856 - val_loss: 0.1880 - val_acc: 0.9450\n",
            "Epoch 195/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0449 - acc: 0.9869 - val_loss: 0.2123 - val_acc: 0.9450\n",
            "Epoch 196/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0281 - acc: 0.9900 - val_loss: 0.1967 - val_acc: 0.9350\n",
            "Epoch 197/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0270 - acc: 0.9925 - val_loss: 0.2630 - val_acc: 0.9300\n",
            "Epoch 198/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0313 - acc: 0.9906 - val_loss: 0.3325 - val_acc: 0.9200\n",
            "Epoch 199/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0254 - acc: 0.9912 - val_loss: 0.1534 - val_acc: 0.9550\n",
            "Epoch 200/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0119 - acc: 0.9969 - val_loss: 0.2292 - val_acc: 0.9350\n",
            "Epoch 201/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0111 - acc: 0.9963 - val_loss: 0.2917 - val_acc: 0.9400\n",
            "Epoch 202/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0252 - acc: 0.9912 - val_loss: 0.2539 - val_acc: 0.9400\n",
            "Epoch 203/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0293 - acc: 0.9900 - val_loss: 0.3031 - val_acc: 0.9350\n",
            "Epoch 204/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0969 - acc: 0.9712 - val_loss: 0.3950 - val_acc: 0.9350\n",
            "Epoch 205/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0423 - acc: 0.9862 - val_loss: 0.1915 - val_acc: 0.9450\n",
            "Epoch 206/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0185 - acc: 0.9925 - val_loss: 0.1328 - val_acc: 0.9600\n",
            "Epoch 207/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0119 - acc: 0.9981 - val_loss: 0.1778 - val_acc: 0.9500\n",
            "Epoch 208/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0159 - acc: 0.9925 - val_loss: 0.1601 - val_acc: 0.9500\n",
            "Epoch 209/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0083 - acc: 0.9969 - val_loss: 0.1826 - val_acc: 0.9450\n",
            "Epoch 210/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0070 - acc: 0.9994 - val_loss: 0.1810 - val_acc: 0.9500\n",
            "Epoch 211/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0052 - acc: 0.9981 - val_loss: 0.2365 - val_acc: 0.9450\n",
            "Epoch 212/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0158 - acc: 0.9950 - val_loss: 0.2000 - val_acc: 0.9500\n",
            "Epoch 213/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0147 - acc: 0.9950 - val_loss: 0.2418 - val_acc: 0.9450\n",
            "Epoch 214/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0214 - acc: 0.9938 - val_loss: 0.2632 - val_acc: 0.9250\n",
            "Epoch 215/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0129 - acc: 0.9956 - val_loss: 0.2813 - val_acc: 0.9550\n",
            "Epoch 216/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0420 - acc: 0.9875 - val_loss: 0.2910 - val_acc: 0.9200\n",
            "Epoch 217/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0487 - acc: 0.9825 - val_loss: 0.2098 - val_acc: 0.9450\n",
            "Epoch 218/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0172 - acc: 0.9975 - val_loss: 0.4182 - val_acc: 0.9100\n",
            "Epoch 219/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0290 - acc: 0.9900 - val_loss: 0.3431 - val_acc: 0.9100\n",
            "Epoch 220/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0370 - acc: 0.9869 - val_loss: 0.3956 - val_acc: 0.9250\n",
            "Epoch 221/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0177 - acc: 0.9950 - val_loss: 0.3438 - val_acc: 0.9350\n",
            "Epoch 222/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0126 - acc: 0.9969 - val_loss: 0.2627 - val_acc: 0.9400\n",
            "Epoch 223/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0144 - acc: 0.9969 - val_loss: 0.2959 - val_acc: 0.9400\n",
            "Epoch 224/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0423 - acc: 0.9869 - val_loss: 0.2346 - val_acc: 0.9600\n",
            "Epoch 225/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0261 - acc: 0.9919 - val_loss: 0.3000 - val_acc: 0.9400\n",
            "Epoch 226/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0151 - acc: 0.9950 - val_loss: 0.3897 - val_acc: 0.9250\n",
            "Epoch 227/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0416 - acc: 0.9862 - val_loss: 0.2242 - val_acc: 0.9450\n",
            "Epoch 228/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0307 - acc: 0.9875 - val_loss: 0.2680 - val_acc: 0.9450\n",
            "Epoch 229/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0108 - acc: 0.9956 - val_loss: 0.2945 - val_acc: 0.9300\n",
            "Epoch 230/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0292 - acc: 0.9912 - val_loss: 0.2963 - val_acc: 0.9350\n",
            "Epoch 231/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0155 - acc: 0.9938 - val_loss: 0.2271 - val_acc: 0.9500\n",
            "Epoch 232/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0153 - acc: 0.9950 - val_loss: 0.2907 - val_acc: 0.9450\n",
            "Epoch 233/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0469 - acc: 0.9825 - val_loss: 0.2604 - val_acc: 0.9450\n",
            "Epoch 234/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0153 - acc: 0.9956 - val_loss: 0.2821 - val_acc: 0.9500\n",
            "Epoch 235/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0128 - acc: 0.9944 - val_loss: 0.2090 - val_acc: 0.9350\n",
            "Epoch 236/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0189 - acc: 0.9944 - val_loss: 0.2836 - val_acc: 0.9400\n",
            "Epoch 237/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0062 - acc: 0.9981 - val_loss: 0.2757 - val_acc: 0.9450\n",
            "Epoch 238/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.2572 - val_acc: 0.9350\n",
            "Epoch 239/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0090 - acc: 0.9963 - val_loss: 0.1962 - val_acc: 0.9450\n",
            "Epoch 240/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0116 - acc: 0.9963 - val_loss: 0.3172 - val_acc: 0.9400\n",
            "Epoch 241/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0053 - acc: 0.9994 - val_loss: 0.3366 - val_acc: 0.9300\n",
            "Epoch 242/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0065 - acc: 0.9969 - val_loss: 0.2967 - val_acc: 0.9500\n",
            "Epoch 243/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0066 - acc: 0.9975 - val_loss: 0.3022 - val_acc: 0.9250\n",
            "Epoch 244/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0435 - acc: 0.9888 - val_loss: 0.3720 - val_acc: 0.9250\n",
            "Epoch 245/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0113 - acc: 0.9956 - val_loss: 0.4392 - val_acc: 0.9250\n",
            "Epoch 246/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0684 - acc: 0.9775 - val_loss: 0.2577 - val_acc: 0.9450\n",
            "Epoch 247/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0760 - acc: 0.9756 - val_loss: 0.2778 - val_acc: 0.9200\n",
            "Epoch 248/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0421 - acc: 0.9831 - val_loss: 0.3628 - val_acc: 0.9250\n",
            "Epoch 249/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0136 - acc: 0.9938 - val_loss: 0.2165 - val_acc: 0.9550\n",
            "Epoch 250/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0106 - acc: 0.9963 - val_loss: 0.2895 - val_acc: 0.9300\n",
            "Epoch 251/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0062 - acc: 0.9981 - val_loss: 0.2296 - val_acc: 0.9400\n",
            "Epoch 252/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0050 - acc: 0.9988 - val_loss: 0.2599 - val_acc: 0.9300\n",
            "Epoch 253/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0082 - acc: 0.9988 - val_loss: 0.2673 - val_acc: 0.9300\n",
            "Epoch 254/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0250 - acc: 0.9919 - val_loss: 0.5436 - val_acc: 0.9250\n",
            "Epoch 255/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0145 - acc: 0.9938 - val_loss: 0.3449 - val_acc: 0.9500\n",
            "Epoch 256/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0105 - acc: 0.9950 - val_loss: 0.3071 - val_acc: 0.9350\n",
            "Epoch 257/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0106 - acc: 0.9969 - val_loss: 0.2817 - val_acc: 0.9400\n",
            "Epoch 258/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0043 - acc: 0.9994 - val_loss: 0.2784 - val_acc: 0.9350\n",
            "Epoch 259/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0117 - acc: 0.9963 - val_loss: 0.3030 - val_acc: 0.9400\n",
            "Epoch 260/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0640 - acc: 0.9781 - val_loss: 0.3071 - val_acc: 0.9250\n",
            "Epoch 261/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0678 - acc: 0.9869 - val_loss: 1.2884 - val_acc: 0.8600\n",
            "Epoch 262/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0594 - acc: 0.9850 - val_loss: 0.2385 - val_acc: 0.9450\n",
            "Epoch 263/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0112 - acc: 0.9969 - val_loss: 0.2151 - val_acc: 0.9600\n",
            "Epoch 264/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0072 - acc: 0.9988 - val_loss: 0.3152 - val_acc: 0.9350\n",
            "Epoch 265/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0138 - acc: 0.9969 - val_loss: 0.2385 - val_acc: 0.9450\n",
            "Epoch 266/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0255 - acc: 0.9912 - val_loss: 0.3075 - val_acc: 0.9350\n",
            "Epoch 267/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0509 - acc: 0.9838 - val_loss: 0.1890 - val_acc: 0.9200\n",
            "Epoch 268/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0205 - acc: 0.9925 - val_loss: 0.1874 - val_acc: 0.9400\n",
            "Epoch 269/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0131 - acc: 0.9956 - val_loss: 0.2519 - val_acc: 0.9500\n",
            "Epoch 270/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0070 - acc: 0.9975 - val_loss: 0.1753 - val_acc: 0.9550\n",
            "Epoch 271/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0092 - acc: 0.9969 - val_loss: 0.1671 - val_acc: 0.9650\n",
            "Epoch 272/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0109 - acc: 0.9969 - val_loss: 0.2241 - val_acc: 0.9450\n",
            "Epoch 273/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0197 - acc: 0.9944 - val_loss: 0.3727 - val_acc: 0.9300\n",
            "Epoch 274/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0280 - acc: 0.9906 - val_loss: 0.4975 - val_acc: 0.9150\n",
            "Epoch 275/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0512 - acc: 0.9838 - val_loss: 0.6036 - val_acc: 0.8850\n",
            "Epoch 276/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0561 - acc: 0.9856 - val_loss: 0.2846 - val_acc: 0.9300\n",
            "Epoch 277/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0244 - acc: 0.9919 - val_loss: 0.2147 - val_acc: 0.9450\n",
            "Epoch 278/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0175 - acc: 0.9938 - val_loss: 0.1916 - val_acc: 0.9550\n",
            "Epoch 279/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0370 - acc: 0.9875 - val_loss: 0.4732 - val_acc: 0.8950\n",
            "Epoch 280/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0269 - acc: 0.9875 - val_loss: 0.2557 - val_acc: 0.9300\n",
            "Epoch 281/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0231 - acc: 0.9912 - val_loss: 0.3404 - val_acc: 0.9250\n",
            "Epoch 282/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0111 - acc: 0.9975 - val_loss: 0.2316 - val_acc: 0.9400\n",
            "Epoch 283/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0066 - acc: 0.9994 - val_loss: 0.3681 - val_acc: 0.9150\n",
            "Epoch 284/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0070 - acc: 0.9988 - val_loss: 0.2805 - val_acc: 0.9350\n",
            "Epoch 285/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0048 - acc: 0.9981 - val_loss: 0.3036 - val_acc: 0.9300\n",
            "Epoch 286/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0118 - acc: 0.9938 - val_loss: 0.2471 - val_acc: 0.9550\n",
            "Epoch 287/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0072 - acc: 0.9963 - val_loss: 0.2591 - val_acc: 0.9400\n",
            "Epoch 288/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0040 - acc: 0.9994 - val_loss: 0.2739 - val_acc: 0.9350\n",
            "Epoch 289/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0094 - acc: 0.9963 - val_loss: 0.3387 - val_acc: 0.9350\n",
            "Epoch 290/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0093 - acc: 0.9956 - val_loss: 0.3607 - val_acc: 0.9400\n",
            "Epoch 291/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0039 - acc: 0.9994 - val_loss: 0.3016 - val_acc: 0.9400\n",
            "Epoch 292/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0074 - acc: 0.9981 - val_loss: 0.3497 - val_acc: 0.9350\n",
            "Epoch 293/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0813 - acc: 0.9769 - val_loss: 0.3718 - val_acc: 0.9350\n",
            "Epoch 294/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0917 - acc: 0.9719 - val_loss: 0.2547 - val_acc: 0.9150\n",
            "Epoch 295/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0212 - acc: 0.9931 - val_loss: 0.2017 - val_acc: 0.9400\n",
            "Epoch 296/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0162 - acc: 0.9944 - val_loss: 0.2291 - val_acc: 0.9450\n",
            "Epoch 297/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0102 - acc: 0.9969 - val_loss: 0.2029 - val_acc: 0.9500\n",
            "Epoch 298/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0087 - acc: 0.9969 - val_loss: 0.2425 - val_acc: 0.9350\n",
            "Epoch 299/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0063 - acc: 0.9988 - val_loss: 0.3266 - val_acc: 0.9350\n",
            "Epoch 300/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.3141 - val_acc: 0.9450\n",
            "Epoch 301/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0096 - acc: 0.9994 - val_loss: 0.2878 - val_acc: 0.9350\n",
            "Epoch 302/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.2864 - val_acc: 0.9350\n",
            "Epoch 303/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0061 - acc: 0.9975 - val_loss: 0.3130 - val_acc: 0.9250\n",
            "Epoch 304/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.2874 - val_acc: 0.9300\n",
            "Epoch 305/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.2746 - val_acc: 0.9400\n",
            "Epoch 306/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0038 - acc: 0.9981 - val_loss: 0.4830 - val_acc: 0.9150\n",
            "Epoch 307/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0597 - acc: 0.9862 - val_loss: 0.5236 - val_acc: 0.8950\n",
            "Epoch 308/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0276 - acc: 0.9894 - val_loss: 0.2876 - val_acc: 0.9300\n",
            "Epoch 309/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0088 - acc: 0.9969 - val_loss: 0.2745 - val_acc: 0.9300\n",
            "Epoch 310/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0215 - acc: 0.9912 - val_loss: 0.1899 - val_acc: 0.9450\n",
            "Epoch 311/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0242 - acc: 0.9950 - val_loss: 0.2888 - val_acc: 0.9350\n",
            "Epoch 312/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0068 - acc: 0.9988 - val_loss: 0.2513 - val_acc: 0.9500\n",
            "Epoch 313/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0099 - acc: 0.9963 - val_loss: 0.1804 - val_acc: 0.9600\n",
            "Epoch 314/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0063 - acc: 0.9994 - val_loss: 0.2614 - val_acc: 0.9500\n",
            "Epoch 315/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0058 - acc: 0.9981 - val_loss: 0.2862 - val_acc: 0.9500\n",
            "Epoch 316/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0054 - acc: 0.9988 - val_loss: 0.2932 - val_acc: 0.9350\n",
            "Epoch 317/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0262 - acc: 0.9944 - val_loss: 0.5053 - val_acc: 0.9150\n",
            "Epoch 318/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0613 - acc: 0.9806 - val_loss: 0.1205 - val_acc: 0.9550\n",
            "Epoch 319/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0122 - acc: 0.9969 - val_loss: 0.2606 - val_acc: 0.9300\n",
            "Epoch 320/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0065 - acc: 0.9981 - val_loss: 0.2845 - val_acc: 0.9200\n",
            "Epoch 321/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0165 - acc: 0.9956 - val_loss: 0.5413 - val_acc: 0.9100\n",
            "Epoch 322/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0921 - acc: 0.9756 - val_loss: 0.4822 - val_acc: 0.9000\n",
            "Epoch 323/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0160 - acc: 0.9975 - val_loss: 0.3334 - val_acc: 0.9400\n",
            "Epoch 324/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0195 - acc: 0.9944 - val_loss: 0.1915 - val_acc: 0.9550\n",
            "Epoch 325/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0080 - acc: 0.9975 - val_loss: 0.3226 - val_acc: 0.9300\n",
            "Epoch 326/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0097 - acc: 0.9981 - val_loss: 0.3781 - val_acc: 0.9300\n",
            "Epoch 327/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0095 - acc: 0.9963 - val_loss: 0.5221 - val_acc: 0.9150\n",
            "Epoch 328/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0114 - acc: 0.9963 - val_loss: 0.3220 - val_acc: 0.9500\n",
            "Epoch 329/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0070 - acc: 0.9969 - val_loss: 0.3216 - val_acc: 0.9400\n",
            "Epoch 330/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.2848 - val_acc: 0.9350\n",
            "Epoch 331/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.3214 - val_acc: 0.9350\n",
            "Epoch 332/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.2926 - val_acc: 0.9550\n",
            "Epoch 333/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0053 - acc: 0.9981 - val_loss: 0.3688 - val_acc: 0.9350\n",
            "Epoch 334/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0038 - acc: 0.9981 - val_loss: 0.2749 - val_acc: 0.9550\n",
            "Epoch 335/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.3609 - val_acc: 0.9350\n",
            "Epoch 336/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0094 - acc: 0.9969 - val_loss: 0.4557 - val_acc: 0.9250\n",
            "Epoch 337/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0820 - acc: 0.9862 - val_loss: 0.2829 - val_acc: 0.9350\n",
            "Epoch 338/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2213 - acc: 0.9500 - val_loss: 0.5557 - val_acc: 0.8450\n",
            "Epoch 339/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0904 - acc: 0.9725 - val_loss: 0.2946 - val_acc: 0.9200\n",
            "Epoch 340/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0228 - acc: 0.9938 - val_loss: 0.2391 - val_acc: 0.9350\n",
            "Epoch 341/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0123 - acc: 0.9963 - val_loss: 0.2790 - val_acc: 0.9200\n",
            "Epoch 342/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0101 - acc: 0.9988 - val_loss: 0.2860 - val_acc: 0.9350\n",
            "Epoch 343/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0105 - acc: 0.9981 - val_loss: 0.2584 - val_acc: 0.9400\n",
            "Epoch 344/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0073 - acc: 0.9988 - val_loss: 0.2983 - val_acc: 0.9400\n",
            "Epoch 345/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0043 - acc: 0.9994 - val_loss: 0.2402 - val_acc: 0.9500\n",
            "Epoch 346/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0058 - acc: 0.9981 - val_loss: 0.3140 - val_acc: 0.9250\n",
            "Epoch 347/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0099 - acc: 0.9956 - val_loss: 0.2949 - val_acc: 0.9500\n",
            "Epoch 348/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0053 - acc: 0.9981 - val_loss: 0.2972 - val_acc: 0.9250\n",
            "Epoch 349/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0040 - acc: 0.9981 - val_loss: 0.2883 - val_acc: 0.9450\n",
            "Epoch 350/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0055 - acc: 0.9969 - val_loss: 0.3110 - val_acc: 0.9400\n",
            "Epoch 351/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0117 - acc: 0.9963 - val_loss: 0.3226 - val_acc: 0.9300\n",
            "Epoch 352/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0107 - acc: 0.9981 - val_loss: 0.2200 - val_acc: 0.9550\n",
            "Epoch 353/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0068 - acc: 0.9975 - val_loss: 0.3346 - val_acc: 0.9250\n",
            "Epoch 354/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0149 - acc: 0.9931 - val_loss: 0.2379 - val_acc: 0.9300\n",
            "Epoch 355/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0135 - acc: 0.9950 - val_loss: 0.3281 - val_acc: 0.9300\n",
            "Epoch 356/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0186 - acc: 0.9950 - val_loss: 0.4573 - val_acc: 0.9200\n",
            "Epoch 357/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0107 - acc: 0.9963 - val_loss: 0.3594 - val_acc: 0.9300\n",
            "Epoch 358/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0070 - acc: 0.9969 - val_loss: 0.3236 - val_acc: 0.9350\n",
            "Epoch 359/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0088 - acc: 0.9969 - val_loss: 0.4423 - val_acc: 0.9200\n",
            "Epoch 360/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0125 - acc: 0.9938 - val_loss: 0.4109 - val_acc: 0.9350\n",
            "Epoch 361/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0081 - acc: 0.9975 - val_loss: 0.4133 - val_acc: 0.9150\n",
            "Epoch 362/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.3469 - val_acc: 0.9350\n",
            "Epoch 363/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0037 - acc: 0.9994 - val_loss: 0.3718 - val_acc: 0.9400\n",
            "Epoch 364/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0046 - acc: 0.9988 - val_loss: 0.3566 - val_acc: 0.9200\n",
            "Epoch 365/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0125 - acc: 0.9944 - val_loss: 0.5175 - val_acc: 0.9150\n",
            "Epoch 366/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0580 - acc: 0.9838 - val_loss: 0.4587 - val_acc: 0.9150\n",
            "Epoch 367/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0256 - acc: 0.9938 - val_loss: 0.2156 - val_acc: 0.9450\n",
            "Epoch 368/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0274 - acc: 0.9931 - val_loss: 0.3544 - val_acc: 0.9350\n",
            "Epoch 369/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0352 - acc: 0.9888 - val_loss: 0.4917 - val_acc: 0.9150\n",
            "Epoch 370/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0403 - acc: 0.9869 - val_loss: 0.3859 - val_acc: 0.9350\n",
            "Epoch 371/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0568 - acc: 0.9831 - val_loss: 0.3724 - val_acc: 0.9150\n",
            "Epoch 372/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0170 - acc: 0.9950 - val_loss: 0.3007 - val_acc: 0.9200\n",
            "Epoch 373/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0154 - acc: 0.9944 - val_loss: 0.2250 - val_acc: 0.9450\n",
            "Epoch 374/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0102 - acc: 0.9981 - val_loss: 0.3177 - val_acc: 0.9300\n",
            "Epoch 375/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0033 - acc: 0.9994 - val_loss: 0.2828 - val_acc: 0.9350\n",
            "Epoch 376/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0052 - acc: 0.9988 - val_loss: 0.2816 - val_acc: 0.9400\n",
            "Epoch 377/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.3020 - val_acc: 0.9400\n",
            "Epoch 378/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0033 - acc: 0.9994 - val_loss: 0.3113 - val_acc: 0.9400\n",
            "Epoch 379/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.3416 - val_acc: 0.9350\n",
            "Epoch 380/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.3636 - val_acc: 0.9150\n",
            "Epoch 381/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.3702 - val_acc: 0.9250\n",
            "Epoch 382/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.2808 - val_acc: 0.9400\n",
            "Epoch 383/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.3002 - val_acc: 0.9350\n",
            "Epoch 384/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0098 - acc: 0.9956 - val_loss: 0.5177 - val_acc: 0.9050\n",
            "Epoch 385/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0161 - acc: 0.9963 - val_loss: 0.3775 - val_acc: 0.9300\n",
            "Epoch 386/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0083 - acc: 0.9975 - val_loss: 0.4535 - val_acc: 0.8950\n",
            "Epoch 387/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0134 - acc: 0.9950 - val_loss: 0.3551 - val_acc: 0.9400\n",
            "Epoch 388/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0101 - acc: 0.9963 - val_loss: 0.3935 - val_acc: 0.9300\n",
            "Epoch 389/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0093 - acc: 0.9981 - val_loss: 0.3325 - val_acc: 0.9300\n",
            "Epoch 390/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0039 - acc: 0.9994 - val_loss: 0.3275 - val_acc: 0.9400\n",
            "Epoch 391/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.3303 - val_acc: 0.9450\n",
            "Epoch 392/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0062 - acc: 0.9975 - val_loss: 0.3046 - val_acc: 0.9350\n",
            "Epoch 393/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0390 - acc: 0.9894 - val_loss: 0.5073 - val_acc: 0.9050\n",
            "Epoch 394/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0319 - acc: 0.9900 - val_loss: 0.3297 - val_acc: 0.9250\n",
            "Epoch 395/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0116 - acc: 0.9950 - val_loss: 0.5074 - val_acc: 0.9150\n",
            "Epoch 396/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0132 - acc: 0.9938 - val_loss: 0.4849 - val_acc: 0.9100\n",
            "Epoch 397/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0051 - acc: 0.9981 - val_loss: 0.4523 - val_acc: 0.9400\n",
            "Epoch 398/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0074 - acc: 0.9975 - val_loss: 0.3848 - val_acc: 0.9400\n",
            "Epoch 399/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0204 - acc: 0.9963 - val_loss: 0.4390 - val_acc: 0.9250\n",
            "Epoch 400/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0276 - acc: 0.9912 - val_loss: 0.3363 - val_acc: 0.9300\n",
            "Epoch 401/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.3169 - val_acc: 0.9350\n",
            "Epoch 402/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.3763 - val_acc: 0.9350\n",
            "Epoch 403/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0087 - acc: 0.9963 - val_loss: 0.3830 - val_acc: 0.9400\n",
            "Epoch 404/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.3423 - val_acc: 0.9350\n",
            "Epoch 405/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.3282 - val_acc: 0.9400\n",
            "Epoch 406/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0054 - acc: 0.9981 - val_loss: 0.2810 - val_acc: 0.9500\n",
            "Epoch 407/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0127 - acc: 0.9963 - val_loss: 0.5318 - val_acc: 0.9050\n",
            "Epoch 408/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.4307 - val_acc: 0.9150\n",
            "Epoch 409/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.3097 - val_acc: 0.9500\n",
            "Epoch 410/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.4211 - val_acc: 0.9450\n",
            "Epoch 411/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.3276 - val_acc: 0.9500\n",
            "Epoch 412/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.3438 - val_acc: 0.9300\n",
            "Epoch 413/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.3069 - val_acc: 0.9350\n",
            "Epoch 414/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1062 - acc: 0.9788 - val_loss: 0.5122 - val_acc: 0.9250\n",
            "Epoch 415/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0880 - acc: 0.9675 - val_loss: 0.3881 - val_acc: 0.9050\n",
            "Epoch 416/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0183 - acc: 0.9938 - val_loss: 0.3810 - val_acc: 0.9150\n",
            "Epoch 417/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0081 - acc: 0.9975 - val_loss: 0.3386 - val_acc: 0.9250\n",
            "Epoch 418/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0060 - acc: 0.9981 - val_loss: 0.2380 - val_acc: 0.9550\n",
            "Epoch 419/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.2783 - val_acc: 0.9350\n",
            "Epoch 420/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.2916 - val_acc: 0.9400\n",
            "Epoch 421/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.4645 - val_acc: 0.9350\n",
            "Epoch 422/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0052 - acc: 0.9981 - val_loss: 0.3378 - val_acc: 0.9300\n",
            "Epoch 423/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.3979 - val_acc: 0.9300\n",
            "Epoch 424/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0037 - acc: 0.9981 - val_loss: 0.3644 - val_acc: 0.9350\n",
            "Epoch 425/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.4469 - val_acc: 0.9250\n",
            "Epoch 426/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0049 - acc: 0.9975 - val_loss: 0.3205 - val_acc: 0.9450\n",
            "Epoch 427/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0037 - acc: 0.9994 - val_loss: 0.3176 - val_acc: 0.9550\n",
            "Epoch 428/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.3577 - val_acc: 0.9550\n",
            "Epoch 429/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0085 - acc: 0.9963 - val_loss: 0.3123 - val_acc: 0.9450\n",
            "Epoch 430/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0215 - acc: 0.9919 - val_loss: 0.5300 - val_acc: 0.9200\n",
            "Epoch 431/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0535 - acc: 0.9819 - val_loss: 0.3836 - val_acc: 0.9250\n",
            "Epoch 432/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0071 - acc: 0.9975 - val_loss: 0.4673 - val_acc: 0.9200\n",
            "Epoch 433/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0055 - acc: 0.9981 - val_loss: 0.3713 - val_acc: 0.9350\n",
            "Epoch 434/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0066 - acc: 0.9988 - val_loss: 0.3347 - val_acc: 0.9300\n",
            "Epoch 435/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.3086 - val_acc: 0.9650\n",
            "Epoch 436/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0144 - acc: 0.9963 - val_loss: 0.5193 - val_acc: 0.9300\n",
            "Epoch 437/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0090 - acc: 0.9975 - val_loss: 0.3423 - val_acc: 0.9450\n",
            "Epoch 438/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0053 - acc: 0.9969 - val_loss: 0.3476 - val_acc: 0.9550\n",
            "Epoch 439/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.4418 - val_acc: 0.9200\n",
            "Epoch 440/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0051 - acc: 0.9981 - val_loss: 0.3699 - val_acc: 0.9450\n",
            "Epoch 441/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.3761 - val_acc: 0.9400\n",
            "Epoch 442/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.4190 - val_acc: 0.9350\n",
            "Epoch 443/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.4602 - val_acc: 0.9200\n",
            "Epoch 444/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0038 - acc: 0.9981 - val_loss: 0.3392 - val_acc: 0.9250\n",
            "Epoch 445/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.3261 - val_acc: 0.9400\n",
            "Epoch 446/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.3208 - val_acc: 0.9450\n",
            "Epoch 447/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.4240 - val_acc: 0.9300\n",
            "Epoch 448/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0012 - acc: 0.9994 - val_loss: 0.3647 - val_acc: 0.9400\n",
            "Epoch 449/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.4803e-04 - acc: 1.0000 - val_loss: 0.3623 - val_acc: 0.9500\n",
            "Epoch 450/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0028 - acc: 0.9981 - val_loss: 0.5785 - val_acc: 0.9250\n",
            "Epoch 451/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0214 - acc: 0.9950 - val_loss: 0.6486 - val_acc: 0.9050\n",
            "Epoch 452/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0333 - acc: 0.9888 - val_loss: 0.7491 - val_acc: 0.9200\n",
            "Epoch 453/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1573 - acc: 0.9581 - val_loss: 0.7320 - val_acc: 0.8800\n",
            "Epoch 454/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0544 - acc: 0.9838 - val_loss: 0.3321 - val_acc: 0.9350\n",
            "Epoch 455/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0277 - acc: 0.9894 - val_loss: 0.3147 - val_acc: 0.9350\n",
            "Epoch 456/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0108 - acc: 0.9975 - val_loss: 0.2908 - val_acc: 0.9400\n",
            "Epoch 457/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0071 - acc: 0.9988 - val_loss: 0.3537 - val_acc: 0.9150\n",
            "Epoch 458/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0062 - acc: 0.9981 - val_loss: 0.3070 - val_acc: 0.9350\n",
            "Epoch 459/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.3392 - val_acc: 0.9400\n",
            "Epoch 460/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.3410 - val_acc: 0.9400\n",
            "Epoch 461/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0037 - acc: 0.9994 - val_loss: 0.3422 - val_acc: 0.9450\n",
            "Epoch 462/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.3488 - val_acc: 0.9450\n",
            "Epoch 463/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0043 - acc: 0.9981 - val_loss: 0.3362 - val_acc: 0.9400\n",
            "Epoch 464/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0125 - acc: 0.9956 - val_loss: 0.5034 - val_acc: 0.9250\n",
            "Epoch 465/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0078 - acc: 0.9956 - val_loss: 0.4257 - val_acc: 0.9150\n",
            "Epoch 466/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0061 - acc: 0.9969 - val_loss: 0.3899 - val_acc: 0.9150\n",
            "Epoch 467/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.3857 - val_acc: 0.9250\n",
            "Epoch 468/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0050 - acc: 0.9988 - val_loss: 0.3534 - val_acc: 0.9500\n",
            "Epoch 469/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0103 - acc: 0.9981 - val_loss: 0.4676 - val_acc: 0.9200\n",
            "Epoch 470/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0053 - acc: 0.9981 - val_loss: 0.4987 - val_acc: 0.9250\n",
            "Epoch 471/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.4579 - val_acc: 0.9300\n",
            "Epoch 472/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.4570 - val_acc: 0.9200\n",
            "Epoch 473/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 9.9445e-04 - acc: 1.0000 - val_loss: 0.5310 - val_acc: 0.9250\n",
            "Epoch 474/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.4523 - val_acc: 0.9300\n",
            "Epoch 475/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.4326 - val_acc: 0.9350\n",
            "Epoch 476/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0115 - acc: 0.9969 - val_loss: 0.6549 - val_acc: 0.9050\n",
            "Epoch 477/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0870 - acc: 0.9800 - val_loss: 0.5868 - val_acc: 0.9150\n",
            "Epoch 478/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0442 - acc: 0.9831 - val_loss: 0.4014 - val_acc: 0.9050\n",
            "Epoch 479/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0314 - acc: 0.9906 - val_loss: 0.4092 - val_acc: 0.9350\n",
            "Epoch 480/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0066 - acc: 0.9981 - val_loss: 0.4446 - val_acc: 0.9200\n",
            "Epoch 481/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.5127 - val_acc: 0.9150\n",
            "Epoch 482/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.4976 - val_acc: 0.9200\n",
            "Epoch 483/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0059 - acc: 0.9981 - val_loss: 0.5837 - val_acc: 0.9150\n",
            "Epoch 484/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0077 - acc: 0.9981 - val_loss: 0.4239 - val_acc: 0.9200\n",
            "Epoch 485/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0033 - acc: 0.9988 - val_loss: 0.3878 - val_acc: 0.9250\n",
            "Epoch 486/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.4962 - val_acc: 0.9100\n",
            "Epoch 487/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0051 - acc: 0.9981 - val_loss: 0.5029 - val_acc: 0.9300\n",
            "Epoch 488/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0241 - acc: 0.9912 - val_loss: 0.4505 - val_acc: 0.9150\n",
            "Epoch 489/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0151 - acc: 0.9950 - val_loss: 0.3064 - val_acc: 0.9300\n",
            "Epoch 490/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0119 - acc: 0.9956 - val_loss: 0.3148 - val_acc: 0.9300\n",
            "Epoch 491/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0037 - acc: 0.9994 - val_loss: 0.3248 - val_acc: 0.9350\n",
            "Epoch 492/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.3757 - val_acc: 0.9300\n",
            "Epoch 493/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.4179 - val_acc: 0.9300\n",
            "Epoch 494/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0052 - acc: 0.9988 - val_loss: 0.3370 - val_acc: 0.9450\n",
            "Epoch 495/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0029 - acc: 0.9981 - val_loss: 0.4073 - val_acc: 0.9350\n",
            "Epoch 496/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.4327 - val_acc: 0.9450\n",
            "Epoch 497/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.4137 - val_acc: 0.9450\n",
            "Epoch 498/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.4221 - val_acc: 0.9450\n",
            "Epoch 499/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0125 - acc: 0.9969 - val_loss: 0.4591 - val_acc: 0.9300\n",
            "Epoch 500/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0071 - acc: 0.9981 - val_loss: 0.4762 - val_acc: 0.9200\n",
            "Epoch 501/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0339 - acc: 0.9919 - val_loss: 0.5568 - val_acc: 0.9150\n",
            "Epoch 502/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0182 - acc: 0.9919 - val_loss: 0.3948 - val_acc: 0.9300\n",
            "Epoch 503/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0046 - acc: 0.9994 - val_loss: 0.3974 - val_acc: 0.9250\n",
            "Epoch 504/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0033 - acc: 0.9981 - val_loss: 0.4113 - val_acc: 0.9300\n",
            "Epoch 505/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0036 - acc: 0.9981 - val_loss: 0.3858 - val_acc: 0.9450\n",
            "Epoch 506/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.3784 - val_acc: 0.9350\n",
            "Epoch 507/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.4416 - val_acc: 0.9250\n",
            "Epoch 508/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.3845 - val_acc: 0.9350\n",
            "Epoch 509/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 7.9421e-04 - acc: 1.0000 - val_loss: 0.4118 - val_acc: 0.9350\n",
            "Epoch 510/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.4095 - val_acc: 0.9350\n",
            "Epoch 511/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0030 - acc: 0.9981 - val_loss: 0.4564 - val_acc: 0.9200\n",
            "Epoch 512/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.3071 - val_acc: 0.9450\n",
            "Epoch 513/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.8117 - val_acc: 0.9000\n",
            "Epoch 514/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0188 - acc: 0.9950 - val_loss: 0.6179 - val_acc: 0.8950\n",
            "Epoch 515/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0355 - acc: 0.9912 - val_loss: 0.4284 - val_acc: 0.9150\n",
            "Epoch 516/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0148 - acc: 0.9956 - val_loss: 0.4514 - val_acc: 0.9100\n",
            "Epoch 517/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0118 - acc: 0.9950 - val_loss: 0.7320 - val_acc: 0.9050\n",
            "Epoch 518/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0784 - acc: 0.9831 - val_loss: 0.7534 - val_acc: 0.8900\n",
            "Epoch 519/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0290 - acc: 0.9906 - val_loss: 0.7087 - val_acc: 0.9100\n",
            "Epoch 520/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0081 - acc: 0.9969 - val_loss: 0.5794 - val_acc: 0.9150\n",
            "Epoch 521/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0061 - acc: 0.9975 - val_loss: 0.4332 - val_acc: 0.9300\n",
            "Epoch 522/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0046 - acc: 0.9981 - val_loss: 0.5302 - val_acc: 0.9100\n",
            "Epoch 523/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0142 - acc: 0.9938 - val_loss: 0.8769 - val_acc: 0.8900\n",
            "Epoch 524/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0069 - acc: 0.9988 - val_loss: 0.5372 - val_acc: 0.9200\n",
            "Epoch 525/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0049 - acc: 0.9969 - val_loss: 0.7553 - val_acc: 0.9100\n",
            "Epoch 526/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.5676 - val_acc: 0.9150\n",
            "Epoch 527/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.5932 - val_acc: 0.9100\n",
            "Epoch 528/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.5849 - val_acc: 0.9150\n",
            "Epoch 529/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.4896 - val_acc: 0.9400\n",
            "Epoch 530/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0034 - acc: 0.9994 - val_loss: 0.4911 - val_acc: 0.9350\n",
            "Epoch 531/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 9.2537e-04 - acc: 1.0000 - val_loss: 0.5158 - val_acc: 0.9200\n",
            "Epoch 532/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.5711 - val_acc: 0.9150\n",
            "Epoch 533/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0026 - acc: 0.9988 - val_loss: 0.6204 - val_acc: 0.9150\n",
            "Epoch 534/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0208 - acc: 0.9950 - val_loss: 0.6675 - val_acc: 0.9100\n",
            "Epoch 535/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0432 - acc: 0.9856 - val_loss: 0.6264 - val_acc: 0.9050\n",
            "Epoch 536/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0332 - acc: 0.9912 - val_loss: 0.4675 - val_acc: 0.9250\n",
            "Epoch 537/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0103 - acc: 0.9975 - val_loss: 0.4310 - val_acc: 0.9350\n",
            "Epoch 538/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0049 - acc: 0.9975 - val_loss: 0.5432 - val_acc: 0.9250\n",
            "Epoch 539/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0066 - acc: 0.9975 - val_loss: 0.5177 - val_acc: 0.9300\n",
            "Epoch 540/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.5885 - val_acc: 0.9100\n",
            "Epoch 541/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0081 - acc: 0.9975 - val_loss: 0.5810 - val_acc: 0.9200\n",
            "Epoch 542/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0400 - acc: 0.9894 - val_loss: 0.4744 - val_acc: 0.9250\n",
            "Epoch 543/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0305 - acc: 0.9931 - val_loss: 0.3866 - val_acc: 0.9250\n",
            "Epoch 544/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0537 - acc: 0.9900 - val_loss: 0.6452 - val_acc: 0.9050\n",
            "Epoch 545/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0111 - acc: 0.9969 - val_loss: 0.5031 - val_acc: 0.9250\n",
            "Epoch 546/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0100 - acc: 0.9963 - val_loss: 0.6968 - val_acc: 0.9000\n",
            "Epoch 547/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.5270 - val_acc: 0.9100\n",
            "Epoch 548/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.4407 - val_acc: 0.9150\n",
            "Epoch 549/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.4481 - val_acc: 0.9150\n",
            "Epoch 550/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0369 - acc: 0.9950 - val_loss: 0.5762 - val_acc: 0.9250\n",
            "Epoch 551/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0124 - acc: 0.9956 - val_loss: 0.5063 - val_acc: 0.9050\n",
            "Epoch 552/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0043 - acc: 0.9981 - val_loss: 0.4044 - val_acc: 0.9200\n",
            "Epoch 553/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.5140 - val_acc: 0.9250\n",
            "Epoch 554/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.4823 - val_acc: 0.9250\n",
            "Epoch 555/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.5450 - val_acc: 0.9150\n",
            "Epoch 556/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0023 - acc: 0.9988 - val_loss: 0.6060 - val_acc: 0.9000\n",
            "Epoch 557/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.5655 - val_acc: 0.9150\n",
            "Epoch 558/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.5919 - val_acc: 0.9100\n",
            "Epoch 559/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0504 - acc: 0.9862 - val_loss: 0.5613 - val_acc: 0.9000\n",
            "Epoch 560/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0464 - acc: 0.9856 - val_loss: 0.2584 - val_acc: 0.9450\n",
            "Epoch 561/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0144 - acc: 0.9944 - val_loss: 0.3353 - val_acc: 0.9350\n",
            "Epoch 562/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0374 - acc: 0.9888 - val_loss: 0.4157 - val_acc: 0.9250\n",
            "Epoch 563/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0068 - acc: 0.9981 - val_loss: 0.2986 - val_acc: 0.9500\n",
            "Epoch 564/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0052 - acc: 0.9975 - val_loss: 0.3732 - val_acc: 0.9300\n",
            "Epoch 565/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.3998 - val_acc: 0.9200\n",
            "Epoch 566/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0033 - acc: 0.9994 - val_loss: 0.3295 - val_acc: 0.9350\n",
            "Epoch 567/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0054 - acc: 0.9981 - val_loss: 0.3302 - val_acc: 0.9450\n",
            "Epoch 568/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.4062 - val_acc: 0.9300\n",
            "Epoch 569/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.2390e-04 - acc: 1.0000 - val_loss: 0.3503 - val_acc: 0.9450\n",
            "Epoch 570/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.3323 - val_acc: 0.9550\n",
            "Epoch 571/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.3191 - val_acc: 0.9550\n",
            "Epoch 572/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.3549 - val_acc: 0.9450\n",
            "Epoch 573/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0250 - acc: 0.9925 - val_loss: 0.7312 - val_acc: 0.9000\n",
            "Epoch 574/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0810 - acc: 0.9781 - val_loss: 0.3644 - val_acc: 0.9300\n",
            "Epoch 575/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0259 - acc: 0.9906 - val_loss: 0.4147 - val_acc: 0.9300\n",
            "Epoch 576/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0065 - acc: 0.9975 - val_loss: 0.3671 - val_acc: 0.9250\n",
            "Epoch 577/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0055 - acc: 0.9994 - val_loss: 0.4291 - val_acc: 0.9050\n",
            "Epoch 578/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.3791 - val_acc: 0.9300\n",
            "Epoch 579/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.3638 - val_acc: 0.9300\n",
            "Epoch 580/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.3853 - val_acc: 0.9300\n",
            "Epoch 581/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.3929 - val_acc: 0.9350\n",
            "Epoch 582/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.3636 - val_acc: 0.9350\n",
            "Epoch 583/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9988 - val_loss: 0.3820 - val_acc: 0.9350\n",
            "Epoch 584/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.3773 - val_acc: 0.9350\n",
            "Epoch 585/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.4483 - val_acc: 0.9200\n",
            "Epoch 586/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0095 - acc: 0.9963 - val_loss: 0.3708 - val_acc: 0.9250\n",
            "Epoch 587/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.3574 - val_acc: 0.9350\n",
            "Epoch 588/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0024 - acc: 0.9988 - val_loss: 0.4236 - val_acc: 0.9400\n",
            "Epoch 589/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.4082 - val_acc: 0.9350\n",
            "Epoch 590/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 9.2139e-04 - acc: 1.0000 - val_loss: 0.4581 - val_acc: 0.9350\n",
            "Epoch 591/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0015 - acc: 0.9994 - val_loss: 0.4319 - val_acc: 0.9250\n",
            "Epoch 592/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.4287 - val_acc: 0.9400\n",
            "Epoch 593/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.4401 - val_acc: 0.9400\n",
            "Epoch 594/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.4178 - val_acc: 0.9350\n",
            "Epoch 595/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.4076 - val_acc: 0.9400\n",
            "Epoch 596/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 5.7946e-04 - acc: 1.0000 - val_loss: 0.3931 - val_acc: 0.9400\n",
            "Epoch 597/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.3553e-04 - acc: 1.0000 - val_loss: 0.4463 - val_acc: 0.9350\n",
            "Epoch 598/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.9634e-04 - acc: 1.0000 - val_loss: 0.4491 - val_acc: 0.9350\n",
            "Epoch 599/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1267 - acc: 0.9856 - val_loss: 0.4513 - val_acc: 0.9400\n",
            "Epoch 600/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0538 - acc: 0.9881 - val_loss: 0.6711 - val_acc: 0.9050\n",
            "Epoch 601/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0200 - acc: 0.9925 - val_loss: 0.4294 - val_acc: 0.9050\n",
            "Epoch 602/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0215 - acc: 0.9931 - val_loss: 0.3615 - val_acc: 0.9250\n",
            "Epoch 603/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.2968 - val_acc: 0.9250\n",
            "Epoch 604/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.3344 - val_acc: 0.9250\n",
            "Epoch 605/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0051 - acc: 0.9981 - val_loss: 0.5345 - val_acc: 0.9150\n",
            "Epoch 606/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.4326 - val_acc: 0.9300\n",
            "Epoch 607/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.4367 - val_acc: 0.9250\n",
            "Epoch 608/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0070 - acc: 0.9975 - val_loss: 0.5033 - val_acc: 0.9100\n",
            "Epoch 609/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 9.9758e-04 - acc: 1.0000 - val_loss: 0.4706 - val_acc: 0.9150\n",
            "Epoch 610/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.3803 - val_acc: 0.9350\n",
            "Epoch 611/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0186 - acc: 0.9938 - val_loss: 0.7538 - val_acc: 0.9150\n",
            "Epoch 612/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0446 - acc: 0.9875 - val_loss: 0.3543 - val_acc: 0.9350\n",
            "Epoch 613/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0070 - acc: 0.9988 - val_loss: 0.4312 - val_acc: 0.9200\n",
            "Epoch 614/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.4551 - val_acc: 0.9200\n",
            "Epoch 615/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.3569 - val_acc: 0.9450\n",
            "Epoch 616/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.3750 - val_acc: 0.9350\n",
            "Epoch 617/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.4071 - val_acc: 0.9400\n",
            "Epoch 618/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0078 - acc: 0.9969 - val_loss: 0.4246 - val_acc: 0.9400\n",
            "Epoch 619/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0719 - acc: 0.9831 - val_loss: 0.4756 - val_acc: 0.9150\n",
            "Epoch 620/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0214 - acc: 0.9931 - val_loss: 0.3703 - val_acc: 0.9300\n",
            "Epoch 621/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0077 - acc: 0.9981 - val_loss: 0.3355 - val_acc: 0.9100\n",
            "Epoch 622/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0133 - acc: 0.9931 - val_loss: 0.4050 - val_acc: 0.9200\n",
            "Epoch 623/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0129 - acc: 0.9969 - val_loss: 0.5804 - val_acc: 0.9100\n",
            "Epoch 624/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0310 - acc: 0.9969 - val_loss: 0.3401 - val_acc: 0.9350\n",
            "Epoch 625/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.3608 - val_acc: 0.9350\n",
            "Epoch 626/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.3893 - val_acc: 0.9250\n",
            "Epoch 627/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0032 - acc: 0.9981 - val_loss: 0.4725 - val_acc: 0.9150\n",
            "Epoch 628/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.9988 - val_loss: 0.5159 - val_acc: 0.9200\n",
            "Epoch 629/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.4455 - val_acc: 0.9150\n",
            "Epoch 630/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.4493 - val_acc: 0.9150\n",
            "Epoch 631/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.4791 - val_acc: 0.9250\n",
            "Epoch 632/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0025 - acc: 0.9988 - val_loss: 0.3997 - val_acc: 0.9200\n",
            "Epoch 633/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0047 - acc: 0.9975 - val_loss: 0.4831 - val_acc: 0.9250\n",
            "Epoch 634/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0058 - acc: 0.9981 - val_loss: 0.5859 - val_acc: 0.9250\n",
            "Epoch 635/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0589 - acc: 0.9862 - val_loss: 0.5270 - val_acc: 0.9100\n",
            "Epoch 636/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0349 - acc: 0.9906 - val_loss: 0.6234 - val_acc: 0.9050\n",
            "Epoch 637/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0077 - acc: 0.9975 - val_loss: 0.5118 - val_acc: 0.9250\n",
            "Epoch 638/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0053 - acc: 0.9975 - val_loss: 0.4836 - val_acc: 0.9300\n",
            "Epoch 639/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0057 - acc: 0.9975 - val_loss: 0.4639 - val_acc: 0.9250\n",
            "Epoch 640/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0040 - acc: 0.9981 - val_loss: 0.4697 - val_acc: 0.9250\n",
            "Epoch 641/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0089 - acc: 0.9975 - val_loss: 0.5826 - val_acc: 0.9150\n",
            "Epoch 642/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0137 - acc: 0.9963 - val_loss: 0.4553 - val_acc: 0.9250\n",
            "Epoch 643/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0046 - acc: 0.9994 - val_loss: 0.4370 - val_acc: 0.9250\n",
            "Epoch 644/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0078 - acc: 0.9969 - val_loss: 0.4425 - val_acc: 0.9200\n",
            "Epoch 645/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.3828 - val_acc: 0.9250\n",
            "Epoch 646/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.3642 - val_acc: 0.9300\n",
            "Epoch 647/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0046 - acc: 0.9981 - val_loss: 0.4607 - val_acc: 0.9300\n",
            "Epoch 648/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.3849 - val_acc: 0.9350\n",
            "Epoch 649/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.3832 - val_acc: 0.9300\n",
            "Epoch 650/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.4217 - val_acc: 0.9300\n",
            "Epoch 651/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.4255 - val_acc: 0.9300\n",
            "Epoch 652/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.6435 - val_acc: 0.9200\n",
            "Epoch 653/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.5104 - val_acc: 0.9300\n",
            "Epoch 654/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.6843 - val_acc: 0.9050\n",
            "Epoch 655/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0598 - acc: 0.9912 - val_loss: 0.9511 - val_acc: 0.8850\n",
            "Epoch 656/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0270 - acc: 0.9956 - val_loss: 0.4192 - val_acc: 0.9400\n",
            "Epoch 657/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0190 - acc: 0.9938 - val_loss: 0.5647 - val_acc: 0.9200\n",
            "Epoch 658/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0094 - acc: 0.9969 - val_loss: 0.3899 - val_acc: 0.9350\n",
            "Epoch 659/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0152 - acc: 0.9956 - val_loss: 0.6532 - val_acc: 0.8900\n",
            "Epoch 660/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0036 - acc: 0.9994 - val_loss: 0.5344 - val_acc: 0.9150\n",
            "Epoch 661/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.5612 - val_acc: 0.9200\n",
            "Epoch 662/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.5373 - val_acc: 0.9250\n",
            "Epoch 663/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.5642 - val_acc: 0.9100\n",
            "Epoch 664/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.5527 - val_acc: 0.9250\n",
            "Epoch 665/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0044 - acc: 0.9994 - val_loss: 0.4364 - val_acc: 0.9300\n",
            "Epoch 666/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.5405 - val_acc: 0.9200\n",
            "Epoch 667/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 7.8619e-04 - acc: 1.0000 - val_loss: 0.5746 - val_acc: 0.9200\n",
            "Epoch 668/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.5379 - val_acc: 0.9200\n",
            "Epoch 669/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.5792 - val_acc: 0.9250\n",
            "Epoch 670/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0062 - acc: 0.9975 - val_loss: 0.6838 - val_acc: 0.9150\n",
            "Epoch 671/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0479 - acc: 0.9881 - val_loss: 0.5257 - val_acc: 0.9050\n",
            "Epoch 672/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0064 - acc: 0.9988 - val_loss: 0.4885 - val_acc: 0.9200\n",
            "Epoch 673/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.6247 - val_acc: 0.9300\n",
            "Epoch 674/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0041 - acc: 0.9994 - val_loss: 0.6506 - val_acc: 0.9000\n",
            "Epoch 675/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0047 - acc: 0.9981 - val_loss: 0.6550 - val_acc: 0.9100\n",
            "Epoch 676/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.5053 - val_acc: 0.9250\n",
            "Epoch 677/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 5.5239e-04 - acc: 1.0000 - val_loss: 0.4842 - val_acc: 0.9150\n",
            "Epoch 678/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.4984 - val_acc: 0.9250\n",
            "Epoch 679/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.7076 - val_acc: 0.9250\n",
            "Epoch 680/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.4856 - val_acc: 0.9200\n",
            "Epoch 681/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.6243 - val_acc: 0.9200\n",
            "Epoch 682/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.6879e-04 - acc: 1.0000 - val_loss: 0.6935 - val_acc: 0.9100\n",
            "Epoch 683/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.8858 - val_acc: 0.9200\n",
            "Epoch 684/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.7659 - val_acc: 0.9200\n",
            "Epoch 685/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0261 - acc: 0.9944 - val_loss: 0.5961 - val_acc: 0.9150\n",
            "Epoch 686/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.4906 - val_acc: 0.9300\n",
            "Epoch 687/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 6.7634e-04 - acc: 1.0000 - val_loss: 0.6151 - val_acc: 0.9300\n",
            "Epoch 688/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.7844 - val_acc: 0.9200\n",
            "Epoch 689/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9988 - val_loss: 0.4517 - val_acc: 0.9400\n",
            "Epoch 690/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.7095 - val_acc: 0.9050\n",
            "Epoch 691/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0039 - acc: 0.9981 - val_loss: 0.6167 - val_acc: 0.9150\n",
            "Epoch 692/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.6530 - val_acc: 0.9050\n",
            "Epoch 693/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0406 - acc: 0.9925 - val_loss: 0.6817 - val_acc: 0.9150\n",
            "Epoch 694/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0194 - acc: 0.9944 - val_loss: 0.6536 - val_acc: 0.9150\n",
            "Epoch 695/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0386 - acc: 0.9894 - val_loss: 0.6954 - val_acc: 0.8950\n",
            "Epoch 696/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0881 - acc: 0.9813 - val_loss: 0.7001 - val_acc: 0.9100\n",
            "Epoch 697/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.5179 - val_acc: 0.9150\n",
            "Epoch 698/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0063 - acc: 0.9981 - val_loss: 0.4060 - val_acc: 0.9300\n",
            "Epoch 699/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.4506 - val_acc: 0.9300\n",
            "Epoch 700/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.5053 - val_acc: 0.9200\n",
            "Epoch 701/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.3946 - val_acc: 0.9250\n",
            "Epoch 702/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0082 - acc: 0.9963 - val_loss: 0.4679 - val_acc: 0.9350\n",
            "Epoch 703/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0087 - acc: 0.9963 - val_loss: 0.4684 - val_acc: 0.9200\n",
            "Epoch 704/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0060 - acc: 0.9988 - val_loss: 0.4383 - val_acc: 0.9400\n",
            "Epoch 705/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.4800 - val_acc: 0.9250\n",
            "Epoch 706/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0037 - acc: 0.9981 - val_loss: 0.4704 - val_acc: 0.9200\n",
            "Epoch 707/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.4623 - val_acc: 0.9300\n",
            "Epoch 708/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.4855 - val_acc: 0.9350\n",
            "Epoch 709/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.5409 - val_acc: 0.9350\n",
            "Epoch 710/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0042 - acc: 0.9981 - val_loss: 0.4524 - val_acc: 0.9350\n",
            "Epoch 711/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 3.0995e-04 - acc: 1.0000 - val_loss: 0.4621 - val_acc: 0.9300\n",
            "Epoch 712/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.5194 - val_acc: 0.9250\n",
            "Epoch 713/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.4294 - val_acc: 0.9350\n",
            "Epoch 714/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 6.6825e-04 - acc: 1.0000 - val_loss: 0.4489 - val_acc: 0.9300\n",
            "Epoch 715/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.5031 - val_acc: 0.9300\n",
            "Epoch 716/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.5859 - val_acc: 0.9250\n",
            "Epoch 717/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.5028 - val_acc: 0.9350\n",
            "Epoch 718/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0024 - acc: 0.9988 - val_loss: 0.4709 - val_acc: 0.9250\n",
            "Epoch 719/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.6034 - val_acc: 0.9150\n",
            "Epoch 720/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0330 - acc: 0.9900 - val_loss: 0.5770 - val_acc: 0.9150\n",
            "Epoch 721/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0391 - acc: 0.9850 - val_loss: 0.5873 - val_acc: 0.9150\n",
            "Epoch 722/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0214 - acc: 0.9956 - val_loss: 0.4606 - val_acc: 0.9300\n",
            "Epoch 723/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0049 - acc: 0.9988 - val_loss: 0.4180 - val_acc: 0.9250\n",
            "Epoch 724/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.4452 - val_acc: 0.9200\n",
            "Epoch 725/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.4597 - val_acc: 0.9150\n",
            "Epoch 726/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.4757 - val_acc: 0.9100\n",
            "Epoch 727/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.4552 - val_acc: 0.9150\n",
            "Epoch 728/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.5036 - val_acc: 0.9250\n",
            "Epoch 729/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0033 - acc: 0.9994 - val_loss: 0.5892 - val_acc: 0.9250\n",
            "Epoch 730/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.5573 - val_acc: 0.9100\n",
            "Epoch 731/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.6478 - val_acc: 0.9150\n",
            "Epoch 732/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0198 - acc: 0.9944 - val_loss: 0.5226 - val_acc: 0.9200\n",
            "Epoch 733/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0233 - acc: 0.9938 - val_loss: 0.5184 - val_acc: 0.9300\n",
            "Epoch 734/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.5363 - val_acc: 0.9250\n",
            "Epoch 735/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.5621 - val_acc: 0.9150\n",
            "Epoch 736/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.8783e-04 - acc: 1.0000 - val_loss: 0.5669 - val_acc: 0.9100\n",
            "Epoch 737/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0086 - acc: 0.9969 - val_loss: 0.6692 - val_acc: 0.9150\n",
            "Epoch 738/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.5157 - val_acc: 0.9250\n",
            "Epoch 739/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0208 - acc: 0.9931 - val_loss: 0.8637 - val_acc: 0.9100\n",
            "Epoch 740/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0554 - acc: 0.9850 - val_loss: 0.7787 - val_acc: 0.9000\n",
            "Epoch 741/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0560 - acc: 0.9844 - val_loss: 0.5071 - val_acc: 0.9250\n",
            "Epoch 742/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0422 - acc: 0.9925 - val_loss: 0.6123 - val_acc: 0.8900\n",
            "Epoch 743/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0118 - acc: 0.9944 - val_loss: 0.4532 - val_acc: 0.9150\n",
            "Epoch 744/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.4291 - val_acc: 0.9250\n",
            "Epoch 745/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0085 - acc: 0.9975 - val_loss: 0.5356 - val_acc: 0.9050\n",
            "Epoch 746/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.5571 - val_acc: 0.9050\n",
            "Epoch 747/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.5662 - val_acc: 0.9150\n",
            "Epoch 748/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.5550 - val_acc: 0.9150\n",
            "Epoch 749/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.5444 - val_acc: 0.9250\n",
            "Epoch 750/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.6376 - val_acc: 0.9200\n",
            "Epoch 751/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.6413 - val_acc: 0.9150\n",
            "Epoch 752/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.6670 - val_acc: 0.9200\n",
            "Epoch 753/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0035 - acc: 0.9988 - val_loss: 0.7130 - val_acc: 0.9150\n",
            "Epoch 754/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0026 - acc: 0.9988 - val_loss: 0.5027 - val_acc: 0.9250\n",
            "Epoch 755/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.4471e-04 - acc: 1.0000 - val_loss: 0.5108 - val_acc: 0.9250\n",
            "Epoch 756/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.4067e-04 - acc: 1.0000 - val_loss: 0.5253 - val_acc: 0.9250\n",
            "Epoch 757/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.5663 - val_acc: 0.9200\n",
            "Epoch 758/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0024 - acc: 0.9988 - val_loss: 0.5567 - val_acc: 0.9350\n",
            "Epoch 759/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0025 - acc: 0.9988 - val_loss: 0.5856 - val_acc: 0.9300\n",
            "Epoch 760/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.5820 - val_acc: 0.9300\n",
            "Epoch 761/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 4.0780e-04 - acc: 1.0000 - val_loss: 0.5739 - val_acc: 0.9300\n",
            "Epoch 762/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 4.2317e-04 - acc: 1.0000 - val_loss: 0.5821 - val_acc: 0.9250\n",
            "Epoch 763/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 4.7786e-04 - acc: 1.0000 - val_loss: 0.5883 - val_acc: 0.9050\n",
            "Epoch 764/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 3.5068e-04 - acc: 1.0000 - val_loss: 0.6116 - val_acc: 0.9100\n",
            "Epoch 765/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0024 - acc: 0.9988 - val_loss: 0.6045 - val_acc: 0.9150\n",
            "Epoch 766/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0085 - acc: 0.9969 - val_loss: 0.5785 - val_acc: 0.9200\n",
            "Epoch 767/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0095 - acc: 0.9963 - val_loss: 0.8626 - val_acc: 0.8950\n",
            "Epoch 768/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0420 - acc: 0.9875 - val_loss: 0.7573 - val_acc: 0.8950\n",
            "Epoch 769/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0402 - acc: 0.9875 - val_loss: 0.5530 - val_acc: 0.9250\n",
            "Epoch 770/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0052 - acc: 0.9969 - val_loss: 0.5198 - val_acc: 0.9200\n",
            "Epoch 771/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.5891 - val_acc: 0.9100\n",
            "Epoch 772/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0012 - acc: 0.9994 - val_loss: 0.5611 - val_acc: 0.9200\n",
            "Epoch 773/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.5479 - val_acc: 0.9250\n",
            "Epoch 774/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.5880 - val_acc: 0.9200\n",
            "Epoch 775/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 7.5084e-04 - acc: 1.0000 - val_loss: 0.5691 - val_acc: 0.9250\n",
            "Epoch 776/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0015 - acc: 0.9994 - val_loss: 0.6856 - val_acc: 0.9050\n",
            "Epoch 777/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.7786 - val_acc: 0.9000\n",
            "Epoch 778/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.5168e-04 - acc: 1.0000 - val_loss: 0.6795 - val_acc: 0.9100\n",
            "Epoch 779/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 6.9496e-04 - acc: 1.0000 - val_loss: 0.6064 - val_acc: 0.9150\n",
            "Epoch 780/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.6375 - val_acc: 0.9250\n",
            "Epoch 781/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 5.7119e-04 - acc: 1.0000 - val_loss: 0.6909 - val_acc: 0.9050\n",
            "Epoch 782/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 7.1679e-04 - acc: 1.0000 - val_loss: 0.7050 - val_acc: 0.9050\n",
            "Epoch 783/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 3.4557e-04 - acc: 1.0000 - val_loss: 0.6770 - val_acc: 0.9050\n",
            "Epoch 784/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.4668e-04 - acc: 1.0000 - val_loss: 0.6682 - val_acc: 0.9100\n",
            "Epoch 785/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 7.1593e-04 - acc: 1.0000 - val_loss: 0.6849 - val_acc: 0.9100\n",
            "Epoch 786/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0108 - acc: 0.9975 - val_loss: 0.8203 - val_acc: 0.9050\n",
            "Epoch 787/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1006 - acc: 0.9794 - val_loss: 0.6426 - val_acc: 0.9050\n",
            "Epoch 788/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0283 - acc: 0.9900 - val_loss: 0.7082 - val_acc: 0.9050\n",
            "Epoch 789/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0074 - acc: 0.9988 - val_loss: 0.6158 - val_acc: 0.9300\n",
            "Epoch 790/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0053 - acc: 0.9981 - val_loss: 0.6932 - val_acc: 0.9050\n",
            "Epoch 791/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.6700 - val_acc: 0.9100\n",
            "Epoch 792/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.7430 - val_acc: 0.9100\n",
            "Epoch 793/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0021 - acc: 0.9988 - val_loss: 0.8327 - val_acc: 0.9050\n",
            "Epoch 794/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.7560 - val_acc: 0.9100\n",
            "Epoch 795/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.0558 - val_acc: 0.9000\n",
            "Epoch 796/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0073 - acc: 0.9969 - val_loss: 0.7037 - val_acc: 0.9150\n",
            "Epoch 797/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0025 - acc: 0.9988 - val_loss: 0.6769 - val_acc: 0.9150\n",
            "Epoch 798/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.6319 - val_acc: 0.9100\n",
            "Epoch 799/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.8302 - val_acc: 0.9100\n",
            "Epoch 800/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.6359 - val_acc: 0.9150\n",
            "Epoch 801/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.4577e-04 - acc: 1.0000 - val_loss: 0.6405 - val_acc: 0.9200\n",
            "Epoch 802/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.7384 - val_acc: 0.9250\n",
            "Epoch 803/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.6679 - val_acc: 0.9250\n",
            "Epoch 804/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0015 - acc: 0.9994 - val_loss: 0.7035 - val_acc: 0.9100\n",
            "Epoch 805/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0681 - acc: 0.9894 - val_loss: 0.7323 - val_acc: 0.9250\n",
            "Epoch 806/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0206 - acc: 0.9969 - val_loss: 0.8882 - val_acc: 0.8950\n",
            "Epoch 807/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0509 - acc: 0.9875 - val_loss: 0.6916 - val_acc: 0.9100\n",
            "Epoch 808/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0163 - acc: 0.9944 - val_loss: 0.6254 - val_acc: 0.9150\n",
            "Epoch 809/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0049 - acc: 0.9988 - val_loss: 0.7327 - val_acc: 0.9000\n",
            "Epoch 810/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 9.9590e-04 - acc: 1.0000 - val_loss: 0.7432 - val_acc: 0.9050\n",
            "Epoch 811/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.7121 - val_acc: 0.9100\n",
            "Epoch 812/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.7343 - val_acc: 0.9100\n",
            "Epoch 813/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.9607 - val_acc: 0.9050\n",
            "Epoch 814/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0036 - acc: 0.9994 - val_loss: 0.8483 - val_acc: 0.9050\n",
            "Epoch 815/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.7167 - val_acc: 0.9000\n",
            "Epoch 816/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.7490 - val_acc: 0.9050\n",
            "Epoch 817/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.8042 - val_acc: 0.9050\n",
            "Epoch 818/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9988 - val_loss: 0.6879 - val_acc: 0.9050\n",
            "Epoch 819/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.6566 - val_acc: 0.9250\n",
            "Epoch 820/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.9240 - val_acc: 0.9100\n",
            "Epoch 821/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 6.1561e-04 - acc: 1.0000 - val_loss: 0.8285 - val_acc: 0.9100\n",
            "Epoch 822/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 5.0923e-04 - acc: 1.0000 - val_loss: 0.8121 - val_acc: 0.9050\n",
            "Epoch 823/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 5.5313e-04 - acc: 1.0000 - val_loss: 0.7173 - val_acc: 0.9200\n",
            "Epoch 824/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 7.5860e-04 - acc: 0.9994 - val_loss: 0.7296 - val_acc: 0.9150\n",
            "Epoch 825/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.7609 - val_acc: 0.9050\n",
            "Epoch 826/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 6.0249e-04 - acc: 1.0000 - val_loss: 0.8391 - val_acc: 0.9000\n",
            "Epoch 827/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 2.5273e-04 - acc: 1.0000 - val_loss: 0.8437 - val_acc: 0.9000\n",
            "Epoch 828/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.7057 - val_acc: 0.9300\n",
            "Epoch 829/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.6700 - val_acc: 0.9300\n",
            "Epoch 830/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 5.2841e-04 - acc: 1.0000 - val_loss: 0.6891 - val_acc: 0.9300\n",
            "Epoch 831/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 5.2495e-04 - acc: 1.0000 - val_loss: 0.6782 - val_acc: 0.9300\n",
            "Epoch 832/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0044 - acc: 0.9981 - val_loss: 0.7683 - val_acc: 0.9100\n",
            "Epoch 833/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0152 - acc: 0.9944 - val_loss: 0.7823 - val_acc: 0.9100\n",
            "Epoch 834/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1090 - acc: 0.9756 - val_loss: 0.7671 - val_acc: 0.8950\n",
            "Epoch 835/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0851 - acc: 0.9731 - val_loss: 0.9519 - val_acc: 0.8650\n",
            "Epoch 836/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0161 - acc: 0.9938 - val_loss: 0.7254 - val_acc: 0.9050\n",
            "Epoch 837/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.7607 - val_acc: 0.9050\n",
            "Epoch 838/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0036 - acc: 0.9994 - val_loss: 0.6973 - val_acc: 0.9100\n",
            "Epoch 839/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0026 - acc: 0.9988 - val_loss: 0.6351 - val_acc: 0.9150\n",
            "Epoch 840/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0027 - acc: 0.9988 - val_loss: 0.6706 - val_acc: 0.9100\n",
            "Epoch 841/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.6783 - val_acc: 0.9100\n",
            "Epoch 842/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.6879 - val_acc: 0.9050\n",
            "Epoch 843/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.5925 - val_acc: 0.9200\n",
            "Epoch 844/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.5291 - val_acc: 0.9300\n",
            "Epoch 845/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.5091e-04 - acc: 1.0000 - val_loss: 0.5616 - val_acc: 0.9150\n",
            "Epoch 846/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 0.9994 - val_loss: 0.5341 - val_acc: 0.9300\n",
            "Epoch 847/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 0.9994 - val_loss: 0.5360 - val_acc: 0.9300\n",
            "Epoch 848/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 6.0254e-04 - acc: 1.0000 - val_loss: 0.6375 - val_acc: 0.9200\n",
            "Epoch 849/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 6.5851e-04 - acc: 1.0000 - val_loss: 0.7088 - val_acc: 0.9200\n",
            "Epoch 850/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 4.3405e-04 - acc: 1.0000 - val_loss: 0.6660 - val_acc: 0.9200\n",
            "Epoch 851/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 0.9994 - val_loss: 0.6989 - val_acc: 0.9200\n",
            "Epoch 852/1000\n",
            "1600/1600 [==============================] - 2s 2ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.7135 - val_acc: 0.9150\n",
            "Epoch 853/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 0.9994 - val_loss: 0.8183 - val_acc: 0.9050\n",
            "Epoch 854/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.5886 - val_acc: 0.9250\n",
            "Epoch 855/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.5453 - val_acc: 0.9200\n",
            "Epoch 856/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 7.5490e-04 - acc: 1.0000 - val_loss: 0.6890 - val_acc: 0.9100\n",
            "Epoch 857/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 3.5479e-04 - acc: 1.0000 - val_loss: 0.6877 - val_acc: 0.9100\n",
            "Epoch 858/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0478 - acc: 0.9944 - val_loss: 0.6828 - val_acc: 0.9200\n",
            "Epoch 859/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0558 - acc: 0.9919 - val_loss: 0.9410 - val_acc: 0.8950\n",
            "Epoch 860/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0554 - acc: 0.9856 - val_loss: 0.6855 - val_acc: 0.9050\n",
            "Epoch 861/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0893 - acc: 0.9831 - val_loss: 0.9255 - val_acc: 0.8750\n",
            "Epoch 862/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0345 - acc: 0.9912 - val_loss: 0.5848 - val_acc: 0.9150\n",
            "Epoch 863/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0133 - acc: 0.9988 - val_loss: 0.6176 - val_acc: 0.9050\n",
            "Epoch 864/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.6506 - val_acc: 0.9150\n",
            "Epoch 865/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.6619 - val_acc: 0.9100\n",
            "Epoch 866/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0119 - acc: 0.9975 - val_loss: 0.6185 - val_acc: 0.9200\n",
            "Epoch 867/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0062 - acc: 0.9994 - val_loss: 0.8454 - val_acc: 0.9000\n",
            "Epoch 868/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.7326 - val_acc: 0.9000\n",
            "Epoch 869/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.9988 - val_loss: 0.6463 - val_acc: 0.9150\n",
            "Epoch 870/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.6347 - val_acc: 0.9150\n",
            "Epoch 871/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 6.9665e-04 - acc: 1.0000 - val_loss: 0.6486 - val_acc: 0.9150\n",
            "Epoch 872/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 9.5580e-04 - acc: 1.0000 - val_loss: 0.6407 - val_acc: 0.9150\n",
            "Epoch 873/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 5.7348e-04 - acc: 1.0000 - val_loss: 0.6513 - val_acc: 0.9150\n",
            "Epoch 874/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 9.8307e-04 - acc: 1.0000 - val_loss: 0.6663 - val_acc: 0.9150\n",
            "Epoch 875/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 4.5004e-04 - acc: 1.0000 - val_loss: 0.6827 - val_acc: 0.9150\n",
            "Epoch 876/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.7500 - val_acc: 0.9100\n",
            "Epoch 877/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0082 - acc: 0.9981 - val_loss: 0.6743 - val_acc: 0.9100\n",
            "Epoch 878/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0299 - acc: 0.9931 - val_loss: 0.8226 - val_acc: 0.9000\n",
            "Epoch 879/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0180 - acc: 0.9950 - val_loss: 0.7345 - val_acc: 0.8950\n",
            "Epoch 880/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0095 - acc: 0.9981 - val_loss: 0.7332 - val_acc: 0.9150\n",
            "Epoch 881/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.7278 - val_acc: 0.9150\n",
            "Epoch 882/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.6905 - val_acc: 0.9150\n",
            "Epoch 883/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0017 - acc: 0.9988 - val_loss: 0.7098 - val_acc: 0.9150\n",
            "Epoch 884/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.7141 - val_acc: 0.9150\n",
            "Epoch 885/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 6.8551e-04 - acc: 1.0000 - val_loss: 0.7179 - val_acc: 0.9100\n",
            "Epoch 886/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 7.6025e-04 - acc: 1.0000 - val_loss: 0.6995 - val_acc: 0.9200\n",
            "Epoch 887/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 2.3484e-04 - acc: 1.0000 - val_loss: 0.6944 - val_acc: 0.9250\n",
            "Epoch 888/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0027 - acc: 0.9988 - val_loss: 0.6851 - val_acc: 0.9200\n",
            "Epoch 889/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.6784 - val_acc: 0.9200\n",
            "Epoch 890/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 0.9994 - val_loss: 0.6838 - val_acc: 0.9150\n",
            "Epoch 891/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0046 - acc: 0.9981 - val_loss: 0.6986 - val_acc: 0.9100\n",
            "Epoch 892/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0203 - acc: 0.9956 - val_loss: 0.7649 - val_acc: 0.9250\n",
            "Epoch 893/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0285 - acc: 0.9950 - val_loss: 0.6046 - val_acc: 0.9200\n",
            "Epoch 894/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0162 - acc: 0.9963 - val_loss: 0.7730 - val_acc: 0.9000\n",
            "Epoch 895/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0071 - acc: 0.9981 - val_loss: 0.8668 - val_acc: 0.9050\n",
            "Epoch 896/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0066 - acc: 0.9981 - val_loss: 0.7557 - val_acc: 0.9000\n",
            "Epoch 897/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0050 - acc: 0.9975 - val_loss: 0.7479 - val_acc: 0.9100\n",
            "Epoch 898/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0026 - acc: 0.9988 - val_loss: 0.7142 - val_acc: 0.9150\n",
            "Epoch 899/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.7275 - val_acc: 0.9150\n",
            "Epoch 900/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.7973 - val_acc: 0.9100\n",
            "Epoch 901/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 7.8128e-04 - acc: 1.0000 - val_loss: 0.7173 - val_acc: 0.9150\n",
            "Epoch 902/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.6807 - val_acc: 0.9350\n",
            "Epoch 903/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.6196 - val_acc: 0.9200\n",
            "Epoch 904/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.5760 - val_acc: 0.9200\n",
            "Epoch 905/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.5982 - val_acc: 0.9100\n",
            "Epoch 906/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0027 - acc: 0.9981 - val_loss: 0.9499 - val_acc: 0.8950\n",
            "Epoch 907/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0034 - acc: 0.9994 - val_loss: 0.6341 - val_acc: 0.9200\n",
            "Epoch 908/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 9.1013e-04 - acc: 1.0000 - val_loss: 0.7344 - val_acc: 0.9150\n",
            "Epoch 909/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0031 - acc: 0.9981 - val_loss: 0.7670 - val_acc: 0.9000\n",
            "Epoch 910/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0441 - acc: 0.9881 - val_loss: 0.9315 - val_acc: 0.8900\n",
            "Epoch 911/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0593 - acc: 0.9844 - val_loss: 0.8087 - val_acc: 0.8900\n",
            "Epoch 912/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0106 - acc: 0.9975 - val_loss: 0.6385 - val_acc: 0.9100\n",
            "Epoch 913/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0067 - acc: 0.9975 - val_loss: 0.6633 - val_acc: 0.9050\n",
            "Epoch 914/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.5378 - val_acc: 0.9150\n",
            "Epoch 915/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.6101 - val_acc: 0.9150\n",
            "Epoch 916/1000\n",
            "1600/1600 [==============================] - 2s 2ms/step - loss: 5.5058e-04 - acc: 1.0000 - val_loss: 0.5768 - val_acc: 0.9150\n",
            "Epoch 917/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.5786 - val_acc: 0.9200\n",
            "Epoch 918/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 4.5505e-04 - acc: 1.0000 - val_loss: 0.5975 - val_acc: 0.9150\n",
            "Epoch 919/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 6.2965e-04 - acc: 1.0000 - val_loss: 0.5743 - val_acc: 0.9300\n",
            "Epoch 920/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 5.5977e-04 - acc: 1.0000 - val_loss: 0.5990 - val_acc: 0.9350\n",
            "Epoch 921/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.2095e-04 - acc: 0.9994 - val_loss: 0.6122 - val_acc: 0.9300\n",
            "Epoch 922/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.5886 - val_acc: 0.9250\n",
            "Epoch 923/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0012 - acc: 0.9994 - val_loss: 0.6760 - val_acc: 0.9200\n",
            "Epoch 924/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 5.5869e-04 - acc: 1.0000 - val_loss: 0.6576 - val_acc: 0.9150\n",
            "Epoch 925/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.6193e-04 - acc: 1.0000 - val_loss: 0.6897 - val_acc: 0.9150\n",
            "Epoch 926/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 5.5690e-04 - acc: 1.0000 - val_loss: 0.6528 - val_acc: 0.9200\n",
            "Epoch 927/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.4354e-04 - acc: 1.0000 - val_loss: 0.6084 - val_acc: 0.9150\n",
            "Epoch 928/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 3.9275e-04 - acc: 1.0000 - val_loss: 0.6280 - val_acc: 0.9150\n",
            "Epoch 929/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.6256 - val_acc: 0.9250\n",
            "Epoch 930/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 3.4339e-04 - acc: 1.0000 - val_loss: 0.6567 - val_acc: 0.9250\n",
            "Epoch 931/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 2.4243e-04 - acc: 1.0000 - val_loss: 0.6368 - val_acc: 0.9200\n",
            "Epoch 932/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 5.5976e-04 - acc: 1.0000 - val_loss: 0.6551 - val_acc: 0.9100\n",
            "Epoch 933/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.7237 - val_acc: 0.9200\n",
            "Epoch 934/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.7251e-04 - acc: 1.0000 - val_loss: 0.7501 - val_acc: 0.9150\n",
            "Epoch 935/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0049 - acc: 0.9969 - val_loss: 1.2509 - val_acc: 0.8750\n",
            "Epoch 936/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0263 - acc: 0.9912 - val_loss: 0.7217 - val_acc: 0.9100\n",
            "Epoch 937/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0564 - acc: 0.9850 - val_loss: 0.7805 - val_acc: 0.9050\n",
            "Epoch 938/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0224 - acc: 0.9925 - val_loss: 0.4145 - val_acc: 0.9250\n",
            "Epoch 939/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0056 - acc: 0.9969 - val_loss: 0.5685 - val_acc: 0.9100\n",
            "Epoch 940/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0029 - acc: 0.9981 - val_loss: 0.5332 - val_acc: 0.9250\n",
            "Epoch 941/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.5226 - val_acc: 0.9200\n",
            "Epoch 942/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.6939 - val_acc: 0.9050\n",
            "Epoch 943/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.6249 - val_acc: 0.9200\n",
            "Epoch 944/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 5.1299e-04 - acc: 1.0000 - val_loss: 0.6266 - val_acc: 0.9100\n",
            "Epoch 945/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 3.6030e-04 - acc: 1.0000 - val_loss: 0.6307 - val_acc: 0.9100\n",
            "Epoch 946/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 6.2943e-04 - acc: 1.0000 - val_loss: 0.6219 - val_acc: 0.9250\n",
            "Epoch 947/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.5490 - val_acc: 0.9250\n",
            "Epoch 948/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 6.7603e-04 - acc: 1.0000 - val_loss: 0.5308 - val_acc: 0.9300\n",
            "Epoch 949/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.6927 - val_acc: 0.9150\n",
            "Epoch 950/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.5378 - val_acc: 0.9200\n",
            "Epoch 951/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 6.2247e-04 - acc: 1.0000 - val_loss: 0.5642 - val_acc: 0.9250\n",
            "Epoch 952/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0033 - acc: 0.9988 - val_loss: 0.7094 - val_acc: 0.9100\n",
            "Epoch 953/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 3.3728e-04 - acc: 1.0000 - val_loss: 0.6402 - val_acc: 0.9150\n",
            "Epoch 954/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.5865 - val_acc: 0.9350\n",
            "Epoch 955/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0013 - acc: 0.9994 - val_loss: 0.7575 - val_acc: 0.9100\n",
            "Epoch 956/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 7.3994e-04 - acc: 1.0000 - val_loss: 0.7668 - val_acc: 0.9000\n",
            "Epoch 957/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 4.8145e-04 - acc: 1.0000 - val_loss: 0.6287 - val_acc: 0.9200\n",
            "Epoch 958/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.7045 - val_acc: 0.9100\n",
            "Epoch 959/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.9988 - val_loss: 0.5746 - val_acc: 0.9200\n",
            "Epoch 960/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0013 - acc: 0.9994 - val_loss: 0.7679 - val_acc: 0.9200\n",
            "Epoch 961/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0114 - acc: 0.9975 - val_loss: 1.1788 - val_acc: 0.8900\n",
            "Epoch 962/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1288 - acc: 0.9700 - val_loss: 0.7227 - val_acc: 0.8850\n",
            "Epoch 963/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0736 - acc: 0.9844 - val_loss: 0.5862 - val_acc: 0.9250\n",
            "Epoch 964/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0115 - acc: 0.9950 - val_loss: 0.5588 - val_acc: 0.9200\n",
            "Epoch 965/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0061 - acc: 0.9975 - val_loss: 0.4485 - val_acc: 0.9250\n",
            "Epoch 966/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.4345 - val_acc: 0.9200\n",
            "Epoch 967/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.4876 - val_acc: 0.9300\n",
            "Epoch 968/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.4505 - val_acc: 0.9200\n",
            "Epoch 969/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.4444 - val_acc: 0.9300\n",
            "Epoch 970/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0112 - acc: 0.9975 - val_loss: 0.5585 - val_acc: 0.9200\n",
            "Epoch 971/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0065 - acc: 0.9975 - val_loss: 0.4529 - val_acc: 0.9300\n",
            "Epoch 972/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.4998 - val_acc: 0.9200\n",
            "Epoch 973/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0026 - acc: 0.9988 - val_loss: 0.5974 - val_acc: 0.9150\n",
            "Epoch 974/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.5499 - val_acc: 0.9100\n",
            "Epoch 975/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.5993 - val_acc: 0.9200\n",
            "Epoch 976/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 0.9994 - val_loss: 0.6196 - val_acc: 0.9000\n",
            "Epoch 977/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.5886 - val_acc: 0.9250\n",
            "Epoch 978/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.8550 - val_acc: 0.9050\n",
            "Epoch 979/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0025 - acc: 0.9988 - val_loss: 0.6678 - val_acc: 0.9200\n",
            "Epoch 980/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.7671 - val_acc: 0.9150\n",
            "Epoch 981/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.6826 - val_acc: 0.9150\n",
            "Epoch 982/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0012 - acc: 0.9994 - val_loss: 0.7503 - val_acc: 0.9150\n",
            "Epoch 983/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0013 - acc: 0.9994 - val_loss: 0.6298 - val_acc: 0.9200\n",
            "Epoch 984/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0013 - acc: 0.9988 - val_loss: 0.6220 - val_acc: 0.9250\n",
            "Epoch 985/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 8.4129e-04 - acc: 1.0000 - val_loss: 0.6530 - val_acc: 0.9150\n",
            "Epoch 986/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0050 - acc: 0.9988 - val_loss: 0.9405 - val_acc: 0.9050\n",
            "Epoch 987/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0081 - acc: 0.9975 - val_loss: 0.7544 - val_acc: 0.9050\n",
            "Epoch 988/1000\n",
            "1600/1600 [==============================] - 2s 2ms/step - loss: 0.0026 - acc: 0.9988 - val_loss: 0.6906 - val_acc: 0.9150\n",
            "Epoch 989/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.6633 - val_acc: 0.9150\n",
            "Epoch 990/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0012 - acc: 0.9994 - val_loss: 0.6388 - val_acc: 0.9100\n",
            "Epoch 991/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 2.1718e-04 - acc: 1.0000 - val_loss: 0.6316 - val_acc: 0.9250\n",
            "Epoch 992/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.6946 - val_acc: 0.9200\n",
            "Epoch 993/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 7.2759e-04 - acc: 1.0000 - val_loss: 0.6463 - val_acc: 0.9250\n",
            "Epoch 994/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 5.8613e-04 - acc: 1.0000 - val_loss: 0.6320 - val_acc: 0.9200\n",
            "Epoch 995/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 3.7009e-04 - acc: 1.0000 - val_loss: 0.6348 - val_acc: 0.9200\n",
            "Epoch 996/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 3.1601e-04 - acc: 1.0000 - val_loss: 0.6495 - val_acc: 0.9250\n",
            "Epoch 997/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 2.2127e-04 - acc: 1.0000 - val_loss: 0.6440 - val_acc: 0.9200\n",
            "Epoch 998/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 2.1338e-04 - acc: 1.0000 - val_loss: 0.6354 - val_acc: 0.9250\n",
            "Epoch 999/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0176 - acc: 0.9963 - val_loss: 1.0038 - val_acc: 0.9000\n",
            "Epoch 1000/1000\n",
            "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0318 - acc: 0.9931 - val_loss: 0.8859 - val_acc: 0.9000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb809993630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY3h-6Mz5zv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}